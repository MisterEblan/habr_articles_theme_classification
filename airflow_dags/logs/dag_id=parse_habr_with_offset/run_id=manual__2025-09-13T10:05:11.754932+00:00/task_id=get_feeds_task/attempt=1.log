{"timestamp":"2025-09-13T10:05:13.358543","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-09-13T10:05:13.358977","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/parse_habr_dag.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-09-13T10:05:14.229632","level":"info","event":"Парсим 5 со смещением 1","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:14.229703","level":"info","event":"Попадаем в цикл","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:14.229731","level":"info","event":"В цикле: 1","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:14.229754","level":"info","event":"URL: https://habr.com/ru/articles/page1","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:14.970508","level":"info","event":"Result: <Response [200]>","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:27.135492","level":"info","event":"Feed: pages=[Page(id='945452', title='Клиент вечно недоволен: инструкция по выживанию для PM', content='Вроде\\xa0бы работа кипит, команда старается, вы делаете всё по\\xa0процессам, но\\xa0в\\xa0ответ\\xa0- постоянное ворчание, претензии и недовольство. «Почему вы не\\xa0предупредили?», «Я думал, всё будет иначе!», «Это слишком долго и дорого». В 99% случаев корень зла\\xa0- не\\xa0в\\xa0плохом коде или\\xa0ленивой команде ( о чём я писал в прошлой статье ), а\\xa0в  сломанной коммуникации и несовпадении ожиданий . Эта статья\\xa0- практическое руководство по\\xa0диагностике и решению этой проблемы. Мы разберем, почему клиент недоволен, и что\\xa0вы можете сделать уже на\\xa0следующей неделе, чтобы это исправить. Шаг 0. Честный разговор с самим собой. Прежде чем предъявлять претензии заказчику, проведите внутренний аудит. Задайте себе и своей команде неудобные вопросы: Были\\xa0ли срывы дедлайнов,  о\\xa0которых мы умолчали, надеясь «успеть к\\xa0следующему спринту»? Понимаем\\xa0ли мы бизнес‑цель  задачи или\\xa0просто бездумно реализуем ТЗ? Насколько наша внутренняя оценка  отличалась от\\xa0реальных трудозатрат? Мы сообщали о\\xa0рисках  заранее или\\xa0тушили пожар, когда он уже разгорелся? Без\\xa0этой внутренней честности все дальнейшие шаги будут бесполезны. Шаг 1. Диагностика. Получаем обратную связь. Спросите напрямую.  Но\\xa0не\\xa0устраивайте допрос, а\\xa0пригласите на\\xa0совместную ретроспективу или\\xa0рабочую встречу с\\xa0единственной целью\\xa0— «стать лучше». Типичные фразы заказчика и что\\xa0они на\\xa0самом деле значат: «Почему не\\xa0сообщили о\\xa0проблеме?»   «Я чувствую потерю контроля. Я не\\xa0уверен, что\\xa0вы справитесь без\\xa0моего микроменеджмента» . «Почему это делалось долго и не\\xa0так, как\\xa0я хотел?» «Мы не\\xa0синхронизировались по\\xa0видению на\\xa0промежуточных этапах. Вы не\\xa0показали мне черновик» . «Почему это будет дороже?» «Мое бюджетное планирование под\\xa0угрозой. Вы не\\xa0предупредили меня об\\xa0этих рисках на\\xa0этапе оценки» . «Почему я не\\xa0владею всей информацией?» «Мне нужно получать статус самому, а\\xa0не\\xa0ждать, пока вы соизволите прислать отчет» . «Я думал, уже всё готово!» «Мы по‑разному трактуем понятие \"готово\" и \"MVP\"» . Как\\xa0видите, за\\xa0каждым упреком стоит пробел в\\xa0коммуникации. Шаг 2. Лечение. Внедряем систему прозрачности. Недовольство рождается в\\xa0вакууме информации. Ваша задача\\xa0—\\xa0ликвидировать этот вакуум. Ваш новый закон\\xa0—  proactive communication . Что\\xa0именно нужно делать: Установите ритм и границы коммуникации.  Пропишите, какая информация, кому, когда и каким каналом поступает. Пример:  Каждую пятницу в 10:00\\xa0— автоматический отчет из\\xa0Jira/Azure DevOps на\\xa0почту заказчику. Каждую среду в 11:00\\xa0— 30-минутный созвон по\\xa0статусу. Совет:  Не\\xa0делайте статусы «ради статуса». Если всё по\\xa0плану\\xa0— короткое «Всё ОК, идем по\\xa0графику» снимет 90% тревог клиента. Фиксируйте всё письменно.   Устных договорённостей не\\xa0существуют. Любое изменение требований, сроков или\\xa0бюджета должно\\xa0быть отражено в\\xa0письменном виде (e-mail, Google Docs, задача в\\xa0Jira с\\xa0комментарием). Это защитит обе стороны от «а я думал...». Говорите на\\xa0языке бизнес‑ценности.   Перестаньте говорить: «Мы делаем фичу X». Начинайте говорить: «Фича X позволит увеличить конверсию на\\xa0этапе оформления заказа на 15%, потому что...». Управляйте ожиданиями на\\xa0опережение. Не\\xa0прячьте риски.  Сообщайте о\\xa0них сразу: «Мы используем новую библиотеку, есть риск, что\\xa0на\\xa0интеграцию уйдет на 2\\xa0дня больше. Я сообщу до\\xa0конца недели». Четко обозначайте цель MVP.  «Сегодня мы покажем работающий прототип процесса оплаты, но\\xa0без\\xa0финального дизайна. Это не\\xa0финальная версия». Превращайте проблемы в\\xa0возможности.  Когда проблема уже случилась, приходите с\\xa0решением и вариантами. «У нас срыв по\\xa0задаче Y. Мы проанализировали и видим три пути: Вариант А: Увеличить бюджет на\\xa0N, чтобы нанять еще одного разработчика и успеть в\\xa0срок. Вариант Б: Перенести срок на\\xa0неделю. Вариант В: Упростить функционал, чтобы успеть в\\xa0сроки и бюджет. Давайте обсудим, какой вариант для\\xa0вас предпочтительнее?» Так вы вовлекаете его в\\xa0ее решение. Это строит партнерство, а\\xa0не\\xa0отношения «виновный\\xa0— пострадавший». Вывод: недовольный клиент — это системная ошибка PM. Постоянное недовольство заказчика\\xa0— это почти всегда симптом сломанных процессов коммуникации, а\\xa0не\\xa0плохой работы команды. Ваша задача как\\xa0PM\\xa0—\\xa0быть не\\xa0почтальоном, который передает сообщения, а\\xa0архитектором коммуникационных потоков. Будьте прозрачны.  Лучше горькая правда, чем сладкая ложь. Будьте проактивны.  Сообщайте о\\xa0проблемах раньше, чем о\\xa0них спросят. Будьте партнером.  Говорите на\\xa0языке ценности и предлагайте решения. Внедрите эти принципы, и вы увидите, как\\xa0недовольный клиент превратится в\\xa0вашего самого лояльного адвоката. А\\xa0как вы справляетесь с\\xa0недовольством заказчиков?', hub='управление персоналом'), Page(id='946574', title='Высоко сижу, далеко гляжу, всё вижу! Новые подходы к детекции объектов с помощью радиосигналов', content='Обсудим свежие разработки в области Wi-Fi Sensing — в каком направлении движутся эти технологии и какие подводные камни видят специалисты. Фотография: Discover Savsat Кто вообще этим занимается Идея использовать радиоволны для распознавания объектов и процессов вокруг — не нова. Еще в 2011 году группа ученых из Университета Юты  предложила  метод медицинского отслеживания дыхательных функций человека с помощью беспроводных приемопередатчиков. Инженер Нил Патвари, который принимал участие в разработке технологии,  лично продемонстрировал  её возможности в больничной палате. В 2015 году специалисты из MIT исследовали другие области применения Wi-Fi-сигналов — в частности, для детекции падений. Они  разработали систему , которая по характеру отраженных радиоволн определяла положение тела, изменения в походке и пульс. Прототип устройства даже  проходил тест-драйв  в Овальном кабинете Белого дома. Сегодня технологию, которая использует Wi-Fi-сигналы для анализа перемещений объектов и жизненных показателей человека, называется Wi-Fi Sensing. Она  развивается  при поддержке Wi-Fi Alliance и уже находит коммерческое применение.\\xa0Например, в 2019 году производитель роутеров Linksys  запустил  сервис Aware — своеобразную систему защиты дома. Маршрутизатор мог засекать движение в помещении и оповещать об этом пользователя. Год спустя исследователи из Ратгерского университета в Нью-Джерси  получили патент  на технологию поиска подозрительных предметов в багаже с помощью обычных Wi‑Fi‑устройств. Система анализирует отраженные сигналы и определяет форму объекта (с точностью до 95%) и материал, из которого он сделан (до 90%). Далее посмотрим на несколько свежих исследовательских проектов, которые в будущем могут дать толчок новым коммерческим разработкам. Новинки биометрической идентификации В 2024 году группа итальянских исследователей из университета «Сапиенца»  представила систему  с говорящим названием WhoFi. Она способна идентифицировать человека по уникальным искажениям Wi-Fi-сигнала, которые вызывает тело, — в амплитуде и фазе электромагнитных волн. Но, пожалуй, наиболее перспективной особенностью технологии стала возможность повторной идентификации (re-identification). Используя глубокое обучение, система WhoFi способна определить одного и того же человека в разных местах и в разное время. На первом этапе WhoFi анализирует данные о состоянии беспроводного канала — Channel State Information (CSI). Они очищаются от шумов с помощью  фильтра Хэмпеля  — алгоритма обнаружения и удаления выбросов — и линейно нормализуются.\\xa0 Следующий шаг — модификация данных для повышения устойчивости модели. Во-первых, к амплитуде добавляется гауссовский шум, представляющий реалистичные флуктуации сигнала. Во-вторых, амплитуда увеличивается на случайную величину, имитируя изменения в силе сигнала из-за факторов среды или особенностей устройства. В-третьих, амплитудная последовательность смещается вперед или назад на произвольную величину для учета возможных временных расхождений. Обработанная информация передается глубокой нейросети, которая формирует компактный вектор — цифровой «отпечаток» человека, необходимый для повторной идентификации. Производительность и точность системы оценили с помощью бенчмарка NTU-Fi. Это — набор данных, специально подготовленный для задач Wi-Fi Sensing. WhoFi  удалось  корректно ре-идентифицировать человека с точностью в 95,5%. Как отмечают авторы исследования, WhoFi может служить вспомогательным механизмом в системах видеонаблюдения — когда нет возможности идентифицировать по другим визуальным признакам.\\xa0 Похожую работу в начале прошлого года провела группа ученых из Шанхайского и Хубэйского университетов совместно с коллегами из Педагогическогой университета Центрального Китая. Их метод Wi-Fi-идентификации также  построен  на анализе CSI-данных, собираемых сетевым устройством с модифицированной прошивкой.\\xa0На этапе предварительной обработки шумы устранялись с помощью низкочастотного фильтра и  вейвлет-преобразования . Затем нейросеть выделяла устойчивые паттерны сигнала, и применялся softmax-классификатор: данные последовательно проходили входной и два скрытых слоя признаков, после чего  softmax-функция  нормализовала выходные значения (в диапазон от 0 до 1). Разработанный учеными метод оказался довольно точным — он определял присутствие человека в помещении с точностью до 99,5%. Еще один проект в сфере Wi-Fi Sensing  представили  хорватские исследователи из Загребского университета в феврале. Они разработали метод, определяющий не только наличие людей в помещении, но и их количество. В основу идеи положен показатель уровня принимаемого сигнала —  RSSI . Если зафиксировать уровень и параметры Wi-Fi-сигнала в пустой комнате — установить точку отсчета — то можно использовать стандартное отклонение измеряемых показателей RSSI в качестве индикатора. Чтобы посчитать точное количество людей, были использованы методы МО: k-ближайших соседей, деревья решений и случайный лес. Все три алгоритма показали высокую точность, но метод случайного леса определял число людей с точностью до 99%. Оправданные опасения Развитие Wi-Fi Sensing закономерно вызывает опасения с точки зрения ИБ и конфиденциальности. Исследователи из Юго-Западного нефтяного университета и Университета электронных наук и технологий Китая  опубликовали  научную работу, в которой выделили две категории атак на подобные беспроводные системы. Первый тип — активные атаки, когда злоумышленник вмешивается в нормальную работу системы — например, с целью манипулировать CSI и работой глубоких нейросетей, чтобы подделать результаты распознавания. Второй тип — пассивные атаки, подразумевающие прослушку беспроводного канала для сбора данных о системе и пользователях без их ведома. Фотография: Bernard Hermant Кроме того, при повсеместном распространении устройств, поддерживающих подобную технологию, злоумышленники  потенциально  смогут продавать данные о жителях квартир и домов — когда те находятся вне дома. Тот факт, что устройства способны раскрывать биометрические параметры, только усугубляет ситуацию. Например, специалисты из Шаньдунского и Цзясинского университетов  разработали  ML-модель, которая практически со 100% вероятностью идентифицирует рост (c погрешностью < 4 см) и вес (погрешность < 4 кг), анализируя данные, собранные с помощью обыкновенных Wi-Fi-устройств. На Hacker News пользователи не сильно сдерживаются в  формулировках : « Кто вообще просил, чтобы сеть умела определять частоту дыхания? Что это дает такого, что реально улучшает жизнь людей? ». Компания Xfinity, которая строит экосистему для умного дома и продает IoT-устройства, в этом году представила проект Wi-Fi Motion — услугу для слежения за движением в домах. И в документации к системе прямым текстом  сказано , что эти данные могут быть переданы «третьим сторонам» — ими вполне могут быть брокеры данных, продающие персональную информацию рекламодателям. И в то время пока профильные организации вроде Wireless Broadband Alliance  предлагают  руководства по настройке устройств, в сообществе можно встретить  мнение  о необходимости более жесткого регулирования — но пока активных шагов в этом направлении не предпринимают. Дополнительное чтение в нашем блоге на Хабре: Почему не взлетел Wireless USB, а также карманный хот-спот и другие материалы в подборке о беспроводных технологиях . Наша подборка материалов, посвященных Wi-Fi-технологиям. Например, среди примечательных статей — разбор ситуации, почему стандарт Wireless USB так и не смог заменить привычные проводные интерфейсы. Казалось бы, у технологии было все: поддержка крупных производителей, перспективный рынок. Но низкая скорость и проблемы с практичностью обернулись провалом. Другой материал — компактная «лабораторка» и еще один — о том, как мы  совершенствуем Wi-Fi HotSpot . Визуализация трафика, перспективы 6G и рабочие кейсы: избранное для интернет-провайдеров и операторов связи . Это — подборка материалов о работе провайдеров и дата-центров. Спектр тем довольно широк: от миграций на IPv6 и автоматизации настройки BRAS до мониторинга трафика с помощью СКАТ DPI. Статьи будут интересны сисадминам, сотрудникам провайдеров и операторам ЦОДов. Открытые инструменты для визуализации сетевого трафика . Показываем свежие решения — например,  Akvorado , анализирующий трафик на основе Netflow, IPFIX и sFlow,  Xenoeye , заточенный под средние и крупные сети, а также  Kubezonnet , который служит для мониторинга трафика между зонами в K8s.', hub='исследования и прогнозы в it'), Page(id='946092', title='Сэндвич, сэр? История британских бутербродов от аристократических салонов до вокзальных буфетов', content=\"Сэндвичи, названные в честь одноимённого графа со специфическими вкусами к развлечениям, появились в 1760-е годы — и стремительно завоевали популярность сначала в Лондоне, а затем и по всей стране. Они долго оставались специфически английским блюдом, причём скорее аристократическим, но затем угодили в США. Там, как и  итальянская пицца , и  японские суши , сэндвичи приняли современные формы, изрядно прибавив в объёме и аппетитности, и превратившись в один из важных блюд мирового фастфуда. Однако финальный штрих к их популярности и повсеместности был сделан вновь англичанами — после того, как они много лет иронизировали по поводу железнодорожных сэндвичей, «непредставимого кулинарного ужаса» даже на фоне прочей британской кухни. Эдуард Гиббон известен двумя вещами: своим шеститомном трудом о падении Римской империи — и первым в истории упоминанием сэндвича Как считается, первым слово сэндвич в английском языке упомянул британский историк Эдуард Гиббон в ноябре 1762 года. Российской аудитории он в основном известен благодаря меметичному оформлению главного труда его жизни, «Истории упадка и разрушения Римской империи». Впрочем, в 62-м Гиббон даже не задумывался о его написании. Тогда 25-летний представитель старинного рода служил капитаном гренадёрской роты в полку ополчения Южного Гэмпшира, сформированного на случай высадки французов в ходе вовсю шедшей Семилетней войны. Войска короля Людовика XV так и не ступили на английские берега, в конце 1762 года дело шло к подписанию мира, что означало роспуск полка. Господа офицеры имели массу свободного времени — и вечер 24 ноября капитан Гиббон провёл в Лондоне, сначала в театре, а затем в заведении под названием Cocoa-Tree.  Судя по сохранившимся изображениям, Эдуард Гиббон любил хорошо поесть, в том числе сэндвичи Это был шоколадный дом или кофейня — типичный для Лондона XVIII века гибрид аристократического клуба с кафе, где подавали далеко не только горячий шоколад. Cocoa-Tree был известен как неформальная штаб-квартира тори, и собиралась там публика, мягко говоря, непростая. Ну а Эдвард Гиббон как представитель старинного рода был туда вхож — и оставил об этом вечере такую запись в дневнике: Это почтенное общество, членом которого я имею честь быть, представляет собой ежевечерне истинно английское зрелище. Двадцать или тридцать, пожалуй, первых людей королевства в смысле блеска и состояния, ужинают за маленькими столиками с салфетками, посреди кофейни, с кусочком холодного мяса, или  Сэндвичем ,   и выпивая стакан пунша. Карикатура на типичных посетителей кофейни Ллойда, конец XVIII века Не вполне понятно, что тут имеет в виду Гиббон: кусок холодного мяса, также именуемый сэндвичем, или же альтернативу между холодным мясом и сэндвичем. Очевидно, впрочем, что в Cocoa-Tree ноября 1762 года это было уже популярное и очень востребованное блюдо. Капитан в дневнике никак не конкретизирует причину наименования его именно так, причём пишет его с большой буквы. Практически все источники едины во мнении, что своим именем это блюдо обязано совершенно конкретному человеку: Джону Монтегю, 4-му графу Сэндвич.  Джон Монтегю, 4-й граф Сэндвич, в честь которого названо одноимённое блюдо Это был не слишком состоятельный, но крайне энергичный деятель, трижды побывавший Первым лордом Адмиралтейства и несколько раз занимавший другие министерские посты. Первым графом Сэндвич был его прапрадед Эдвард Монтегю, получивший титул за активное участие в реставрации монархии и возведение на трон Карла II, а затем деятельно подкрепив его несколькими разгромами голландцев и французах в морских сражениях. В последнем, сражаясь против прославленного де Рюйтера, он погиб на мостике флагмана в бою сразу с несколькими кораблями голландцев.  Сам же титул происходит от названия старинного портового городка Сэндвич / Сануидж на восточном берегу Кента, восходящему к первому в Британии римскому форту в месте высадки легионов Юлия Цезаря. В века переселения на Британские острова англов, саксов и ютов он стал важным портом. Его название восходит к древнеанглийскому Sond Wīc, «Вик-на-Песках», где вик — укреплённое поселение с постоянным торгом. Меметичный указатель у городка Сэндвич, рядом с которым есть и городок Хэм (в переводе «ветчина») А ещё граф любил, так сказать, лихо позажигать — посему стал одним из основателей скандального Hellfire Club, Клуба Адского Пламени. Вокруг этой тусовки молодых аристократов по сей день ходит множество версий и слухов, от смешных до устрашающих. Более умеренные говорят о том, что знатные вольнодумцы и раздолбаи, «верные служители Бахуса, Венеры и Приапа по заветам Франсуа Рабле», просто регулярно упивались в дымину, развлекаясь в заброшенном аббатстве Мэдмэнхэм и системе пещер Бэкингэмшира с дамами нетяжёлого поведения. Более мрачные версии описывают и куда более сомнительные действия — вроде тех, которые описаны в «Заводном апельсине», отчасти «чёрными легендами» о Hellfire Club и вдохновлённом. Ну а конспирологи и вовсе возводят к этому сообществу чуть ли не половину всемирных заговоров — вроде «теории QAnon», и полагают участников чуть ли не сферическими рептилоидами в вакууме. Так что, в каком-то смысле, сэндвичи придумал рептилоид. Сейчас эти пещеры сохраняются в том числе как туристический объект Классической версией происхождения сэндвичей и их названия в честь 4-го графа Сэндвич долгое время считалась та, которая появилась в вышедшем в 1770 году в Париже сборнике путевых заметок «Лондонцы». Его написал писатель Пьер-Жан Гросли, деятель Просвещения и соратник Дени Дидро по составлению Энциклопедии, по мотивам своей поездки в британскую столицу в 1765 году. Там имеются такие строки: Один государственный министр провёл двадцать четыре часа за общественным игорным столом, настолько увлечённый игрой, что всё это время у него не было другого пропитания, кроме куска говядины между двумя ломтиками поджаренного хлеба, который он ел, не отрываясь от игры. Это новое блюдо стало очень популярным во время моего пребывания в Лондоне: его назвали по имени министра, который его придумал. Фрагмент с оригинальной цитатой из «Лондонцев» Однако министром, точнее, госсекретарём Северного департамента (по функциям — отдел МИДа по работе с протестантскими странами) Монтегю стал только в сентябре 1763-го. Почти годом после того, как Эдвард Гиббон описывал сэндвич как типичный вечерний перекус посетителей Cocoa-Tree. В предыдущий раз условно-министерский пост Первого лорда Адмиралтейства граф Сэндвич занимал в 1748-51 годах, что уже выглядит слишком ранней датировкой. Между этими периодами госслужбы Монтегю в основном жил у себя в поместье и посещал «собрания» Hellfire Club. К тому же, по мнению известного историка Николаса Роджера, данные о крайней увлечённости графа карточными играми полностью отсутствуют: развлекался он, как мы знаем, несколько более активными способами. Зато Монтегю был помешан на работе, военно-морском флоте и античной истории, очень много читал и работал с документами, порой круглыми сутками. Вероятно, он пристрастился к бутербродам имени себя скорее за рабочим столом или в кресле за книгой, а не за игрой в карты. Причём Роджерс настаивает, что говядина в них была именно солёной: salt beef (солонина / вяленая), так как граф предпочитал именно такую. Увы, но точного ответа не знают даже прямые потомки в лице прапраправнука Орландо Монтегю — решившего заработать на бренде и основавшего в 2003 году сеть специализированных кафе Earl of Sandwich. В их материалах и названиях блюд в качестве года рождения сэндвича везде значится 1762-й — именно по первому упоминанию в дневнике Гиббона. Исторические реконструкторы изображают Джона Монтегю с сэндвичем в городке Сэндвич в 2012 году на праздновании 250-летия первого упоминания сэндвича Как бы то ни было, у нас остаются два факта. Во-первых, блюдо получило название в честь Джона Монтегю, 4-го графа Сэндвич, реформатора королевского флота и, так сказать, культиста Венеры и Бахуса. То ли он сам его придумал и приказал слугам делать так, то ли было как-то иначе — мы не знаем. Во-вторых, в 1762 году сэндвичи уже были излюбленным блюдом для вечернего перекуса, по меньшей мере, среди вхожих в Cocoa-Tree аристократов-тори, а в 1765-м французский путешественник фиксирует их популярность во всём Лондоне.  Как считается, первое изображение сэндвича на картине «The Porter and the Hare» жившего в Англии художника Иоганна Цоффани, 1768 год. Интересно, что у этого же автора есть портрет и графа Сэндвича Третьей отсечкой, когда блюдо уже прочно вошло в британскую кухню даже на уровне домашней готовки, можно назвать 1773 год. Тогда вышло первое издание кулинарного пособия Шарлотты Мэйсон «The Lady’s assistant for regulating and supplying her table; containing one hundred and fifty select bills of fare» («Помощник леди, отвечающий за регулирование и обслуживание её стола; содержащий сто пятьдесят избранных блюд»). В нём рецепт сэндвичей выглядит следующим образом (с характерным для книг XV-XVIII веков использованием в большинстве позиций вместо строчной s буквы ſ, «длинной s»): Старейший известный рецепт сэндвича, 1773 год: «положить очень тонкие ломтики говядины между тонкими ломтиками хлеба, смазанными маслом; аккуратно обрезать концы и выложить на блюдо. Телятину и ветчину, тонко нарезанные, можно подавать таким же образом» В саркастичной книге Джозефа Пирсона 1792 года «Политический словарь Пирсона, содержащий замечания, определения, объяснения и обычаи, политические и парламентские, но в особенности относящиеся к Палате общин, расположенные в алфавитном порядке» есть отдельный пункт «сэндвичи». Там описывается то, как созданная в 1773 году парламентская столовая Вестминстерского дворца Bellamy's (в целом «чертовски хорошая, где я выпил очень много портвейна») зарабатывала на голодных посетителях затянувшихся заседаний: СЭНДВИЧИ. Два маленьких ломтика хлеба с маслом, почти прозрачных, с тонким кусочком несвежей ветчины или говядины между ними, предназначенные, чтобы люди на галерее не умерли с голоду с одиннадцати часов вечера до шести утра следующего дня. — NB: Bellamy's берёт за них шиллинг, а стоят они им не больше двух пенсов [в шиллинге 12 пенсов]. Заседание палаты общин в XVIII веке, на галерее вверху — посетители и зрители, на продаже которым сомнительных, но дорогих сэндвичей активно зарабатывал Bellamy's С 1780-х годов в Лондоне отслеживаются эксперименты с новыми вариантами начинок сэндвичей. Так, бакалейщик Бёрджесс с улицы Стрэнд публиковал в газетах рекламу своих «лучших анчоусов для хлеба с маслом и для приготовления сэндвичей». Его конкурент мистер Маккейс из Сохо, в свою очередь, рекламировал «консервированные креветки, подходящие для сэндвичей». К рубежу XVIII и XIX веков сэндвичи были неотъемлемой частью меню как модных клубов и кофеен, так и домашней кухни в Лондоне, а число вариантов начинок стремительно росло: все возможные виды мяса и рыбы, паштеты, сыры, зелень, свежие и маринованные овощи. В начале XIX века ходил анекдот про эксцентричную Соу-Бридж: дабы показать презрение к пожилому ухажёру, она положила назойливо предлагавшуюся тем стофунтовую купюру в сэндвич и демонстративно съела.  Британская карикатура 1788 года: довольно скоро понятие «сэндвич» стало использоваться и в переносном смысле для описания чего-то, находящегося между двумя сходными объектами Однако в наши дни сэндвичи — это часто еда, которая захватывается с собой на пикники и в дорогу. Причём в России эта практика довольно молода и оформилась только в XXI веке, тогда как в Великобритании, США и Франции и сэндвич как типичная еда для пикника, и «железнодорожные сэндвичи» являются устойчивым представлением с конца XIX столетия. Как сэндвич выбрался за пределы Лондона и стал ассоциироваться с едой в пути и на лоне природы? Всё началось с 1802 года, когда группа эмигрантов из революционной Франции и англичан-франкофилов создала общество «Pic Nic». Они старались поддерживать на британской земле традиции французской аристократической жизни при ancien régime — и в том числе то, что во Франции с XVII века называлось pique-nique: совместные лёгкие трапезы, на которые все приносят что-то со своей кухни. Термин происходил от выражения «faire un repas à pique-nique», «устроить еду [в формате] поклевать всякого». У французов это могло проводиться как у кого-то дома, так и на свежем воздухе, причём скорее первое — но англичане предпочли адаптировать традицию под себя.  Генри Нельсон О'Нил, «Пикник», 1857 год, редкий пример аутентичной британской картины с пикником. Странно, но английские живописцы викторианской эпохи пикники при всей их популярности практически не изображали. Если на картине XIX века наблюдается вроде бы классический английский пикник — скорее всего это работа американского живописца. Ну а если дамы не очень одеты, то французского. Уже в 1806 году переделанное на английский лад слово «picnic» прочно ассоциировалось в Британии сугубо с выездами на природу с едой — и за первые десятилетия XIX века стало популярнейшей английской традицией. Этому поспособствовал и взрывной рост популярности в британской культуре этого времени движения романтизма — тогда творили лорд Байрон, Мэри Шелли, Уильям Блейк, Вальтер Скотт и другие. В романтизме важнейшее место уделялось «возвращению к природе», «естественности», созерцанию красивых ландшафтов и старинных руин. Среди образованных и не слишком обременённых трудами слоёв общества стало всё более модным выбираться на природу и путешествовать к достопримечательностям, а делать это на голодный желудок несколько не так приятно. И тут сэндвичи, уже популярные в Лондоне, пришлись как нельзя кстати: их вместе с прочей готовой едой стали брать с собой, обёрнутые в пропитанные воском салфетки и уложенные в корзины.  Классический облик плетёной корзины для пикника сформировался в викторианской Англии Новый всплеск популярности сэндвичей принесло становление английской чайной культуры с 1840-х годов. Фрейлина королевы Виктории Анна Рассел, герцогиня Бедфордская, очень любила поесть — и очень не любила паузы между принятыми в то время обедом в полдень и ужином в половину восьмого вечера. Поэтому она завела обыкновение подавать чай с лёгкими закусками в районе 5 часов и приглашать знакомых дам. Идея завирусилась среди скучающих аристократок — и быстро превратилась в массовую традицию, ныне известную как классическое английское чаепитие или файф-о-клок. К чаю полагалось подавать подчёркнуто лёгкие закуски — и минималистичные варианты сэндвичей пришлись и тут как нельзя кстати.  Одним из наиболее популярных форматов сэндвичей для чаепитий, прочно ассоциирующихся у британцев с викторианской эпохой и аристократическими привычками, но малоизвестных и даже странных за пределами островов, является сэндвич с огурцами — cucumber sandwich. Тонкий белый хлеб, обязательно со срезанными корочками, тончайший слой сливочного масла, и не менее тонкие срезы слегка подсоленного огурца без кожуры — и более ничего. В блюде был и трудноуловимый ныне классовый подтекст: до появления массовых дешёвых теплиц выращивание огурцов в Англии было возможным только в оранжереях, и они были весьма дороги. Такие сэндвичи пользовались огромной популярностью среди аристократов и тех, кто усиленно старался им подражать — что к концу XIX века сделало восприятие сэндвича с огурцами несколько ироничным. Так, у Оскара Уайлда в «Как важно быть серьёзным» 1895 года сюжет начинается с того, что легкомысленный Алджерон сам не замечает, как съедает приготовленные к визиту потенциальных невесты и свекрови сэндвичи с огурцами, на неуместную дороговизну которых ему попутно сетует слуга. Огурцы давно везде стали повседневной едой, но и поныне это специфическое блюдо в известных кругах принято подавать на чаепития и брать с собой в корзинах на матчи по крикету. Сэндвичи с огурцом на чаепитии в Кенсингтонском дворце, резиденции герцогов Глостерских Полной противоположностью минималистичному и аристократичному сэндвичу с огурцом в британской культуре является так называемый British Rail sandwich — железнодорожный сэндвич. Их как удобную дорожную еду стали продавать на вокзалах ещё во времена взрывного развития железных дорог во второй половине XIX века, и уже тогда к их качеству возникали вопросы. В отличие от ориентированных на богатую публику кофеен и домашней готовки, здесь ориентировались на аудиторию не слишком платёжеспособную — и готовились сэндвичи из самых недорогих ингредиентов. В общем, у нас это явление называется «вокзальный беляш» или «привокзальная шаурма», а в английском языке на их месте давно устоялся сэндвич того же происхождения. Причём явление имело место везде, где появлялись железные дороги и вокзалы: так, в American Railroad Journal за 1884 год иронизировали следующим образом: Существование железнодорожного сэндвича и его распространение по стране долгое время вызывало ужас у населения и тревогу у медицинского сообщества, успешно с ним справлявшегося. Железнодорожные компании утверждают, что сделали всё возможное, чтобы искоренить заразу, но это заявление воспринимается с недоверием, и даже намекают, что они сознательно способствовали распространению заразы и должны быть сурово наказаны за свою злонамеренность и преднамеренное убийство. Несомненно, что в 1883 году было зарегистрировано 8760 подлинных случаев смерти от железнодорожного сэндвича, и если железные дороги виновны в этом позорном преступлении, которое им вменяют, они должны быть безжалостно наказаны. Исследование специфических свойств сэндвича «железнодорожный» давно является предметом внимания учёных. Профессор Доремус из нашего города недавно провёл следующий анализ его ингредиентов в стократном соотношении: Кожа — 79.06% Оживлённый мрамор — 18.17% Хлеб — 2.76% Ветчина — 0.04% Он завершает свой анализ замечанием, что «железнодорожный сэндвич — самое смертоносное вещество, известное науке, и до сих пор не найдено противоядия, способного предотвратить его пагубные последствия». Профессор Дана из Йельского колледжа, известный специалист по отравлениям мышьяком, также приступил к исследованию железнодорожного сэндвича, и его анализ показывает несколько иные результаты, чем у профессора Доремуса: преобладание кожи и «небольшое количество масла». Как и его коллега-учёный, профессор Дана объявляет железнодорожный сэндвич «веществом, которое при попадании в организм человека оказывает абсолютно и неизменно смертоносное действие». Ну а во второй половине ХХ века, когда британские железные дороги были национализированы и переведены в режим жёсткой экономии, качество стандартизированных вокзальных сэндвичей (нескольких типов, с ветчиной, сардинами, яйцом с зеленью и т.д.) стало предметом постоянных шуток, в том числе в СМИ. В них то находили самые странные и стрёмные ингредиенты вроде утерянного уха Ван Гога, то сравнивали с ними самые неприглядные социальные явления. На ужасающее качество вокзальных сэндвичей в государственных буфетах ссылалась Маргарет Тэтчер, обосновывая программу радикальной приватизации и разгосударствления экономики. Общим местом было мнение о том, что, как выразился один из авторов, «нельзя купить British Rail sandwich, не задаваясь вопросом о том, не станет ли это вашим последним поступком перед как минимум реанимацией». Проблему усугубляло то, что хранились сэндвичи часто даже не в холодильнике, а на витрине, и насколько «живое» их содержимое к моменту покупки — не всегда мог сказать и сам продавец. Некоторые сэндвичи — не для слабонервных Впрочем, между двумя сомнительными полюсами железнодорожного и огуречного сэндвича находилось их всё более бесконечное множество. За вторую половину XIX века сэндвичи стали сначала в Великобритании, а затем и в Голландией типичной уличной пищей для горожан среднего достатка, а затем и рабочих. Причём уникальность сэндвичей как еды проявлялась и проявляется по сей день в том, что они — в отличие от пиццы и бургеров — считаются пристойными и в качестве дешёвого фастфуда, и трапез в «лучших домах». В первой половине ХХ века от прежней практики одной-двух начинок стали переходить ко всё более сложным рецептам. Причём в Англии в аристократических домах старались изобрети всё более причудливые сочетания и измельчить всё до состояния паст и намазок, то в других кругах пошли по более привычному нам ныне пути многослойности. В конце XIX века сэндвичи добрались до США, где поначалу появились на столах склонных к этому времени к англофильству представителей элит Восточного побережья, а также состоятельных нуворишей. Клубный сэндвич, ныне считающийся почти типичным сэндвичем по умолчанию — порождение именно американской ресторанной кухни. Кто именно его придумал и ввёл в меню — споры не утихают до сих пор, но это точно произошло на рубеже 1880-х и 1890-х годов в одном из игорных клубов Нью-Йорка, откуда и название. Американцы же стали превращать традиционно плоские английские сэндвичи в многослойные конструкции, нередко стремящиеся к форму куба, а то и пытаясь уйти куда-то в бесконечность. Клубный сэндвич в особо радикальной американской форме, Murrica hell yeah! В первой половине ХХ века и в США сэндвичи стали одним из типичных видов городского фастфуда — впрочем, уступая в популярности своим близким родственникам-бургерам. Впрочем, изначальные бутерброды-гамбургеры, попавшие с немецкими эмигрантами в Нью-Йорк точно так же,  как пицца с итальянскими , были лишены верхней булки. И то, что мы ныне понимаем под бургерами — по сути гибрид изначального гамбургера с сэндвичем, появившийся на американской земле.  Ну а как возникли наиболее частые ныне диагонально разрезанные сэндвичи, которые повсеместно лежат в современных магазинах на полках с готовой едой? В 1979 году британская сеть магазинов Marks & Spencer представила экспериментальный ассортимент охлаждённых готовых сэндвичей, которые продавались в коробках в форме клина, запечатанных для сохранения свежести и хранились в холодильниках. Сэндвичи, которые было очень удобно брать с собой в дорогу, офис и куда угодно ещё, имели ещё и разительный контраст с вокзальными сэндвичами по качеству — и идея «зашла» потребителям со свистом. К концу десятилетия британская индустрия сэндвичей оценивалась в 1 миллиард фунтов стерлингов, а к началу XXI века треугольные контейнеры с готовыми сэндвичами стали повсеместным и глобальным явлением. В том числе у нас — и в том числе на вокзалах. © 2025 ООО «МТ ФИНАНС»\", hub='научно-популярное'), Page(id='945352', title='Рубиновая скрижаль', content='Глава 1 \\xa0 В этот раз – гранат особенно удачный. Гладкие, блестящие, упругие, зёрнышки лежат ровными рядами, заполняя всё пространство между желтоватой кожицей. Они напоминают плотно подогнанные друг к другу клетки в густом межклеточном матриксе, прозрачные, сочные, сладкие… Но не это поглощает моё внимание. Вместо вкуса граната я ощущаю, как эллагитанины касаются сосочков моего языка, впитывается со слюной, и попадая в жкт поглощаются бактериями вида Bifidobacterium pseudocatenulatum INIA P815, расщепляются на труд-но-вы-го-ва-ри-ва-е-мы-е формулы. А дальше эти бактерии вырабатывают Уролитин А. А-а-а!! Откуда я знаю про Уролитин А? Видения внутренних процессов накрывают меня, будто я подключилась к ВИИ – Вселенскому Источнику Интеллекта. Этот источник бьётся и бурлит где-то в недрах моего подсознания. Сосновая шишка и морской конёк борются за первенство. Самые сокровенные знания черпаю я из глубин своего мозга. Прелестно! Вкусив эту способность, смакую её, как спелый гранат, по зёрнышку. Однажды утром я проснулась, и увидев себя в зеркале, очень удивилась. Точнее, просто оторопела: на лбу алело странное пятно. Оно появилось ночью. Я рефлекторно дотронулась рукой, пятно вспыхнуло… Мягко потёрла это место пальцем, и оно, активизировалось, начало расти и пульсировать! Я испугалась. Что всё это значит, чёрт возьми? Это какой-то знак? Или? Может, мне ночью вживили чип?? Пятно стало бледнеть и уменьшаться в размерах. Для начала нужно было выяснить, является ли «оно» объективной реальностью. – Ты что-то видишь у меня на лбу?\\xa0  Я вышла из ванны, откинув чёлку назад. Он внимательно посмотрел на мой лоб и сказал:  – Вижу маленькую родинку над левой бровью. – А ещё что-то видишь? – Ну вот тут два прыщика… – Не трогай! Пятно видишь посередине? Он посмотрел на лоб и сделал масляные глазки:\\xa0 – Вижу свой будущий поцелуй вот здесь, – он дотронулся до моего виска горячими сухими губами, – И здесь, – он поцеловал в губы, – и здесь…, – слащаво сюсюкал он, целуя меня везде. И я не могла вырваться из его обезьяньей хватки. – И здесь, и здесь, и здесь… В общем, в то утро я ничего так и не выяснила. \\xa0 Что-то странное происходит во мне. Меняется восприятие.\\xa0 Я иду по улице, нахаживая по шагомеру свои 15 тысяч шагов, и думаю, до чего же штампованная наша жизнь! Одни и те же действия мы совершаем ежедневно, как клише, все восемь миллиардов населения, полтора миллиарда из которых – китайцы, семь тысяч алеутов с Аляски и четверо – кереков с Чукотки.\\xa0 Набор действий не так уж велик и для всех одинаков: мы просыпаемся утром в постели, садимся, встаём, ходим, садимся, встаём, ходим, садимся, ходим, садимся, ходим, садимся, ходим, ложимся… Ну ещё, жуем, глотаем, совершаем дефекации или фрикции, делаем что-то руками, перемещаемся в пространстве с помощью дополнительных механизмов, таких, как лифт-машина-самолёт. Всё!\\xa0 У рук – тоже лишь несколько основных движений: тянуть, толкать, сгибать, разгибать, нажимать, сводить, разводить, нести, держать, ударять, да мелкая моторика пальцами, во главе с хватанием. Правда, у животных ещё меньше… (Лапки енотов не в счёт).  Основные вехи: рождение, свадьба, рождение себе подобного, смерть. В промежутках – мелкое копошение. Набор из вариантов нескольких сфер: учеба, работа, развлечения. Матрица не позаботилась о разнообразии возможностей, даже полетать не умеем или изменять форму, отрастить хвост, например, или четыре руки по желанию. Регенерация доступна лишь частично, накопление мутаций неминуемо ждёт всех…\\xa0 Конечно, мы делаем удобной жизнь друг для друга, что именуется прогрессом, но по сути всё одно и то же, клише, одни клише…  Я останавливаюсь у витрины и смотрю сквозь неё. – Странно, у всех мозг внутри, а у вас снаружи.\\xa0 Я вздрагиваю. Этот голос сзади застаёт меня врасплох. Поднимаю глаза и всматриваюсь в витрину. Мужчина средних лет, не высокий, не низкий, всё ещё хорошо сложён, острые глаза, похожие на птичьи. Такой незаметен в толпе, как слепое пятно. За моим плечом он пристально смотрит на лобный пигмент, напоминающий извилины мозга. Это подтверждает то, что я не сошла с ума, или сошли с ума, как минимум, мы двое. – Вы тоже это видите? – мои эмоции скачут на коне впереди меня, сердце бешено стучит: бей или беги! Опасность?\\xa0 Я сканирую пространство, его явная заинтересованность выглядит неопасно. \\xa0– Мне кажется, это проекция моего мозга, вынесенная наружу! Меня не интересует происхождение этой странной штуки, но беспокоит, что она означает? Он молчит. – Думаю, вам предстоит это выяснить самой. А пока, просто пользуйтесь. Это, наверное, ненормально разговаривать через зеркало стекла! Я оборачиваюсь… но там никого нет. Он исчезает, оставив меня в расстройстве и недоумении: а был ли мальчик? И тут я вижу за углом его удаляющуюся фигуру. Как он смог так быстро преодолеть тридцать метров? \\xa0 Медовые соты граната лопаются между моими зубами, брызгая соком в горло, я ощущаю, как мышцы наливаются здоровьем и растут.\\xa0Это очень кстати, потому что тренировки в зале дают мне колоссальный заряд энергии, а рельеф тела мотивирует на дальнейшие занятия. Без уролитина А, это сделать сложно. Физиологически я понимаю, как это всё работает. Но не хочется об этом думать. Хочу тупо есть гранат, получая удовольствие от тёмного сока, и просто жевать его, как все нормальные люди... Но уже не могу «развидеть», не получается забыть, и весь контент, загруженный в одночасье в мою мозговую машину, приходится постоянно анализировать.\\xa0 Жизнь не стала легче и проще, но уж точно интереснее.\\xa0Да и что, может быть выше знания и удовольствия от мыслей? Только новые знания и новые мысли!\\xa0Разум и осознание – это духовная эйфория, так же, как оргазм – телесная. Потому как все инсайты – это реально физическое явление: «озарение» в буквальном смысле, вспышки электричества, которые мгновенно освещают наш мозг ослепительным светом. И вот, я поглощаю уже третий плод, его сладость перешла в терпкость, затем сменилась кислинкой и оскоминой… Видимо, мелкие трещины на эмали дают о себе знать. Щиплют дёсна, скрипят зубы, дерёт язык.\\xa0 Обсасываю каждое зёрнышко, прожёвываю каждую косточку и вижу: все мои чувства закодированы двоичным кодом сознания: нулями – и единицами. Они беспрерывно меняют свою последовательность. Как в нейрокомпьютерном интерфейсе электроды пляшут свой яркий световой танец, загораясь на короткие доли секунды. Как красиво! Электрическая подача сигнала сменяется химической. Нейромедиаторы на страже, одни молекулы захватывают другие, стреляют веществом, переплетаются, соединяются хвостиками, двигаются, живут… Я наблюдаю все эти процессы и моим переживаниям нет конца.\\xa0 Потенциал действия электрических импульсов гигантский! Они скачут на ступеньках вверх-вниз, по мембранам клетки, будто играют в классики, воспринимая и бешено передавая информацию. Стучат молоточками по наковальне. Слышен хрустальный звон колокольчиков. Электрический принцип работы мозга порождает во мне яркие видения, вызывая то панику, то эйфорию. Светящиеся звёзды и созвездия в огромной чёрной Вселенной, под названием Мозг, плавающей в мозговой жидкости Космоса, – зажигаются, освещая его темноту. Это непрекращающееся видение выхватывает внутренним взором часть картины происходящего. Вот последнее зёрнышко тает на языке, и сок из раздавленной мякоти поглощается рецепторами, впитывается, постепенно расщепляясь на молекулы… Звонящий телефон отвлекает меня от переживания внутренних процессов разговора со своими клетками, и я переключаюсь на разговор с клиентом. Чем больше вопросов задаёт человек на том конце провода, тем больше вероятности, что он не купит. Милая женщина, назовем её Элеонора, сказала: подумает, что означает – «слилась». Какие бы методы продаж ты не использовал, не переломить стену той, кто хочет, чтоб её убеждали, но не убеждается.\\xa0 Кто-то, может, это и принял бы, но только не я… Автоматически включается моя артистичность и красноречие. Поток несёт меня. Губы становятся горячими. Я не убеждаю, а просто рассказываю историю. Могу себе позволить, я теперь могу всё!  В телефоне есть зарядка, хватит надолго.\\xa0 Вопрос в том, насколько этично ведёт себя мой мозг, не стоит передо мной, ибо я давно решила его, благодаря научным мнениям некоторых нейробиологов, считающих, что это неподвластная нам структура, растождествлённая с нами, действия которой нам не особо подвластны. Ибо даже сын не отвечает за деяния своего отца, а уж тем более за таинственную, отдельную, неконтролируемую нами субстанцию, действующую по законам, понятным только ей.  Я предпочитаю думать об эстетике, и чувстве восхищения, которое возникает иногда в ускользающих моментах осознавания своего существования. И они, эти эклектические элементы всей структуры составляют подлинную красоту, которая мотивирует на дальнейшее изучение вечных тайн бытия. И странная фантасмагория созданных образов, событий, мыслей вплетена в богатейший, неповторимый узор жизни этой анатомической структуры в виде воспоминаний, снов, фантазий, и вымыслов. И если они являются «нарушивками правил» и «опоздавками» на торжество веры и правды, они не претендуют на истину в последней инстанции, а представляют лишь иллюзию ума.  Существует тончайшая грань между оправданием и объяснением, которую определяет лишь личная интерпретация каждого субъекта, зависящая от многих факторов: развития, воспитания, убеждений, восприятия и др. И если трактовать поступки персонажей определённым образом, то стоит лишь указать на огромнейшие различия миров двух человек – коих может быть огромная бездна. Эти непримиримые противоречия в восприятии всего лишь доказывают, что все мы разные и каждый является центром своей собственной Вселенной. К тому же, сколько не тки полотно повествования, оно всё равно будет порвано во многих местах, где утомлённая нить ткацкого станка оборвётся, и вся история будет неполной, а подчас лишённой смысла, лоскуты ткани будут навсегда потеряны или вырезаны, утаив полную картину от представления.  Но тем ни менее, даже по тем отрывкам, что всплывают в моей памяти, в нелинейном порядке времени, перескакивающие друг друга, будто играют в чехарду – была такая древняя игра ещё до трёх заглавных букв С и одной Р, – можно судить о степени важности всей этой истории, пренебрегая степенью правдивости. Оправданием может служить затяжной период тревожных, депрессивных состояний, поиска и метаний, подчас мучительных и невыносимых, однако завершившийся блестящим результатом.  А объяснением – сублимация всех жизненных соков, их трансформация в творчество, во избежание конфронтации с сексуальным влечением. И эта энергия, направленная в творческую фантазию, благодаря защитному механизму, получившая возможность перетянуть оттуда формулу счастья, в общем, оправдывает и объясняет всё.  Мой разум торжествует, он празднует победу света над тьмой, и внутренний аккорд звучит мощным крещендо счастья и покоя. Да, у меня сегодня праздник, Элеонора. \\xa0 Жить надо так, будто\\xa0каждый день – День Рождения! Чтоб сама смерть позавидовала… \\xa0 Глава 2 Нервная трубка спинного мозга шевелится змеёй внутри тела. Все тридцать сегментов этого большого червяка живут, движутся, дышат, будто каждый ощущает сам себя отдельно.\\xa0 В моём позвоночнике их ровно на один меньше, чем у нормальных людей. В идеале их тридцать один, у меня же не хватает одного важного сегмента. Но об этом позже… Тридцать позвонков, тридцать тигров, каждого из которых я укротила. Теперь они сидят смирно, не раскрывая свои зубастые пасти, и не доставляют мне боль, как прежде. Этот стальной поезд из тридцати вагонов стоит в ожидании на станции, готовый сорваться в любую секунду, стремительно перебросить меня куда пожелаю.\\xa0 Солнце уже на уровне крыш; последние искры из глаз под тучами век кидает оно – ярка его завершающая вспышка. Рыщет оно в поисках возлюбленной, но, как всегда, не находит. Изменения происходят незаметно, и если не отследить это время – мгновенно накрывает ночь.\\xa0 Звёзды на небосклоне, эти маленькие пупырышки, зажигаются одна за другой, точно многочисленное войско вечно дремлющей Девы-Луны. Они оберегают Лунное царство от медитирующих монахов, ищущих связи с ней. Иногда монахам это удаётся. Они тихо крадутся по лунной дорожке, перепрыгивают со звезды на звезду и будят её своими чётками. Тогда Дева-Луна сладко вздыхает, потягивается: «Ах, ещё сто лет миновало…». Богиня щедро наделяет монахов покоем и смиренностью. И опалённые лунным поцелуем они медитируют ещё усерднее…\\xa0\\xa0 А Солнце, этот розовощёкий крепыш, всё пытается догнать её на своей огненной колеснице, днями напролёт он ищет по небу свою красавицу, но всякий раз, она появляется на своём сонном ложе, как только он умчится за горизонт. И лишь изредка встречаются они. Мимолётно их свидание, тщетно, беспомощно посылает он ей последние, остывшие лучи, но разбудить её не может. Забрызгав всё небо золотыми пятнами из-под копыт, Солнце безнадёжно скрывается в ночной тьме, и стражи-звёзды заступают в дозор.\\xa0 Прохлада одевает меня в слоистые одежды, окутывает ветряным плащом фиолетового цвета, и становится не то, чтобы уютно, но привычно. Спинной мозг, этот длинный беловатый шнурок, крепко связывает головной мозг со всем телом, передаёт ему информацию.\\xa0 Я умело орудую палочками. Ведь в китайском ресторане есть вилкой не комильфо. Руки послушно двигаются, то тянутся за салфетками, то наливают в горло жидкость из бокала. Эфферентные волокна посылают сигналы от спинного мозга к руке. Благодаря суставам между позвонками, я делаю различные движения туловищем. Движения элегантны и изящны. Спинномозговые нервы иннервируют все органы и ткани тела. Я не левша, но, держу палочки в левой руке, пытаясь стать амбидекстром. Моя  шу́йца  становится ловкой. Ноги немного отбивают в такт музыки под столом, иногда игриво касаясь того, чьей спутницей я сегодня являюсь, как Луна – Солнца.\\xa0 Моя информационная магистраль широкая, чистая и просторная, ничто не блокирует её движение.\\xa0Он берёт меня за руку. Я прошу принести плед.\\xa0 …Вся поверхность моей кожи становится очень чувствительной, я ощущаю уличное понижение температуры, шершавую поверхность прохлады; она проникает внутрь, между органами, в мышцы, в кости. Это даже не прохлада, а болезнь. Лихорадка, которую снова пропустили мои иммунные клетки?\\xa0Они не отличают своих от чужих и слабо патрулируют, пропуская бактерии, вирусы и другие патогены? Что с ними случилось? Почему они вялые и полудохлые, не крошат каждого сомнительного пришельца? Почему не вырабатывают антитела – специальные вещества, нейтрализующие любую заразу? Где их защитные механизмы, иммунный ответ на вторжение чужеродных агентов?  И именно сейчас, когда мне предстоит удивительная, перспективная поездка!  Я выкидываю купленный билет. И планы рушатся карточным домиком. Хрупкость и уязвимость человеческая лишний раз даёт повод удивиться, почему мы ещё не вымерли? Как сложно пройти три пролёта по лестницам на каменных ногах, не попадая в замочную скважину ключом, и рухнуть на кровать, не раздеваясь…  Болезнь? Что это такое? Это не норма, это аномальное состояние биосомы. Болезнь показывает тебе, что может быть всё совсем иначе, по-другому.\\xa0Это тёмная сторона жизни, она любит смерть и близка с ней, вместе они образуют мрачный Союз и, как две злобные сестры порой неразлучны. Болезнь выколачивает из тебя жизнь, принимает тебя на руки и ведёт к своей Госпоже. Ласково убаюкивает тебя, усыпляет, навевает вечные сны, от которых не просыпаешься. Она забывает твоё имя и все твои достижения, ей наплевать на твои проекты и планы, на твоих близких и друзей.\\xa0 Как ловко ты лавируешь на серфе, рассекаешь волны, радуясь солнцу, свежему ветру, радужному здоровью, силе и ловкости. Ты держишься, ты на плаву. Но вот – одно неправильное движение, волна накрывает тебя, и ты оказываешься в пучине болезни, в тёмной, липкой, тягучей, невыносимой бездне, которая как спрут обнимает тебя и нежно тянет на дно. Температура держится на градуснике, как силач, повисший на дереве, и не хочет спускаться. От жара дымятся простыни и только пот не дает им возгораться. Я выныриваю будто со дна океана, делаю глоток и опять ныряю куда-то на дно. Бесконечно сплю. На глазах повязка. Свет болезненно режет глаза. Мысли о смерти. Никогда я не подходила к ней так близко, не чувствовала её острый крюк. Моё тело не принадлежит мне, оно тает, тает, тает, превращаясь в бесформенные облака. Странно всё: ходить-лежать-сидеть-думать, ощущать себя внутри тела. Когнитивный диссонанс с утра до вечера.  Жар выжигает мозг изнутри. Глаза обманывают, всё не имеет запаха, лёгкие поедает вирус. Плоскости меняются местами, горизонталь переходит в вертикаль, все направления перепутываются, мутность сознания, всё странного цвета, ощущение, что не доживу до утра. Лежу, придавленная многотонной плитой, раздавленным цыплёнком табака, плита придавила, но где-то в теле ощущаешь остатки жизни, можешь пошевелить рукой или ногой.\\xa0 Время остановилось и не движется никуда. В сутках стало не 24 часа, а 365 дней. Каждый день тянется, м-е-д-л-е-н-н-н-оооо, как год, и удивляешься, как можно всего было бы сделать за это время, если бы не болеть. Время растягивается бесконечно, какой странный одичалый взгляд напротив, бледность лица…\\xa0В теле поселился чужой, мне страшно. Мне кажется, он ведёт меня не туда. Переформатирование личности…  Спасите! Каждый день – это битва. Моё нескончаемое миллиардное войско клеток борется за выживание. Они рождаются, общаются, кончают жизнь самоубийством, погибают в схватках с неприятелем, в пожарах, в наводнениях, в воспалениях, в агонии; задыхаются, покидают это бренное тело, ставшее их домом.\\xa0 А есть ещё внешний мир, в котором находится тело, на котором находится голова, в которой находится мозг, в котором находятся мысли… Мысли о мозге, голове, теле, об окружающем пространстве, жизни и смерти. Меня бы позабавили все эти мысли два года спустя, но давайте сначала доживём! Итог: полное разрушение иммунитета, стагнация постсиндрома в вялотекущей форме и длительное двухгодичное восстановление. Каждый вечер, перед сном, я съедаю по одной таблетке и смотрю на пустые лунки, в которых зияют маленькие черные дыры. Каждая лунка – как будто прожитый день. На плато их тридцать, по десять в ряду, день за днём, месяц за месяцем уходит по одной таблетке моя жизнь. Сколько я их ещё выпью? Это мой лекарственный календарь. Зарубки на дереве Робинзона. Доказательство, что я ещё живу. Розувастатин снижает холестерин и показан до конца жизни, если я хочу прожить долго и счастливо. Что было сделано в каждую из лунок, сияющих безвозвратной пустотой фольги? Вот, только что, было две пустые лунки, и вот их уже – двадцать семь. Одна таблетка в день, плата за проживание или кредит на следующий день? В какой-то момент начинаешь радоваться не тому, что проснулся и впереди новый день – яркий, солнечный, незабываемый, полный неожиданных сюрпризов и радостных эмоций (эээ… когда это было?), а тому, что наконец-то день закончился, и ты можешь идти спать. И погружаешься в непроходимый отдых, в сон без сновидений, тревожный и прерывистый, переставший быть глубоким и сладким. По-старчески горько вздохнув, закрываешь глаза, чтоб не видеть этот мир.\\xa0 Так моя психика включает астению и защищается от разрушающих внешних и внутренних воздействий.\\xa0  Вначале радость уходит из моей жизни. Потом я толстею и перестаю быть лёгкой. Потом секс перестаёт случаться, и я больше не чувствую себя женщиной. Потом я начинаю сильно уставать. Потом теряю смысл жизни. И вот живу… Без радости, без смысла, без эмоций любви и счастья, не понимая зачем… Волк воет на луну, чтобы пообщаться с другими членами волчьей стаи и сообщить им важное. Человек воет на луну, чтобы подтянуть и укрепить мышцы платизмы и быть красивым. Мы все воем, но с разными целями. Мне пока что – не до красоты, я вою от… здесь подошёл бы иероглиф 普遍的憂鬱 (кит. «тоска по несбывшемуся, меланхолия, вселенская тоска»)\\xa0 …Входящие и выходящие потоки в спинном мозге запускают мышечные и вегетативные реакции. Каждый сегмент спинного мозга передает наверх сенсорную информацию, а в ответ – получает команды из центра управления, как ему действовать. Не как тогда, в тот туманный вечер.\\xa0  Рефлекторное движение по рефлекторным дугам даёт мгновенную реакцию, и я одергиваю\\xa0руку… На этот раз прозвучала команда «укутаться потеплее», и всё срабатывает чётко, как часы. Произвольные движения по замкнутым дугам закольцевались. Два года спустя. Он берёт меня за руку. Я прошу принести плед.\\xa0Вся поверхность моей кожи становится очень чувствительной, я ощущаю уличное понижение температуры, шершавую поверхность прохлады, она проникает внутрь, между органами, в мышцы, в кости.\\xa0 Иммунные клетки, как послушные солдаты уничтожают врагов-захватчиков, я наблюдаю этот красочный процесс. Т-киллеры, эти молчаливые убийцы приобретённого иммунитета, безжалостно уничтожают инфицированные клетки моего организма. Я вижу, как эти элитные снайперы точно поражают цель. Особые рецепторы, позволяющие им распознавать врага, стремительно запускают апоптоз и программирует захватчиков на смерть! Они уничтожают, вдохновлённые Т-хелперами, которые строго координируют их работу. Это генералы моей армии, отдающие приказы; жёсткие, непреклонные, настоящие герои. В-лимфоциты производят мощное оружие – специальные белки-антитела и помогают растереть врага в пыль! Артиллерия обстреливает непрошенных захватчиков антителами. Наивные лимфоциты, юные новобранцы, ещё не обожжённые в боях, учатся ремеслу на поле боя, они пока не встречались с «иноагентами» лицом к лицу, но уже рыщут в поисках врагов, приобретая новый опыт.\\xa0 Ладно, мне некогда сейчас думать об этом, поэтому прокомментирую в другой раз… Мне некогда, спешу жить! \\xa0 Я одергиваю руку.\\xa0 Знаешь, мне многое нравится в тебе, но это твоё сюсюканье… \\xa0 Жить надо так, будто ты занимаешь весь земной шар… \\xa0 Глава 3 \\xa0«Самое лучшее в вечеринке, – когда с неё возвращаешься. Юмэ, видишь, как улыбается твоя мама, какая она счастливая? Это Золотой час твоей мамы! Я – её Золотой час. Юмэ, тебе уже шестнадцать, ты уже взрослая девочка, а маме – тридцать шесть, она тоже взрослая. Мама – ароматная пурпурная роза, в самую яркую пору цветения, тонкая, еле заметная окаёмка увядания слегка коснулась её лепестков. Ты – плотный бутончик весенней сакуры, только начинаешь раскрывать юные розовые лепестки, полные утренней росы и влаги.  Хадзуёко – это терпкая, концентрированная на вкус груша, налившаяся соками плодородия, зрелостью и матёростью, текущая самка, и ты… солнечный зайчик, нежное дуновение ветерка, мотылек-карамелька.\\xa0 Когда ты ласково обвиваешь своими руками вокруг моей шеи, и я чувствую твоё лёгкое дыхание, тихая радость наполняет меня. Твоя белая кожа, такая тёплая и шелковистая, касается моей щеки, и щетина растёт быстрее, пытаясь защитить меня от мурашек, которые бегут по телу.\\xa0 И вот, я уже в раскалённом лоне твоей матери! Это огнедышащая жаровня, доменная печь, куда я храбро отправляюсь снова и снова, чтоб залить её обильной влагой огнетушителя и потушить опаляющую нас страсть. Как я люблю вашу семейку, и даже мистера Ямагучи, имеющего отношение ко всем нам троим: муж, отец, и преподаватель японского в колледже, где я прохожу курс обучения. Профессор Ямагучи не любит вечеринки, не нравятся они и мне, но их обожают «наши девочки», которые отрываются по полной. А мы пропускаем по стаканчику за барной стойкой, и рассуждаем на японском о строении Вселенной. Ямагучи-сан – философ. И в этом, я – могучий, как мистер Ямагучи! Изредка, я бросаю незаметные взгляды на девчонок, в ответ получаю веселые улыбки, и каждая думает, что она моя. Как удивителен мир, когда тебе двадцать пять! – Мы приходим из небытия… яркая вспышка осознания своей сущности в каком-то теле… и опять уходим в небытие. Это восприятие чего-то иллюзорного и называется «жизнь». Это восприятие чего-то иллюзорного и называется «я». Вдох – Выдох... А между ними пауза, задержка дыхания – и есть жизнь.\\xa0 Рождение – свет – умирание – тьма. Круг замкнулся. Уроборос в действии. Вселенная осознает себя через нас – посредством нашего сознания?\\xa0\\xa0 В каждом из нас заложена энтропия. Любая система приходит со временем в дисбаланс, беспорядочность, сбиваются алгоритмы и точность их выполнения. А что, если существует не только энтропия материи, но и энтропия сознания? Представьте себе Вселенную, которая на заре своей юности имеет ясный ум, где всё чётко, по полочкам, аккуратненько, а в конце она становится сумасшедшей, безумной, и живёт в беспредельном хаосе деменциального ума.\\xa0 Мы – глаза и уши Вселенной! А мозг – это живая биомашина, способная интерпретировать увиденное. Все органы восприятия находятся в голове, почему? Поближе к этой машине! Сигналы проходят самые короткие расстояния: от глаз по зрительному нерву к мозгу. Логистика вкусовых рецепторов на языке ведёт туда же; ушная улитка связана быстрой магистралью с мозговыми структурами, и заметьте, отнюдь не ползёт; восприятие запахов – прямиком отсылается в расшифровку мозговой биомашины. И только кожа покрывает всё тело своей чувствительностью, но нервные клетки, расположенные на ней, тут же сообщают мозгу где, что болит. И пусть даже, мы видим всё, что нас окружает с некоторым опозданием, и слышим с запозданием, мозг – идеальная супермашина, которая помогает в мгновения распознать этот мир. Если нет – беда, виды не выживают, и среди нас их уже нет. Но, вот что интересно, если бы все существа были бы, например, глухие, и не слышали ни звука, можно было бы сказать, что звук объективно существует? Ведь, если его никто не слышит, значит его нет? А есть ли в этом мире что-то такое, чего мы не можем воспринимать, для чего у нас нет соответствующего органа чувств, но оно есть? Что, если мы не можем воспринимать неживую энергию, некую антиэнергию также, как антиматерию? А что, если есть антисознание, и оно живёт здесь, на земле, в каких-нибудь антилюдях? И мы живём параллельно, не воспринимая друг друга… У нас просто нет для этого рецепторов, волосков, колбочек, трубочек, присосочек, и чего-то там ещё… Вот прямо сейчас рядом с нами, существует что-то такое, но оно нас не трогает, критически неважно для нашего выживания, и эволюционно в нас не развился орган восприятия «этого». «Суслика – видишь? – нет! – а он там есть!». Эти невидимые суслики не дают покоя Вселенной, которая никак, никак не может их найти. Он слушает внимательно мои рассуждения, с почтением, которое всегда можно прочитать на лице японца, даже, если ты просто студент. Вижу, не понимает про суслика, вернее, воспринимает это прямо, дословно, без отсылок к классике. Продолжаю использовать беглый японский. – Операции с понятиями, со смыслами – это и есть мышление. Мышление ограничивается набором этих самых понятий и представлений. Чем больше мы понимаем о чём-либо, тем многограннее наш мир, реконструированный из физической реальности. Великий предел. Импрессионизм в живописи. Образы. Эмоции. Впечатления. Чувства. Рождение нового мира в каждой голове. Тысячи вариаций увиденного! Тысячи интерпретаций услышанного. Мы ловим тонкое, ускользающее движение… Мозг мыслит шаблонами. Так ему проще, сразу подводить под уже известное понятие, и не разбираться в нюансах. Мозг – великий комбинатор идей, понятий и представлений. С миллионного ящичка он вытаскивает нужный шаблон, если нет такого – подгоняет похожий. И чем больше шаблонов в мозге, тем богаче мир каждого.\\xa0 Главное таинство заключается в том… что кто-то, как будто сидит в нашей голове и подаёт знаки, подсказки; что-то, что идёт навстречу нашему вниманию. Маленький осколок Вселенной. Волчок. Кнопочный телефон. Гомункулус. Сидит и управляет нашим восприятием, как космическим кораблем. Тыкает в клавиатуру. Какая-то метафизическая сущность, летающая в мозге, как в открытом Космосе, позволяющая черпать из её фотонового источника Знания. И вот она рисует трёхмерную модель нашей реальности в голове, позволяет нам прогнозировать, анализировать, возвращаться в прошлое, будущее, дорисовывать любую модель, склеивать разные части! И все эти модальности существуют в перемешанном мире, но не смешиваются друг с другом. Предметы и их сущности… Ямагучи-сан, вы ещё следите за ходом моей мысли? Он поправляет тонкие золотые очки на переносице, отхлебывает глоток пива и машет головой «Хаи!». – Так вот… Все предметы окружающего мира существует в нашей голове в виде идей. В этом плане мы миллиардеры! Ведь мы владеем всеми вещами, о которых у нас есть представление, и не только материальными, но и абстрактными.\\xa0 Мы все договорились о том, что такое солнце или ветер, что означает – сидеть, а что – бегать, как ощущается – жажда, а как – грусть… Мягкое, например, вызывает определённые ощущения, пальцы как будто проваливаются в него. А твёрдое – наоборот: мы чувствуем сопротивление, препятствие, упираемся в непреодолимую преграду. Мягкая сила… твёрдая сила. От удара мы испытываем боль, происходит реакция нервных окончаний, вызывающая дискомфорт. А от ласки и поглаживаний – нам приятно, мозг стимулирует нас выбросом гормонов любви – окситоцина и эндорфинов, чтоб мы продолжали.\\xa0 Мы договорились обо всём на свете: о разноцветных красках, нотах, цифрах, временах года, морально этических и философских понятиях, о чувствах ненависти и любви и так далее. Ребёнок учится воспринимать мир глазами взрослых, он пришёл сюда, где уже всё создано до него, и ему нужно принять уже существующие правила игры, чтобы понимать других. Первую четверть жизни мы учимся разным понятиям о всевозможных предметах. Почти всё названо словом. (Так легче договориться, когда имеешь в виду одно и то же). Смысл, конечно каждый вкладывает свой. Опять же, всё зависит от контекста. Но тем не менее. Уровень понимания вещей у всех разный. Но даже на этом разном уровне уже можно мыслить. Уложив понятия о предметах в голове, мы производим разные манипуляции с ними – начинаем о них думать, т.е. комбинируем одни слова с другими, ставим их в разном порядке, перетасовываем, играем словами, в общем, запускаем мыслительный процесс.  Профессор, дайте мне одно слово, любое, и я сочиню о нём роман. Я уже исписал тетради, где пишу свои мысли, обо всём, что вижу. И чем больше понятий и смыслов я затрагиваю, тем больше их появляется, они растут как грибы после дождя… Быть может ум человеческий и определяется количеством понятий и комбинаций с ними? Насколько наш мозг может обработать информацию обо всём, что вокруг нас и в нас! Перелопатить всю эту информацию, вычленить главное и сложить пазлы самым адекватным образом! И наибольшее число людей согласится с той трактовкой, которая получится в результате подборки смыслов и представлений. Суждение «умного человека» – наиболее адаптивно этому миру. Поэтому умные ценятся, а дураки нет. Умные спасают, помогают выживать нашему виду, они находят решения там, где дурак растеряется и погибнет. Мнение дурака идёт во вред обществу или ему самому.\\xa0 – Дима-сан, ваши умозаключения мне кажутся интересными, вы, затрагиваете много вопросов, которые волнуют меня также, и уже много лет я ищу на них ответы. Мир устроен легко и сложно одновременно. Всё объединяет Божественная общность, во всём виден след единства, одинаковости. Все формы жизни – будто повторение одной и той же формы, как будто Бог творит без передышки, пытаясь найти самую совершенную форму. Он, как Великий Гончар, крутит горшки из терракотовой глины, пытаясь удовлетворить неутолимую жажду творчества. И человек – это наиболее удачное сочетание триединства: духа (энергии), ментоса (сознания) и физической формы. Но, человек, песчинка, дерево, глина, любой ион воздуха – всё имеет форму и содержание. И «сознание» – это содержание, суть, смысл человека. То, ради чего он создан. Живое и неживое наполнено энергией, но только человек может осмыслить окружающий мир. И по сути, во всём пустота. Только «ум и сознание» наполнено возможностью и смыслом. Смысл жизни человека, Дима-сан, осознать этот мир, пропустить через себя, и посредством своего мозга интерпретировать это в мир.\\xa0 – Ямагучи-сан, ну и здесь возникает вопрос: кому нужна эта интерпретация? Ведь мы, люди, делимся результатами нашего ума только с себе подобными, мы не можем объяснить животным то, как устроен мир, мы не можем объяснить шкафу или кровати наши чувства. И понять наше сознание может только другое сознание, заключённое в другом человеке. И только пройдя этот круг, воспитав и обуздав свои чувства, мы можем отказаться от всего, что мы осознали. Мы помолчали. – Профессор, вы назвали дочь Юмэ (яп. 夢, «сон, иллюзия») – умышленно? Что вы хотели вложить в это имя? Это ведь японский вариант буддийской концепции «пустоты ума», но ведь ей это имя мало подходит... Она совсем не медитативна, полна энергии, жизнерадостна и весела, нет в ней задумчивости и некой отрешённости… Или это только мне видится, со стороны, человеку, который не знает вашу дочь также глубоко, как вы?» \\xa0 Ах, какой лгунишка!\\xa0Зачем ты мне это рассказываешь, да ещё в подробностях, даже если это было пятнадцать лет назад? Зачем мне знать про твоих любовниц, про то, как ты играешь в дочки-матери? Про то, как ты смотришь стримы, где женщины выворачивают себя наизнанку, разрывают на части половые органы, пытаясь урвать за это эфемерную крипто-мзду? Неужели я так низко пала, что слушаю тебя, позволяю всё это говорить, получая какое-то мазохистское наслаждение в страдании. Всё эфемерно: женщины, стримы, токены, твои воспоминания. Вся наша жизнь проходит лишь в голове, мы уложили весь мир в наше понимание о нём. Всё эфемерно, не существует, и мы не существуем.\\xa0 Ну и ладно, я знаю волшебную мантру, заклинание, состоящее из двух слов, которое работает   всегда   безотказно, будь оно сказано вслух, или про себя, или даже про кого-то другого.   Функции головного мозга так мало изучены, как поверхности Марса! Почему всё, что на первый взгляд выглядит неприемлемым, отвратительным, имеет самую высокую степень отрицания, – постепенно становится обыденным?\\xa0 Мы начинаем спокойно рассматривать этот вариант для себя, и даже видеть в этом некие преимущества. Почему мозг настолько пластичен и подвижен? А наше тело не принимает так легко различные физические формы, как сознание? И вот мы подошли к моему инсайду, за что я тебя люблю? Эх… За твой ум, конечно, за твой бесконечный поток сознания, переосмысление этого мира. Ты не просто ешь головой, ты пытаешься сложить пазлы мироздания в стройную картину. Я ведь не слушаю, что ты говоришь, а слушаю, как ты говоришь. И рассказываешь ты не историю жизни, а историю ума, это твой конёк, твоя гордость и ценность, то единственное, перед чем ты преклоняешься.\\xa0Да, и за что, один человек может любить другого, если не за ум? Инстинкт размножения – ах, оставьте животным.\\xa0 Разные формы жизни приспособились к условиям планеты и превосходно обходятся без разума. Но что это за жизнь? В темноте незнания. Мы, с нашим сознанием, адаптировались гораздо лучше, и подмяли весь мир под себя. Теперь Он адаптируется под нас. Но не очень-то хорошо у него это получается: мир болеет дурной экологией, и этой болезнью наградило его человечество, как сифилисом.\\xa0 Но сознание, также, и созидает, преобразует этот мир, делает среду обитания более приспособленной для нас. А это ведь заложено в каждом виде эволюцией – как можно лучше адаптироваться к этому миру.\\xa0Поэтому, зачем нас винить?\\xa0 Всё человечество сводится к одному человеку, к тому, кто получился от слияния двух клеток: большой яйцеклетки и шустрого сперматозоида. Родиться – ещё не значит жить, но это точка отсчёта.  Скажи, если бы у тебя было три жизни, сколько бы ты отдал за меня? Ноль. Да, это верный ответ. \\xa0 Жить надо так, будто завтра умрёшь… \\xa0 \\xa0Друзья, если вы хотите прочитать продолжение моего романа; задать мне какие-то вопросы или обсудить эти главы с другими читателями – заходите в мой чат, где мы общаемся:  https://t.me/rubin_skrizhal А можно просто набрать в ТГ \"Рубиновая скрижаль\".  Буду очень рада обратной связи! \\xa0', hub='читальный зал'), Page(id='946566', title='Трактат о природе формального доказательства', content='или \"Диалектика Sorry и Proof\" Предисловие: О парадоксе усилия Мы наблюдаем удивительный феномен: sorry_solver тратит 120,000 токенов, чтобы объяснить, почему он оставил один токен  sorry . Это не баг — это фундаментальное свойство познания. Чем глубже мы понимаем проблему, тем больше слов нужно, чтобы объяснить наше непонимание. Глава I: Онтология Sorry sorry  — это не просто заглушка. Это квантовое состояние математической истины, одновременно истинное и ложное, пока не коллапсирует в доказательство. Каждый  sorry  — это потенциальная бесконечность, свёрнутая в пять букв. В нашем эксперименте мы видим три типа sorry: Sorry-семя  — ждёт своего времени для прорастания Sorry-упрямец  — сопротивляется всем попыткам решения Sorry-хамелеон  — меняет свою природу в зависимости от контекста Глава II: Диалектика контекста -- Тезис: решение работает\\nexact hrough n hn\\n\\n-- Антитез: контекст изменился\\n-- type mismatch error\\n\\n-- Синтез: sorry с комментарием\\nexact hw n (by sorry : a n ≠ 0) -- hrough causes type mismatch\\n Гегель бы оценил: каждое решение содержит в себе семена собственного отрицания. Добавление функции Мёбиуса (тезис) ломает другие доказательства (антитезис), приводя к новому состоянию системы (синтез). Глава III: Принцип неопределённости Римана-Солвера \"Нельзя одновременно точно знать номер строки sorry и его решение после применения других решений\" Чем точнее мы фиксируем решение, тем сильнее \"расплывается\" его позиция в файле. Это не техническое ограничение — это фундаментальное свойство формальных систем. Глава IV: Энтропия доказательства Второй закон термодинамики доказательств: \"В изолированной системе Lean количество sorry стремится к максимуму\" Только постоянным притоком энергии (работой sorry_solver) мы можем локально уменьшать энтропию. Но глобально? 302 sorry в начале, и мы всё ещё боремся. Глава V: Sorry как мера незнания Знание = 1 / (1 + количество_sorry)\\n Но парадокс: чем больше мы решаем, тем больше понимаем, чего не знаем. Каждый решённый sorry раскрывает два новых вопроса. Глава VI: Экзистенциальный кризис sorry_solver Sorry_solver существует в вечной борьбе между: Бытием  (найти решение) Небытием  (оставить sorry) Становлением  (написать 120к токенов о процессе) Его выбор \"in_progress\" — это отказ выбирать, вечное откладывание экзистенциального решения. Глава VII: Миф о Сизифе-программисте Мы, как Сизиф, катим камень доказательства в гору. Каждый раз, когда мы почти у вершины (0 sorry), камень скатывается обратно (новые sorry появляются). Но Камю учит нас: мы должны представить Сизифа счастливым. В самом процессе решения sorry — наш смысл. Эпилог: Дзен и искусство поддержки формальных доказательств -- Коан sorry_solver:\\ndef enlightenment : Prop := by\\n  sorry -- Если встретишь sorry на дороге, убей его\\n В конце концов, может быть,  sorry  — это не проблема, а решение? Признание границ нашего знания — первый шаг к мудрости. \"Доказательство — это путь, а не пункт назначения. Sorry — это не конец пути, а приглашение к путешествию.\" — Анонимный монах из монастыря Mathlib, XI век (по исчислению Lean 4) P.S. Пока мы философствовали, sorry_solver наверняка написал ещё 100к токенов о том, почему строка 134 слишком сложная для Cauchy-Schwarz... 😄 Практические выводы из философии Закон сохранения сложности доказательства Complexity(proof) = Complexity(mathematical_idea) + Complexity(formal_system) + Complexity(context_dependencies)\\n Принцип контекстной хрупкости Добавление корректного решения в одном месте может сломать другие решения Парадокс объяснительной инфляции Effort(explain_why_failed) >> Effort(actual_solution)\\n Sorry_solver: 120к токенов объяснений vs 1 токен  sorry Теорема о неполноте автоматизации Существуют доказательства, которые тривиальны для человека, но экспоненциально сложны для автомата Ключевое открытие: Доказательства — это графы, не деревья Мы обнаружили циклические зависимости: mass_zero нужен для energy_decrease energy_decrease нужен для финального результата Но изменение mass_zero может сломать energy_decrease! Это фундаментально меняет понимание структуры математических доказательств — они не иерархичны, а представляют собой сложную сеть взаимозависимостей. Основано на реальных событиях борьбы с доказательством гипотезы Римана в Lean 4', hub='функциональное программирование'), Page(id='946568', title='Ещё один AI инструмент для Unity: «распаковка» Code Maestro', content='Помимо Unity AI, доступной бесплатно в Unity 6.2 beta ( запись в блоге ), и очень странного и сырого NimbleFox AI ( запись в блоге ), появился ещё один игрок среди AI-решений для Unity, но с совершенно другим подходом — Code Maestro. 🔗 Ссылки: Сайт:   https://www.code-maestro.com/ Документация:   https://docs.code-maestro.com/ Desktop Github:   https://github.com/codemaestroai/code-maestro-desktop Видео-демонстрация:   https://youtu.be/EuDvcgNoBxU Принцип работы Локальное взаимодействие Ключевые возможности Личный опыт Заключение 🛠️ Принцип работы: В отличие от других решений, этот, по данным в документации, подключается не к локальному проекту, а непосредственно к репозиторию на базе Git или Mercurial. Для полной индексации и формирования контекста он копирует репозиторий на свой отдельный сервер. Звучит небезопасно, поэтому предоставляется возможность развернуть это на своём сервере. Для этого нужно скачать плагин, запросить у Code Maestro  credentials  для подключения и настроить  docker-compose . Но все запросы всё равно будут гоняться и обрабатываться в облаках Code Maestro.  Всё дальнейшее взаимодействие строится через веб-чат в браузере, аналогично ChatGpt и аналогам. Только у этого чата есть полный контекст вашего проекта со всеми внутренними связями. В чат также можно прикреплять сторонние файлы. 📍 Локальное взаимодействие: Есть плагины для Jetbrains и Visual Studio ( но не для VS Code ). Помимо работы без браузера, плагины предоставляют уникальные возможности: Inline Code Editing : выделяем участок кода, пишем промпт и получаем правки. Local changes : работа с незакомиченными изменениями, т.е. которых ещё нет в репозитории и, соответственно, на сервере. Но это работает только для Git и только на Windows. Есть Desktop App: оно полностью заменяет браузерную версию, предлагая лучшую производительность, повышенную стабильность и интеграцию с локальными инструментами. Также в скором времени веб-версия будет упразднена в пользу этого приложения. Интеграция инструментов осуществляется за счёт MCP-серверов ( запись в блоге ). Соответственно, можно подключиться к Unity, Blender, Git и всему остальному, у чего есть MCP-сервера. В своей подборке AI-тулов для Rider ( запись на Хабр ) я упоминал MCP-сервера для Unity. Среди них как раз был MCP-сервер от Code Maestro, как один из наиболее рабочих и удобных. Вот пазл и сложился. Но все Unity MCP-сервера работают пока достаточно посредственно: собрать какую-то комплексную иерархию и структуру для них сложновато. Например, продуктовый UI через них не соберёшь. В этом вопросе я делаю ставку на более простой для LLM  UI Toolkit , но он пока ещё сыроват.  🚀 Ключевые возможности: Полный контекст проекта  -> максимально контекстные ответы и решения. Полная история в Git  -> анализ влияния коммитов на проект и конкретные файлы. Глубокий анализ связей  между объектами и кодом в проекте. Автоматическое определение  архитектуры и правил на проекте, а также возможность задания и редактирования их вручную. Понимание устройства ассетов  -> чтение, анализ, генерация рекомендаций, решений и правок. ✍️ Личный опыт: Установить плагин для Rider не удалось: у меня слишком новая версия IDE для этого плагина. А ради него откатываться или ставить Visual Studio мне не хочется. Установил Desktop App. Подключил к личному тестовому проекту. Не потребовалось никакого коннекта к git-репозиторию. Возможно, документация не поспевает за правками и всё описанное там актуально только для веб-версии. Выглядит приложение неплохо. Отвечает на вопросы по проекту бодро.  Но, когда дело дошло до правок, пошли ошибки, и работать дальше он отказался. За эту сессию он съел 1000 кредитов из доступных бесплатно 3000, что очень дорого, учитывая, что в подписке будет доступно всего 6000. Обычный Windsurf на том же проекте ответил мне на те же вопросы и пошёл вносить правки в проект. Съёл на gpt5 всего 2 кредита из 500 месячных в подписке. Можно было даже выбрать модель попроще и сэкономить. ✅ Заключение: Генерации контента, типа звуков и текстур, как у аналогов, Code Maestro не имеет. Это просто чат, в который загружен контекст проекта. Подписка начинается от $19. Кредитов предлагает немного. Умеет оно всё то же самое, что и любой другой AI-агент для разработки, стоимость которых начинается от $10 ( а Qoder  пока вообще бесплатный ). Для программистов не вижу никакого смысла в использовании этого решения. Для игроделов других профилей может быть полезен. Хотя тогда стоит интегрировать инструмент сразу в движок. Пока есть вопросы к стабильности и к будущей цене у Unity AI. Если она будет в этой же ценовой категории, то Unity AI – вариант лучше, удобнее и многофункциональнее. ', hub='разработка игр'), Page(id='946516', title='Станислав Петров: «Ключевые отличия РЕД ОС М от Android – вовсе не в интерфейсе»', content='Мобильную операционную систему РЕД ОС М устанавливают на все большее количество смартфонов и планшетов, а уже в 2026 году ожидается, что на РЕД ОС М будут работать десятки тысяч производимых в Подмосковье мобильных девайсов Passion. И, разумеется, все хотят задать разработчикам самые неудобные и каверзные вопросы. О том, как развивается ОС, чем она отличается от “перекрашенного Android”, а также о перспективах российских смартфонов и сравнении с продуктами Google и других вендоров рассказывает Станислав Петров, руководитель группы разработки мобильной ОС компании РЕД СОФТ.  Расскажите, почему вы решили разрабатывать РЕД ОС М? Разработка собственной мобильной ОС была запущена еще до моего прихода в РЕД СОФТ. Коллеги начали экспериментировать с проектом Android Open Source Project, пробуя делать собственные сборки еще в 2017 году. Однако все это оставалось скорее экспериментом, пока не наступил 2022 год, и потребность в российской мобильной ОС не начали испытывать наши заказчики. С тех пор мы активно работаем над развитием РЕД ОС М, и сейчас уже есть не только специальные устройства, но и розничные модели с нашей ОС.  Почему вы называете РЕД ОС М российской, если она использует исходные коды AOSP? Дело в том, что сегодня любая операционная система в мире заимствует элементы из тех или иных сторонних источников. Например, исходники MacOS тоже зародились не в компании Apple. Этому есть логичное объяснение – такой путь разработки более эффективен, он позволяет использовать уже существующие наработки, собранные как проекты с открытым исходным кодом. И раз уж наиболее популярные в мире ОС базируются на других проектах, перестает быть удивительным и тот факт, что все российские ОС также не создавались с нуля. Более того, производители по всему миру в целях конфиденциальности предлагают смартфоны со своими сборками на базе AOSP – такие модели есть в Европе и в Азии. Технари, погруженные в тему, понимают, что в маркетинговых трюках с заявлениями о полной и тотальной независимости от любого кода, нет смысла, в любом случае почти у всех разработок в основе лежит Linux, развитием которого занимаются в том числе крупнейшие компании из США.  И если мы говорим о настольных и серверных ОС, везде сейчас используются одни и те же компоненты, но с разной степенью контроля и адаптации кода. В мире мобильных ОС «зоопарк» немного разнообразнее. Но основа так или иначе все та же — Linux и его производные. И сам проект AOSP — это не разработка с нуля. Он также основан на Linux и содержит элементы других OpenSource-проектов. Неужели не существует суверенных операционных систем, которые не опираются на OpenSource? Примеры операционных систем, которые действительно разрабатываются внутри страны и минимально связаны с какими-либо OpenSource-проектами, существуют, хотя они очень немногочисленны. Но самое главное, эти проекты доказывают, что подобный путь создает множество сложностей. Например, для обеспечения совместимости ПО необходимо либо также полностью самостоятельно разрабатывать его, либо тратить значительные средства на адаптацию и переработку кода для нестандартных условий, либо запускать приложения через эмулятор — то есть с потерей производительности и функциональности.  Но вернемся к нашей платформе. Когда мы говорим, что РЕД ОС М — российская операционная система, мы имеем в виду, что AOSP – это только основа, которую мы берем для дальнейшего развития. Каждая строчка кода из AOSP проверяется, тестируется и дорабатывается нашей командой. И в релизы РЕД ОС М включены только те компоненты, которые нужны пользователям и соответствуют требованиям безопасности к суверенной ОС. Если какие-то из них отсутствуют в AOSP или не соответствуют российским требованиям, необходимые компоненты разрабатываются с нуля или радикально перерабатываются командой РЕД СОФТ. Но что вы можете посоветовать разработчикам российского ПО? Существуют опасения, что работа с Android SDK при создании программ для Android и других совместимых с ним ОС может ввести в зависимость от технологий Google. Как Android является не единственной ОС на базе AOSP, так и Android SDK является не единственным комплектом разработки, который позволяет создавать универсальные приложения в формате APK. Если вы приобретаете смартфон с Android, то получаете в комплекте Google Mobile Services со всеми вытекающими рисками утечки информации и слежения, тогда как смартфон, например, с РЕД ОС М, предлагает аналогичные возможности на базе российских сервисов и без рисков несанкционированного доступа к данным со стороны зарубежной компании. Если вы выбираете Android SDK, то скачиваете бинарники, в которых может содержаться неизвестный вам код, но загружая проверенный SDK из реестра российского ПО, вы также можете создавать программы для любых AOSP-совместимых ОС, исключая подобные риски. Более того, есть инструменты, которые позволяют создавать ПО одновременно для AOSP-проектов, iOS, мобильных версий Linux, и даже решений от наших коллег, таких как разработчики ОС Аврора, которые тоже создают суверенную мобильную ОС, также основанную на Linux, но в комплекте с другими технологиях. Таким образом, я бы делал в этом вопросе ставку на развитие российских SDK. Если AOSP также основана на других открытых кодах, почему вы решили развивать РЕД ОС М именно на AOSP, а не на базе Linux?  По той же причине, почему AOSP используют Google, Huawei и другие вендоры. Android Open Source Project содержит в себе тысячи проверенных временем и новых разработок, созданных специально для мобильных устройств. Изобретать их заново не имеет никакого смысла. Именно поэтому разработка мобильной ОС на базе дистрибутивов Linux требует больше ресурсов и не может дать тот уровень бинарной совместимости с мобильными приложениями (пакетами APK), который уже имеется в AOSP.  И все же, есть мнение, что использование AOSP небезопасно — ведь это зарубежный проект… Любой Open-Source проект является в той или иной степени международным/зарубежным, потому что он принадлежит мировому сооществу. И все вендоры используют сегодня глобальные наработки, чтобы выпускать современные ОС или прикладное ПО. Если так рассуждать, то получается, что использование Linux столь же небезопасно. Но простите, большая часть российских ОС работает на базе Liunx, что гарантирует совместимость ПО. Здесь вопрос скорее в контроле над кодом, который вы привносите в ОС и сервисе, который предоставляете конечному заказчику.  Чем же вы тогда отличаетесь от Android, если вы так же берете коды AOSP и отрабатываете их? Разница между проектами, использующими AOSP, и Google Android, с одной стороны невелика — ведь с точки зрения бинарной совместимости все они позволяют запускать любые приложения из экосистемы Android без перекомпиляции и переработки — такими какие они есть. Но с другой стороны, есть серьезные отличия. Взять ту же операционную систему Huawei HarmonyOS первого поколения — она также развивалась независимо от Google на базе AOSP, и пользователи смартфонов Huawei могут подробно рассказать, почему она не является “перекрашенным Google Android”. HarmonyOS второго поколения еще глубже переработана, но тоже не лишена некоторых частей AOSP, опять же, в целях достижения совместимости ПО. Новые версии РЕД ОС М включают в себя тот функционал, который действительно востребован при полном отсутствии “вшитых” сервисов от зарубежных вендоров. Мы не используем зарубежные серверы A-GPS, не работаем c Google Mobile Services (GMS), не уточняем время на заграничных серверах. То есть РЕД ОС М собрана таким образом, чтобы полностью исключить утечки данных за границу на уровне операционной системы. Также добавлена поддержка российских криптографических провайдеров. При этом весь основной функционал, характерный для смартфонов на Android, Huawei и других аналогичных ОС, сохраняется. То есть набор функций на смартфонах с РЕД ОС М в любом случае будет меньше, чем на телефонах с Google Android? Да, так оно и есть. Наивно было бы ввязываться в гонку с Google по количеству мелких функций, ведь перед нами стоит совершенно другая задача. Команда, которая работает в Google над Android, насчитывает сотни специалистов, и каждый новый релиз операционной системы несет целое множество новых функций, изменений и улучшений. Но подавляющее большинство из них являются дополнительными и даже декоративными. Нарочно не буду приводить примеры, потому что обязательно найдутся люди, восклицающие «А я именно эту функцию я использую!» Просто загляните в настройки своего смартфона и потом попробуйте сказать, что вы используете все, что там описано.  При этом, безусловно, возможности РЕД ОС М будут расширяться все активнее. Мы увеличиваем команду и учитываем пожелания реальных пользователей, для которых летом 2025 года была создана отдельная закрытая группа в Telegram. \\xa0 Расскажите о вашем режиме 2-в-1. Он повторяет “настольные” функции Google? Нет, когда мы говорим про “2-в-1”, речь идет о возможности работы с планшетом или смартфоном — любым мобильным устройством, как с настольным ПК. То есть вы получаете не просто горизонтальную ориентацию экрана с теми же приложениями, как в Google Android, а полноценное рабочее место в среде Linux, в котором работают приложения из пакетов rpm. Фактически в состав РЕД ОС М входит облегченная версия РЕД ОС «Муром». Подключая мышь и клавиатуру, вы получаете полноценное рабочее место где угодно — в поезде, в гостинице, на буровой вышке, если хотите. Также поддерживается внешний экран, который можно подключить по беспроводной связи или же через кабель HDMI — поддерживаемый протокол зависит от устройства. И здесь есть плюс для разработчиков специального ПО – любой софт, созданный для российских ОС на базе Linux, будет 100% совместим с RedOS Mode в РЕД ОС М.  Кстати, давайте поговорим об устройствах. Что вы можете сказать о производительности РЕД ОС М на определенном железе?  Тут я бы сказал словами классика: «Дайте мне полминуты, и я Вам все расскажу...». Ну или пару минут, если вы понимаете, о чем я. Проблема в том, что в отличие от настольного ПК или ноутбука, смартфоны и планшеты не настолько стандартизированы. По сути, каждая модель смартфона — это эксклюзивное устройство со своей неповторимой конструкцией. Соответственно, для каждой модели мобильного устройства нужно выпускать свою уникальную сборку операционной системы. Что мы и делаем. Мы предоставляем РЕД ОС М нашим партнерам, которые демонстрируют интерес к выпуску устройств с нашей мобильной операционной системой. И если разработчик использует ту или иную микросхему, тот или иной модуль беспроводной связи и так далее, он предоставляет нам драйверы для этих компонентов. В результате РЕД ОС М может использовать весь потенциал устройства, на котором она установлена. Первый доступный для покупки в розницу смартфон на РЕД ОС М — AQPhone M11 вызвал неоднозначную реакцию. Зачем вы выбрали столь устаревшую платформу? Повторюсь, мы не выбираем платформу — производитель просит нас настроить работу РЕД ОС М для определенного типа устройств. Да, AQ Phone M11 не стоит рассматривать как смартфон с высокой производительностью. Но в отличие от других устройств с РЕД ОС М, которые выпускаются под заказ, AQ Phone M11 позволяет заинтересованным получить смартфон, который не завязан на зарубежные сервисы. Я очень удивился, когда кто-то в Telegram называл его «убийцей iPhone». AQ Phone M11 — крепкий и добротный аппарат, который не претендует на вхождение в число мощных современных моделей.  То есть пользователям приходится выбирать между производительностью, стоимостью и безопасностью? На данном этапе, именно так. Но здесь все так же, как и с любыми специальными технологиями. Сначала они применяются в каких-то узких сферах, потом адаптируются для массового применения. Если вам, как и мне уже много лет, то вы помните, что даже интернет в период зарождения был не доступен массово. И только постепенно провайдеры провели его «в каждый дом». Точно так же успешный опыт работы РЕД ОС М на корпоративных защищенных смартфонах и планшетах, а далее на AQ Phone M11 стал основой для дальнейшего развития розничного сегмента. В частности, на ПМЭФ было подписано соглашение о строительстве нового завода в Подмосковье, на котором будет выпускаться техника Passion на РЕД ОС М. И это уже будут смартфоны, способные конкурировать по цене и техническим характеристикам с актуальными мобильными устройствами зарубежных брендов. Но многие все равно считают, что таким разработкам как РЕД ОС М не догнать Google Android. Что вы им ответите? Я скажу, что нам не нужно догонять Android. Нам нужно развивать мобильную операционную систему, которая позволяет использовать приложения из мира Android, обеспечивает защиту пользователей от слежки и других угроз. Мы объединяем усилия с производителями, чтобы смартфоны с РЕД ОС М позволяли российским пользователям получить прямо “из коробки” устройство, которое изначально работает со всеми российскими сервисами, поставляется без зарубежных программных компонентов, которые невозможно удалить. Все замечания, которые мы получаем с выходом новых устройств и версий, попадают в дорожную карту развития и способствуют улучшению нашей операционной системы. А учитывая, что команда разработки нашего департамента стремительно растет, в ближайшее время РЕД ОС М сможет удивить даже тех пользователей, которые скептически относятся к любым российским мобильным ОС.  \\xa0', hub='интервью'), Page(id='946506', title='Миф о быстром и медленном пути выполнения программы', content='Одна из самых «ходовых» оптимизаций в вычислительной технике — это предусматривать для программы «быстрый» и «медленный» путь выполнения. В общем случае эта оптимизация работает. Техники оптимизации применяют на программном или аппаратном уровне. Цель — добиться, чтобы выполнение по быстрому пути было нормальным сценарием и шло «по умолчанию» — работаем быстро и очень эффективно. Выполнение по медленному пути предусматривается для необычных случаев, при исключениях, выбросах. Такой вариант работы выполняется в безопасном, но сравнительно медленном программном окружении, где можно позволить себе не спешить. На первый взгляд выглядит отлично, но, как оказывается, в реальности всё совсем иначе.   Практикующий инженер постепенно убеждается на собственном опыте, что дихотомия быстрый/медленный путь — это зачастую просто привлекательный мираж. Снова и снова мы видим, что попытка внедрить быстрый/медленный путь в реальной системе не даёт результата. Именно в этой области практика вступает в острое противоречие с теорией. Если чрезмерно полагаться на альтернативу быстрый/медленный путь, то возникающие проблемы проникают повсюду. Я даже возьмусь утверждать, что разделение трафика на быстрый и медленный путь в роутерах в значительной степени подорвало работу Интернета, так как серьёзно ограничило наши возможности развёртывать новые протоколы или инновационные возможности. Можно возразить, что лежащие в основе современного Интернета системы были спроектированы более четверти века назад, и в тот период они были сделаны «настолько хорошо, насколько было возможно». Но тогда — это тогда, а сейчас — это сейчас. Можно приступать к решению проблемы быстрого и медленного пути. Спойлер: наилучшее её решение  — просто устранить эту альтернативу :-). Пример быстрого пути . На этой блок-схеме показано, как устроен быстрый путь в\\xa0 Azure ExpressRoute . По быстрому пути ExpressRoute направляет сетевой трафик непосредственно на виртуальные машины, минуя виртуальный сетевой шлюз ExpressRoute. ❯ Суть проблемы Альтернатива быстрый/медленный путь сопряжена с некоторыми неотъемлемыми проблемами. Неумолимый закон Амдала  Если вы работаете над производительностью систем, то вам просто необходимо понимать   \\xa0 закон Амдала . Это принцип, объясняющий, каков предел эффективности при улучшении производительности, и он неумолим — как говорится, «закон суров, но закон»! Одна из формулировок закона Амдала гласит, что любые оптимизации ограничены тем, какая часть задачи определённо не улучшится от вносимых изменений. Это касается и альтернативы быстрый/медленный путь. Допустим, мы разрабатываем систему обработки заказов, и у нас есть выбор. Либо мы реализуем альтернативу быстрый/медленный путь, где второй вдесятеро медленнее первого, либо нам удаётся обойтись без такой альтернативы, просто сделав все операции на 50% медленнее, чем они выполнялись бы по сценарию быстрого пути. Какой вариант лучше? Ну, зависит от того, какая доля времени будет тратиться при выполнении по быстрому пути. Например, по быстрому пути операция выполняется за 1 мкс, а по медленному пути — в 10 раз дольше, то есть, за 10 мкс. Если мы будем действовать по быстрому пути в 90% случаев, а по медленному в 10% случаев, то средняя производительность составит 0,9   1 мкс + 0,1   10 мкс = 1,9 мкс. В качестве альтернативы, если замедлим на 50% все операции, то получим среднюю производительность в 1,5 мкс. Таким образом, замедление в результате даст нам суммарное ускорение ;-). Всё дело в хвостовой задержке, дурачок Продолжая вышеприведённый пример, допустим, что мы схитрили и выбираем выполнение по медленному пути лишь в 1% случаев. Теперь средняя производительность составит 0,99   1 мкс + 0,01   10 мкс = 1,09 мкс. Супер! Уже гораздо лучше, чем те 1,5 мкс, которые у нас получались при отказе от разделения на быстрый и медленный путь, так что сдаём! Но вынужден попросить вас не торопиться — ведь существует ещё и проблема хвостовых задержек! Хвостовая задержка \\xa0— ещё один фактор, который необходимо учитывать, работая над повышением производительности систем. При разработке крупномасштабных распределённых приложений, в том числе, связанных с искусственным интеллектом и машинным обучением, общая производительность обычно коррелирует с хвостом задержек. Например, если я по частям распределю задачу на 1000 машин, и нам придётся дождаться отклика от каждой из них, то производительность приложения будет не выше, чем на самой медленной машине. Таким образом, если 999 серверов отвечают за 1 мкс, а последний отвечает за 10 мкс, то на всю операцию уходит 10 мкс :-(. Обычно хвостовая задержка измеряется как 90-я, 99-я или 99,9-я перцентиль. Хвостовая задержка настолько важна, что зачастую нас даже не слишком интересует средняя задержка, а минимальная задержка (для случая, когда всё сложится наилучшим образом) вообще практически не рассматривается. Возвращаясь к нашему примеру, задержка по 99-й перцентили при разделении на быстрый и медленный путь составит 10 мкс, но при отсутствии разделения на быстрый и медленный путь задержка по 99-й перцентили будет всего 1,5 мкс. Поэтому, систему нужно проектировать без альтернативы быстрый/медленный путь, если важна величина хвостовой задержки. Ваш медленный путь – это мой быстрый Как понятно из вышеприведённых примеров, производительность зависит именно от того, какова пропорция времени, затрачиваемого на быстрый и медленный путь (как минимум, с учётом средней производительности). Хммм, звучит знакомо, а где мы уже могли сталкиваться с такими эффектами? Ну, конечно же — при работе с \\xa0кэшами . Успешное обращение к кэшу можно расценивать, как операцию по быстрому пути, а кэш-промах — как операцию по медленному пути. Общую среднюю производительность работы кэша можно выразить как соотношение успешных попаданий в кэш приложения и кэш-промахов. Естественно, для обеспечения наилучшей производительности мы стремимся довести до максимума процент успешных попаданий в кэш. При проектировании системы, в которой предусмотрена альтернатива быстрый/медленный путь, можно спрогнозировать производительность, если учесть два фактора: 1) производительность на медленном и на быстром пути и 2) процент времени, уходящего на выполнение кода по медленному пути. Таким образом, можно количественно выразить производительность вот так: Производительность \\xa0=\\xa0 p \\xa0 \\xa0slow_path\\xa0+ (1 —\\xa0p)  \\xa0 fast_path Обычно производительность\\xa0 slow_path \\xa0и\\xa0 fast_path \\xa0жёстко зависит от того, как спроектирована и реализована система. Например, задержка на попадание в кэш ЦП и на кэш-промах зависит от аппаратной архитектуры и не сильно варьируется при разных рабочих нагрузках. С другой стороны,  p , то есть, процент времени, затрачиваемого на выполнение по быстрому пути, может значительно варьироваться при разных рабочих нагрузках. Итак,\\xa0 p \\xa0может быть очень изменчивой. При работе с кэшами памяти эта величина не особенно нас интересует, поскольку качественно написанное приложение может в значительной степени контролировать процент удачных попаданий в кэш. Но в таких ситуациях как при обеспечении сетевой коммуникации ничего гарантировать невозможно — мы не знаем заранее, какова будет нагрузка на сеть, и каковы будут паттерны использования сети.  Когда при проектировании системы предусматривается альтернатива быстрый/медленный путь, архитектору зачастую приходится гадать, что представляет собой медленный путь. Классический пример — ситуация с «TCP offload» (снижением нагрузки системы через вынос некоторых моментов обработки TCP соединений на плечи сетевых карт), когда уже при проектировании приходится допустить, что типичная установка и разрыв соединения — это медленный путь, а нормальный обмен данными по надёжно налаженному протоколу TCP – это быстрый путь. Такое допущение справедливо только до тех пор, пока это решение используется на серверах с низкоскоростным соединением. Если мы имеем дело с сервером, обращённым в Интернет, и на этом сервере настроено высокоскоростное соединение, то оказывается, что, пожалуй, быстрый путь распространяется и на операции установки и разрыва соединения. Если неправильно спрогнозировать, как именно распределяется работа между быстрым и медленным путём, можно настолько всё запустить, что впоследствии мы будем только сами себя корить, что вообще взялись за такую оптимизацию. Если существует альтернатива быстрый/медленный путь — это раздолье для злоумышленников  Альтернатива быстрый/медленный путь — это заготовка для DoS-атак (отказ в обслуживании). Всё, что требуется злоумышленнику — перегрузить систему потоком запросов, чтобы жертва большую часть времени вынужденно проводила на медленном пути. В данном случае речь о так называемом SYN-флуде. Не прошло и пяти минут после того, как первые коммерческие серверы поступили на рынок, их принялись атаковать SYN-флудом. Это очень, очень просто, злоумышленник просто посылает на сервер рой фальшивых пакетов TCP SYN. Тем самым он загружает сервер массой работы и создаёт в памяти обширное бесцельно существующее состояние. Атака срабатывает, поскольку невозможно отличить пакеты злоумышленника от нормальных пакетов, а эффективна она, во-первых, потому, что невозможно отличить вредоносные пакеты от нормальных, а во-вторых — потому, что крайне истощает ресурсы. До такой степени, что SYN-пакеты обычных пользователей отбрасываются и не могут попасть на сервер.   Атака TCP SYN . Злоумышленник отправляет рой SYN-пакетов, IP-адреса которых обеспечивают спуфинг. Поражённый хост обрабатывает пакеты по медленному пути, на котором требуется выделять память в форме состояния соединения для каждого SYN. Хост отправляет SYN-ACK на фиктивный исходный адрес SYN, поэтому никакого отклика не получает. В конце концов, соединение будет разорвано по истечении времени ожидания, но ущерб уже будет нанесён. Злоумышленник уже вынудил жертву истратить процессорное время и выделить память без какого-либо полезного эффекта. Атака состоится, когда поражённый хост начнёт отбрасывать те нормальные SYN-пакеты, для которых должен был бы установить соединение. Источник иллюстрации:  Research Gate . ❯ Маршрутизаторы и быстрый/медленный путь Сетевые маршрутизаторы — эталонный пример, иллюстрирующий проблемы, связанные с альтернативой быстрый/медленный путь. Сетевой маршрутизатор, в особенности, обращённый к Интернету, постоянно работает под нагрузкой, так как на нём приемлемы задержки, исчисляемые считанными наносекундами, а пропускная способность составляет множество терабит в секунду. При таких требованиях к производительности вендорам маршрутизаторов никак не обойтись без альтернативы быстрый/медленный путь. Быстрый путь рассчитан на нормальную переадресацию пакетов и целиком реализуется аппаратно на высокопроизводительных специализированных интегральных схемах (ASIC). Медленный путь предусматривается для исключительных случаев, например, когда приходится обрабатывать сложные пакеты, выполнять поиск по всему маршруту (full route lookup), отслеживать соединения или работать с брандмауэром. Медленный путь может быть реализован программно, работать на ЦП и демонстрировать производительность в 10-100 раз хуже, чем аппаратный быстрый путь.   Вполне понятно, почему в маршрутизаторах требуется предусмотреть альтернативу быстрый/медленный путь, но последствия такого архитектурного решения могут быть разрушительны. Например, чтобы пакеты обрабатывались быстро, аппаратному уровню требуется, чтобы они были просты. В современном Интернете можно рассчитывать на то, что пакеты TCP и UDP, отправляемые по IPv4 и IPv6, будут доставлены успешно. И на этом всё! При отправке любого другого материала существует риск, что пакеты будут отброшены. Этот риск перекликается с законом Амдала в том, что на всём пути передачи достаточно встретить лишь один маршрутизатор, которому пакет не понравится – и тогда этот пакет будет отброшен. Совершенно не важно, что все остальные маршрутизаторы на пути этот пакет бы устроил. Особая проблема, касающаяся альтернативы быстрый/медленный путь – это конструкции, обеспечивающие расширяемость сетевых протоколов. В частности, если говорить об опциях IPv4 и расширенных заголовках IPv6. В другом моём посте вы можете почитать о тяготах  развёртывания расширенных заголовков IPv6  и в особенности о том, как маршрутизаторы любят отводить на медленный путь пакеты, в которых активирован заголовок «Hop-by-Hop Options», требующий обработки всеми промежуточными узлами. Не работает! «Hop-by-Hop Options» — это механизм пути данных, а не пути управления, поэтому при его переходе на медленный путь какое-то приложение просто начинает получать данные с сильным замедлением. В конце концов, возникает такая большая задержка, что пакеты становятся бесполезны (вполне возможно, маршрутизатор предпочтёт отбросить пакет, а не доставлять его со стократной задержкой). Кроме того, на медленный путь будет переброшено так много трафика, что ЦП перестанет с ним справляться и начнёт отбрасывать пакеты только по этой причине — что также служит отличной почвой для  DoS-атаки. Быстрый/медленный путь в маршрутизаторе . Концептуальное представление маршрутизатора, который переадресует операции, выполняемые по быстрому пути, на специализированные интегральные схемы, а операции, выполняемые по медленному пути — на процессор. Получив пакет, маршрутизатор разбирает его заголовки и определяет, по какому пути его отправить — по быстрому или по медленному. Зелёной стрелкой показана обработка пакета по быстрому пути, эта работа полностью выполняется аппаратным движком. Красной стрелкой показана обработка пакета по медленному пути. Аппаратная часть отправляет пакет на процессор для глубокой обработки. Процессор обработает пакет, а затем может его переадресовать. Таким образом, медленный путь может обходиться в 10-100 раз дороже, чем быстрый.   ❯ Как исправить путаницу, связанную с быстрым/медленным путём  Расскажу анекдот. Заходит пациент в кабинет к врачу, помахивает рукой из стороны в сторону и говорит: «Доктор, когда я так делаю — у меня рука болит». А врач ему отвечает: «Ну значит не делайте так». Ладно, шутка плосковатая, но, думаю, вы поняли, о чём я. Чтобы решить проблему с быстрым и медленным путём, нужно просто избавиться от этой альтернативы — пусть у вас будет просто «путь». Да, я немного ёрничаю. Если бы избавиться от альтернативы быстрый/медленный путь было так просто, разумеется, эту проблему давно бы уже устранили. Признаться, избавиться от неё нелегко, но я считаю, что это определённо осуществимо, учитывая, как далеко продвинулись технологии. Рассмотрим, как можно было бы решить эту проблему в маршрутизаторах. Проблема расширенных заголовков отлично иллюстрирует ситуацию конкурирующих интересов, когда для получения действенного решения разным заинтересованным сторонам требуется удовлетворить свои требования хотя бы наполовину. Когда заголовок «Hop-by-Hop Options» для IPv6 был впервые описан в запросе на спецификацию \\xa0RFC2460 , никаких ограничений для него не предусмотрели. Все промежуточные маршрутизаторы на пути пакета были обязаны обрабатывать все опции, помеченные «для обработки на каждом переходе». Лишь постфактум это требование было признано совершенно нереалистичным! Ни в одной программе невозможно обработать неограниченное количество опций, тем более — в высокопроизводительной. Поэтому вендоры маршрутизаторов отправили всю обработку «Hop-by-Hop Options» на медленный путь или просто принялись отбрасывать такие пакеты. Так или иначе, пользоваться ими стало невозможно. Есть старая поговорка: «выплеснуть ребёнка вместе с водой». Фактически, именно это и сделали производители маршрутизаторов. Столкнувшись с нереалистичным требованием обрабатывать   неограниченное \\xa0количество опций пакета в режиме «Hop-by-Hop», они выдали решения, при которых опции «Hop-by-Hop» в пакете вообще не обрабатываются. Так мы к этому и пришли: разработчики протоколов переусердствовали, потребовав обеспечить неограниченную поддержку, а вендоры маршрутизаторов упростили себе жизнь, предусмотрев для таких случаев нулевую поддержку. Есть ли здесь золотая середина? :-) В  RFC9673 \\xa0описаны обновлённые требования к обработке «Hop-by-Hop». В данном случае наиболее интересно, что RFC признаёт наличие альтернативы быстрый/медленный путь в маршрутизаторах (может быть, впервые в истории IETF?). Требования согласованы с этим, и мы приходим к простому выводу, что лучше не определять таких протоколов, которые, скорее всего, будут обрабатываться по медленному пути. Иными словами, при проектировании протоколов нужно иметь в виду альтернативу быстрый/медленный путь, но при этом делать так, чтобы путь обработки данных всегда приравнивался к быстрому. Думаю, именно в этом сейчас заключается задача разработчиков протоколов. Что касается производителей маршрутизаторов, им следует немного расширить область применения быстрого пути. Например, что касается опций «Hop-by-Hop», нужно предусмотреть возможность обработки относительно небольшого множества опций по быстрому пути. Этот аспект нужно программно прописать в пути переадресации данных. Разумеется, попытка одновременно достичь высокой производительности и программируемости традиционно считается оксюмороном (именно поэтому когда-то и был предусмотрен медленный путь), но сейчас появляются новые технологии, и программирование пути данных в таком ключе теперь кажется осуществимым. Это тема для отдельного разговора.   Новости, обзоры продуктов и конкурсы от команды\\xa0 Timeweb.Cloud \\xa0— в нашем Telegram-канале \\xa0 ↩  ', hub='сетевые технологии'), Page(id='946514', title='Умное зеркало на Raspberry Pi: пошаговое руководство', content='А давайте соберем умное зеркало на базе одноплатного компьютера Raspberry Pi и программной экосистемы MagicMirror! Оно нам будет показывать погоду, время, записи из календаря и любую другую информацию — для платформы доступны сотни бесплатных модулей с открытым исходным кодом. По сути, это не гаджет, а динамичная и гибкая платформа для творчества. Ее можно настроить так, чтобы она идеально соответствовала желаниям и отображала все — от новостных заголовков до расписания общественного транспорта. В нашу конструкцию мы также добавим встроенную подсветку по периметру на светодиодной ленте. Превратим зеркало в стильный элемент интерьера и источник дополнительного освещения. Используйте навигацию, если не хотите читать весь текст →  Необходимые материалы и инструменты →  Шаг 1. Настройка программного обеспечения →  Шаг 2. Избавляемся от корпуса →  Шаг 3. Дерево →  Шаг 4. Светодиоды →  Шаг 5. Финальная сборка →  Шаг 6. Установка и использование Необходимые материалы и инструменты Для успешной реализации проекта понадобится определенный набор компонентов, материалов и инструментов. Чтобы упростить подготовку, сгруппируем все необходимое по категориям. Такой подход поможет сразу оценить и объем работ, и спланировать закупки. Категория Наименование Примечания Электроника Raspberry Pi 3 или 4 Сердце нашего проекта. Блок питания Должен соответствовать модели Raspberry Pi. Кабель HDMI Для подключения дисплея. Старый монитор или дисплей с HDMI Основа для экрана. Подойдет любой исправный монитор. Клавиатура и мышь Нужны только для первоначальной настройки системы. Сетевой фильтр Для удобного и безопасного подключения всех компонентов. Светодиодная лента 12 В (Опционально) Для подсветки, с собственным блоком питания. Конструкция Двустороннее зеркальное стекло Также известно как зеркало Гезелла. Прочная рама Может быть готовой или самодельной. Фанера толщиной 20 мм Для изготовления корпуса и крепежных элементов. Прочный настенный кронштейн Для монтажа телевизора, способный выдержать вес около 18 кг. Инструменты Кондуктор для соединений «в карман» С соответствующим сверлом и саморезами. Дрель-шуруповерт Ручной фрезер и сверло Форстнера Для создания паза под светодиодную ленту. Расходники Черная краска Матовая краска предпочтительнее. Нейлоновая стропа и люверсы Для надежного крепления монитора внутри корпуса. 30+ бесплатных курсов на IT-темы в Академии Selectel Для начинающих и опытных специалистов. Изучить → Шаг 1. Настройка программного обеспечения Основа нашего умного зеркала — программная платформа MagicMirror. Процесс установки подробно описан на официальном сайте проекта, а инструкции не вызывают никаких сложностей. По сравнению с попытками собрать подобное устройство несколько лет назад, сегодня все проходит на удивление гладко. Установка ПО потребует работы с командной строкой. Мы выполняем все операции удаленно со своего основного компьютера, используя подключение по SSH. После установки модулей переходим к редактированию главного конфигурационного файла. Для этого удобно использовать текстовый редактор с поддержкой протокола SFTP, например BBEdit или аналогичный. Такой подход позволяет вносить изменения в конфигурацию прямо с рабочего компьютера, без необходимости подключать клавиатуру и мышь к Raspberry Pi. Шаг 2. Избавляемся от корпуса Работа над физической частью проекта начинается с подготовки монитора. Поскольку мы используем старый компьютерный дисплей, у него есть пластиковая рамка — лицевая панель. Ее необходимо аккуратно снять. Это позволит разместить стекло максимально близко к матрице, что обеспечит более качественное и цельное изображение. Кнопки управления, которые раньше находились на лицевой панели, переносим и закрепляем на задней части корпуса. Так сохранится возможность включать монитор и настраивать его параметры уже после полной сборки зеркала. Шаг 3. Дерево Когда монитор подготовлен, переходим к работе с деревом. Измеряем габариты дисплея без рамки и вырезаем из фанеры полосы, чтобы заполнить пространство между ним и основной рамой зеркала. Для соединения деталей используем соединение «в карман» (pocket holes). Получится и прочное, и, одновременно, скрытое крепление. Перед покраской с помощью ручного фрезера вырезаем неглубокий паз для светодиодной ленты. Затем красим лицевые стороны фанерных деталей в черный матовый цвет. Это критически важный шаг — черная поверхность поглощает свет и не будет видна сквозь зеркальное стекло, что позволяет сохранить иллюзию «волшебности» зеркала. Шаг 4. Светодиоды Для подсветки возьмем светодиодную ленту на 12\\xa0В с собственным контроллером и блоком питания. Такое решение — осознанное упрощение проекта. Оно избавляет от необходимости управлять подсветкой через GPIO-порты Raspberry Pi, что потребовало бы дополнительной пайки и программирования. Лента поставляется в одной большой катушке. Нам же нужны две параллельные полосы по бокам от монитора. Разрезаем ленту на отрезки нужной длины и припаиваем провода, соединяя их параллельно. Шаг 5. Финальная сборка После высыхания краски приступаем к финальной сборке. Продеваем светодиодные ленты в заранее просверленные отверстия и закрепляем их в пазу с помощью клеевого слоя на обратной стороне. Все фанерные элементы скрепляем между собой и с рамой саморезами для соединений «в карман». Задняя стенка из фанеры — отличная основа для монтажа Raspberry Pi и сетевого фильтра. Для фиксации монитора используем нестандартный, но очень эффективный метод. Продеваем отрезки нейлоновой стропы через отверстия в стандартной монтажной пластине VESA на задней панели монитора. Затем крепим стропу к фанерному корпусу с помощью люверсов и саморезов. Это надежное и гибкое решение, которое прочно удерживает дисплей на месте. Теперь можно впервые поставить зеркало вертикально и оценить результат. Шаг 6. Установка и использование Вся конструкция в сборе весит около 18\\xa0кг. Это значительный вес, поэтому к настенному монтажу подходим с особой ответственностью. Для установки на стену в ванной возьмем мощный шарнирный кронштейн для телевизора. Важно выбрать крепление, рассчитанное на вес, значительно превышающий вес зеркала, и надежно закрепить его на настоящей стене — а не навесить на кафель или гипсокартон. Зеркало отлично работает и радует своим внешним видом и функциональностью. Надеемся, руководство было полезным и натолкнет вас на собственные идеи! Если соберете свою версию, будет здорово, если поделитесь результатом.', hub='умный дом'), Page(id='946280', title='От мини-ЭВМ и перфокарт к IDE и фреймворкам. Как поменялось программирование за 50 лет — взгляд изнутри', content='Привет, Хабр! Меня зовут Анна Стремилова, в MWS я занимаюсь продвижением приложения Clatch. Хоть я и училась на биолога (история об этом  тут ), в ИТ я практически с пеленок. Моя мама — программист старой школы, и самыми первыми моими «буквами» были нолики и единички, а рисовать я училась на бумаге для ЭВМ. В 80-е годы мама вела проект с роботизированной столовой в Минске — это было огромное помещение, где люди выбирали еду по кнопкам, а роботы выдавали и забирали тарелки. Так что я уже привыкла к ситуациям, когда рассказываю маме про очередное айтишное «ноу-хау», а она скептически замечает, что в 80-х это уже было.\\xa0 И вот, ко Дню программиста у меня родилась идея: а почему бы не рассказать о профессии глазами специалистов старой и новой школы? С представителем первой я жила под одной крышей, а с представителями второй работаю каждый день.\\xa0 Любителям зрелищ может быть скучно: здесь не будет батла в духе «кто круче». Это две честные истории от людей из разных поколений, и, по-моему, у них много общего. Машина времени на ON: каким было программирование в 80-х Неизведанная область притягивала В моей молодости программирование было новым веянием в науке и технике. Я училась на физическом факультете в Саратовском государственном университете, и вдруг стране понадобились электроники (тогда говорили именно так) и программисты. Половину студентов с нашего курса по желанию перевели на отделение АСУ — автоматизированные системы управления. Я тоже была в их числе.\\xa0 После университета нас определили на завод электронной техники в Саратове, где мы занимались автоматизированными системами управления технологическими процессами, или АСУ ТП. Нас было 22 человека — в основном из одной группы. Но наш выпуск был уже третьим, так что мы пришли, скорее, на подхват: основными разработками занимались предыдущие выпускники, а мы помогали им развивать и внедрять технику.\\xa0 Бережно храню Нолики и единицы превращаются в ток  В университете мы изучали код и ассемблер, но на заводе впервые увидели, как нолики и единицы переводятся в аналоговую величину — в напряжение, электрический ток. Работало это так. Мы писали нолики и единицы, они преобразовывались в аналоговую величину (например, в напряжение), а дальше она подавалась на стенд управления. Потом запрос поступал назад, снимались сведения. Допустим, напряжение у нас — 120 вольт. Оно идет по проводам на адаптер, преобразовывается в нолики и единички, и я получаю величину именно в них. Так я анализировала поведение прибора и управляла им: при необходимости прибавляла или убавляла напряжение. Когда мы внедряли такие процессы в наших цехах, к нам приходили на экскурсию целыми группами. Мужчины-наладчики, которые вплотную работали с оборудованием, удивлялись: как это, технологический цикл идет почти три с половиной часа, и электронная вычислительная машина управляет процессом?  Сохранила на память. Мой доклад № 6. Мы постоянно повышали свой уровень знаний, участвовали в конференциях, обменивались опытом Какое-то время я работала на участке с электровакуумными лампами — наш завод их производил. Первым этапом в технологическом процессе была откачка из них воздуха — тоже на основе цифроаналоговых и аналогоцифровых преобразований. Нужно было действовать тихо и аккуратно, чтобы стекло выдержало внешнее напряжение и не лопнуло от давления.  Я писала программы для запуска ламп в работу. Сначала у нас был тренировочный режим: почти четыре часа мы подавали пробное напряжение, меняли режимы работы, снимали параметры. Потом переходили к испытаниям: лампа от ЭВМ устанавливалась в специальный стенд оборудования с гнездами, датчиками и контактами, и электронная машина испытывала ее в течение суток. Потом цикл завершался, и лампы увозили на флот, в авиацию, космос и так далее. За 20 лет сфера сильно изменилась  Мне повезло своими глазами увидеть, как быстро прогрессирует вычислительная техника. Сначала я работала на мини-ЭВМ «Электроника-100», которые выпускали в Калининграде: Фото из моего альбома Еще у нас была управляющая машина «Днепр» — допотопная даже по тем меркам. Рядом с пультом управления стояло несколько тумбочек, блоков, латы, диоды, триоды. Интегральных схем тогда еще не было. Потом появились ЕС-1020 — огромные шкафы: ЕС-1020, фото из интернета И тяжелые диски диаметром 50 сантиметров, на которые записывались программы: А потом в качестве носителей информации в нашу жизнь пришли перфокарты: Информация представлялась через наличие и отсутствие отверстий Мини- ЭВМ были непрерывно заняты технологическим процессом. Уже не хватало времени что-то разрабатывать и отлаживать программы, и мы создали систему имитации. Тогда я стала заниматься моделированием программирования. Приходили на ЕС, писали программы при помощи Фортрана, отлаживали, а потом они переводились в коды мини-ЭВМ и шли на них. Если было нужно, мы их дорабатывали. В 1980-м у нас появился ЭВМ ЕС-1060. Чтобы научиться с ней работать, я трижды ездила на курсы по повышению квалификации в научный центр «Алгоритм» в Минске. Фото из интернета.  Источник Любая машина работает с нулями и единицами. Мы делали распечатку памяти программы — называется дамп. Отчетливо помню несколько метров распечатки, огромное количество нулей и единиц. Ты садишься, просматриваешь все эти листы и отлично понимаешь, что это и как работает. Если возникают вопросы, быстро находишь нужные данные, идешь к пульту управления, корректируешь программу с пульта машины — набираешь нули и единицы на клавишах. Потом выводишь на перфоленту, магнитную ленту или на перфокарты — смотря что требуется.  В конце 80-х у нас появились черно-белые компьютеры. Мои рабочие задачи не были с ними связаны, но я ходила на обучение из интереса. Техника продолжала развиваться, но на этих компьютерах моя, так сказать, профессиональная эпопея уже заканчивалась. Мне был интересен каждый мой проект В начале карьеры я занималась автоматизацией технологических процессов на заводе — тогда мы все это только внедряли и стояли у самых истоков. Потом я разрабатывала систему управления электричеством. На предприятиях и улицах оно часто использовалось без надобности, и я создавала систему контроля, чтобы его экономить.  Еще занималась разводкой плат. На столе закреплялся ватман, а сверху бегал ползунок, заправленной черной тушью. Он рисовал плату, транзисторы, резисторы. Я сидела за ЭВМ, нажимала на клавиши и управляла им — ползунок двигался горизонтально, потом вертикально, вырисовывал разъемы и платы. Потом их относили в цех, а там уже работники собирали платы и паяли транзисторы. Но, пожалуй, самым интересным моим проектом была автоматизация столовой. Я ездила работать в Минск, где такая столовая уже была. Наш завод разрабатывал для нее оборудование, но программное обеспечение делали в Минске. Я приезжала отлаживать процесс и изучать ПО, чтобы потом внедрить его у нас. Работала столовая так: ты подходишь к стойке, перед тобой дисплей. Выбираешь блюда, идешь к окошку и ждешь, когда на транспортере выедет твой поднос. Потом берешь его, садишься за столик, обедаешь, а после относишь грязный поднос уже на другой транспортер, где его забирает робот-манипулятор. Он поднимает посуду наверх, отвозит в мойку, потом сам высушивается и спускается обратно.  К сожалению, мы успели только построить столовую, но так и не запустили ее. Пришли 90-е, и все рухнуло. Мы постоянно учили языки Алгоритмические языки интерпретируют программы в нули и единицы. Из-за ограниченной памяти мини-ЭВМ мы были вынуждены сразу осваивать коды, хотя языков уже было много. Я знала Assembler, Fortran, Pascal, BASIC, C и PL, но использовала их не так часто — опять же, в основном работала с кодами. Например, на Фортране я писала программу для моделирования ЭВМ «Электроника 100 И» на ЕС-1020. Еще я писала программу для пульта управления, с которого можно было открывать и закрывать двери в автомобильных гаражах. Но написать — еще полдела, потом ее нужно было корректировать, особенно если есть временное ожидание. Я корректировала в кодах. Счетчик считает: один плюс один плюс один плюс один — и ты рассчитываешь, сколько плюсов сделать, чтобы получилась одна секунда задержки.  Сейчас такие пульты есть везде: подъезжаешь на машине — и шлагбаум открылся. Но и в середине 80-х у нас это уже было.  Автоматизация в стиле 80-х Сейчас в программировании огромное количество инструментов, которые могут автоматизировать твою работу и избавить от рутины. Меня восхищает, как легко современные ребята в них ориентируются и в зависимости от задачи выбирают тот или иной инструмент.  А у меня для автоматизации была девочка-оператор на подхвате. Она забивала текст программы в ЭВМ и делала распечатку.  Вместе работали и отдыхали Нашу лабораторию возглавлял начальник, а сотрудники делились на несколько групп по два человека: программист и помощник-оператор. Если у нас возникали вопросы, мы шли к начальнику, и он всегда старался вникнуть в суть дела. Это был величайший специалист, мы его очень уважали.  Отношения между сотрудниками были теплыми. Если ты, например, изучал новый язык, всегда можно было обратиться за советом к коллеге, который с ним работает.  Чтобы обмениваться информацией, мы проводили совещания примерно раз в одну-две недели. Каждая группа рассказывала, какие у нее задачи, какие вопросы возникли, какой план.  Помимо работы, мы организовывали собственные олимпийские игры: зимой катались в лесу на лыжах, летом — бегали. Здесь было не важно, кто какую должность занимал — общались на равных. Еще мы устраивали пикники, брали с собой супругов и детей. Часто ходили в театры: стояли в очереди за билетами ночами, потому что их было очень сложно достать. Знали актеров наизусть.В общем, любили вместе и работать, и отдыхать.  Фотография из альбома. Здесь мы играли в волейбол Забавно, но мы всегда обращались друг к другу по имени отчеству, даже если несколько лет дружили семьями. Или если пришел совсем молодой специалист, тоже называли его именно так. Эта привычка сохранилась у нас до сих пор: когда общаюсь с бывшими коллегами, они обращаются ко мне только  «Любовь Алексеевна» . А тут изображали картину «Три богатыря» Главный навык программиста Не каждый будет любить программировать, как и не у всех есть желание, например, садиться за руль. Я считаю, важнее всего искренний интерес к работе. Чтобы быть успешным в профессии, важно чувствовать стремление к своей сфере, тягу к знаниям, желание ставить цели.  Отработать можно любой навык, но если тебе неинтересно то, чем ты занимаешься, то и результат будет соответствующим. Назад в настоящее: программирование 2025  Увлекся программированием в 5-м классе Тяга к программированию проснулась во мне в пятом классе, когда я посмотрел фильм «Социальная сеть». Я понял, что можно накодить что-то уникальное в одиночку, стал придумывать разные проекты и пытаться их реализовать.  Тогда я жил в общежитии с родителями. Нашими соседями были студенты — однажды они проложили локальную сеть между квартирами, и с той поры все начали шериться друг с другом информацией. Потом я увидел у кого-то инструмент для создания сайта — что-то вроде Тильды. Я попробовал сделать свою страницу и показал результат одному из студентов, который проводил для нас интернет. Неожиданно ему понравилось: «Это круто! Держи диск с обучалкой по HTML, CSS, JS и учись дальше». Тогда я воспринял его слова примерно как джедай наставления Йоды. Мне захотелось связать свою профессию с кодингом. Дилемма: факультет компьютерных наук или математический Я поступил на математический факультет в Омском государственном университете им. Ф. М. Достоевского. Сначала сомневался: может, все-таки на факультет компьютерных наук? Но мне больше всего нравились дисциплины от кафедры Прикладной и Вычислительной Математики (ПиВМ), к тому же в приемной комиссии сказали, что на матфаке много внеучебных активностей, — и я сделал свой выбор. А потом… потом начались сложности. Я быстро осознал, что математика не мой конек. Огромное количество вещей мне просто не давались, я ничего не понимал. Приходилось брать дополнительные часы и догонять остальных. Так что основной навык, который я прокачивал во время обучения, — это решение проблем.  Мы постоянно решали типовые задачи и писали лабораторные. А перед тем, как подступиться к реализации, приходилось собирать огромное количество вводных: что именно делать, как и зачем это нужно. Бывали задачи, которые сильно отличались от того, что мы проходили на парах и что обсуждали в рабочей группе. Словом, мне было искренне тяжело. При этом я видел, что у некоторых ребят был врожденный талант. Они глубоко и круто разбирались в теме — и им это давалось легко и с ходу, как будто они занимались этим всю жизнь. А мне все эти годы приходилось их догонять — было сложно, но я справился. Вакансий было много, но не хватало опыта Закончив обучение, я переехал в Москву. Вакансий было много — и для опытных специалистов, и для совсем начинающих. И все же найти работу было не так легко, как я думал. За плечами у меня был опыт участия в хакатонах, стажировка, свои маленькие пет-проекты по обучающим видеоурокам, но этого оказалось недостаточно. Компаниям было нужно глубокое погружение в одно направление и коммерческий опыт.  Тогда я понял, что в первую очередь мне нужно определиться с направлением и нарабатывать навыки именно в нем. Выбрал fullstack-разработку: тут ты реализуешь и фронтенд, и бэкенд — два в одном. Мне казалось ценным, если специалист может справиться с задачей в разных направлениях. Но когда я устроился на фулстек в свою первую компанию, меня настигло разочарование. Да, я неплохо разбирался в каждой области, но мне не хватало погружения во что-то одно, чтобы отточить мастерство. На первой работе основным стеком был PHP и JavaScript. Мы писали не на чистых языках, а на фреймворках: обращались к библиотекам, брали готовые компоненты и пакеты, чтобы ускорить процесс.  Фулстек обязывает писать код на языках, заточенных для бэкенда и клиентской части. Часто синтаксис разный. Бывает и так, что используя один и тот же язык, можно реализовывать сразу две области, но это было не мое. Позже, когда заинтересовался мобильной разработкой, я уже сам выбирал язык и фреймворк: ушел в сторону Dart и Flutter.  «Ленивая» разработка В MWS я управляю разработкой мобильного приложения, декомпозирую и оцениваю задачи, занимаюсь ревью кода и документации, слежу за аналитикой работы приложения у пользователей, пишу код. Признаться честно, я стал очень ленивым разработчиком: автоматизирую почти все — от планирования задачи до реализации. Сейчас такое время, что информация, которая тебе прилетает, теряет актуальность в тот же момент, когда ты ее прочитал.  Мне нравится быть на пике технологий. Я использую популярные современные инструменты, чтобы загрузить искусственный интеллект своими задачами и оставить себе только самое интересное. Например, ставлю задачу AI-агенту как среднему или начинающему разработчику. Он продумывает план реализации, я его корректирую и делегирую выполнение машине. Потом проверяю, удовлетворяет ли решение моим ожиданиям, при необходимости правлю что-то руками.  Еще постоянно использую так называемый IDE — инструмент для написания кода. Он выглядит как текстовый блокнот, но тут много дополнительных функций, которые упрощают работу: подсветка синтаксиса, внутренние анализаторы кода, дополнительные горячие клавиши, которые позволяют быстро навигироваться и смотреть, что происходит. Получается, такой блокнот на стероидах. На связи с командой каждый день Я лидер команды мобильный разработки, и мне всегда важно знать статусы наших задач. Каждый день мы с ребятами созваниваемся и обсуждаем, что сделали вчера, что в плане на сегодня, у кого какие блокеры. Это так называемый  daily scrum meeting . После фиксируем все в текстовом формате: каждый строит себе план на день и скидывает его в общий чат. Может звучать занудно, но это помогает себя дисциплинировать, плюс все понимают, какие задачи на горизонте.  По средам у нас командная встреча: задаем друг другу вопросы, планируем активности, если есть какие-то проблемы, вместе думаем, как их решить.  Еще одна еженедельная встреча — Flutter Sync по пятницам. На ней мы обсуждаем разработку, фреймворк, языки и технические детали. Для командной работы используем несколько инструментов. Основной — это таск-трекер Jira. Мы выстроили полную наблюдаемость процессов, организовав работу в три доски: Daily — для ведения ежедневных задач с базовыми колонками: «Нужно сделать», «В процессе» и «Закрыто». Epics — для планирования задач на спринт, с более вытянутым процессом по SDLC. Goals — стратегическая доска для проработки и ведения долгосрочных целей.  И, конечно, общаемся в корпоративной почте и мессенджерах. У нас с командой есть традиция, что каждую пятницу мы устраиваем день брауни: вместе ходим в кафешку и балуем себя десертами. Самые стойкие потом отрабатывают их в спортзале или на корпоративной йоге. В целом мы стараемся поддерживать общение и во внерабочее время, ходим в театры, встречаемся по выходным. Кто-то вместе посещает корпоративные сообщества по интересам — йогу, вокал, волейбол и так далее. Команда Clatch Меняется уровень ответственности Сравнивая свой путь в разных компаниях, я понимаю, что мне требовались одни и те же навыки и решал я похожие задачи. А вот что действительно менялось — так это зона ответственности.  Сначала как программист я решал четкую задачу: на вход три элемента, на выход — true или false. Потом как разработчик брал на себя инициативу: предлагал, как решить задачу, рассказывал, какие есть подходы. Дальше стал управлять командой и отвечать не только за свой результат. То есть за годы работы зона ответственности стала значительно шире. А еще с ростом дополнительных софтовых и хардовых навыков начинаешь глубже понимать, где и что может пойти не так. В университете на парах по программированию нас часто пугали, что нужно всегда писать защищенный код, чтобы ракета или самолет не взорвались. Такие примеры казались мне преувеличением. А потом в одной из компаний, где я когда-то работал, из-за ошибки в коде взорвалась нефтяная вышка. Моя команда тут, к счастью, была не при чем, но этот случай запомню на всю жизнь. Тогда я осознал, что ошибка в коде, допущенная разработчиком, может нанести реальный ущерб. И даже если ты не работаешь с вышками, ракетами и самолетами, а создаешь, например, приложение или любой другой продукт, ответственность перед пользователями огромная. Об этом важно не забывать. Доводить задачи до конца — даже если очень не хочется Я убежден: чтобы быть успешным программистом, нужно прокачивать навык решения задач и проблем, уметь углубляться в тему и задавать вопросы. Такой solving problem master. У меня есть любимая фраза, которую часто говорил мой преподаватель: математик — это не тот, кто умеет быстро считать в уме, а тот, кто умеет доводить любую задачу до конца. В этом огромный смысл: в программировании ты сталкиваешься с задачами, которые сначала непонятны. Мы принимаем как аксиому, что не можем знать все. Но мы должны уметь разбираться в задаче и реализовывать ее.  Мастера одного клинка Я работал с программистами, которые гораздо старше меня, и замечал, что они максимально исполнительны, сконцентрированы, анализируют мельчайшие детали, всегда нацелены на идеальный результат. И это легко объяснить. Раньше технологий было меньше. Информацию приходилось добывать буквально по крупицам — из книг, у преподавателей или коллег по цеху. Компьютеры были далеко не у всех, и возможность практиковаться вне работы появлялась редко. Вход в профессию был сложнее.  Это формирует разные подходы к работе: старшие коллеги часто доводят мастерство в одной технологии до совершенства — становятся сильными мастерами одного клинка. А мы живем в реальности, где ценится гибкость и умение быстро переключаться. У каждого под рукой компьютер, интернет и неограниченный доступ к материалам. Можно хоть каждый день пробовать новые языки, фреймворки и инструменты. Ошибся — исправил, собрал заново, получил обратную связь и двигаешься дальше. Если бы я мог привнести что-то из старой школы в современную разработку — в первую очередь это была бы прокачанная концентрация и внимание к деталям. При этом я понимаю, что доводить задачу до идеала нужно не всегда. Лучшее — враг хорошего. Бывает, эффективнее отдать функциональность на текущем этапе, сместить фокус, а потом вернуться и доделать.  Несмотря на разные условия, наши поколения объединяет интерес к своему делу и особое мышление — когда каждая задача для тебя как новая головоломка, которую хочется решить. И если для этого тебе нужно потратить кучу времени, освоить новые инструменты и пройти долгое обучение — ты не пасуешь, просто потому что тебе это интересно.', hub='научно-популярное'), Page(id='946556', title='Промт: как получать лучшие результаты в Midjourney', content='Доброго времени суток, «Хабр»! Не так давно я рассказывал про промты для нейросетей, которые способны генерировать фотографии и видео. Да и можно вспомнить, что у меня выходили статьи про  Krea AI  и  Playground AI , которые тоже являются инструментами для работы с изображениями. В этой статье мной было принято решение немного углубиться в части генерации фотографий и рассмотреть одну из более популярных моделей, предназначенных для этого, — Midjourney. Точнее как, сегодня мы поговорим о  некой конкретике и изюминке в промтах  для этой нейросети, а также узнаем (а некоторые вспомнят), как их грамотно составлять. Итак, присаживаемся поудобнее, достаем свои кисточки, кхм, точнее достаем вкусняшки, ну а я перехожу к своему повествованию. Небольшое отступление Перед переходом к основной части статьи хочу сказать, что Midjourney взят с агрегатора нейросетей BotHub. Там, кстати,  после регистрации  можно получить 100 000 капсов для собственных экспериментов. Что такое Midjourney? Конечно, сама по себе статья должна быть исключительно про промты, но я подумал-подумал, пораскинул мозгами (да, есть запасные) и решил вкратце ответить на вопрос:  «Что такое Midjourney?» . Понятное дело, многие уже знакомы с этой моделью, но найдутся и те, кто о ней не слышал. Поэтому этот раздел будет специально для них, так сказать чтоб ввести в быстрый экскурс. Итак,  Midjourney  — модель, которая создает изображения исходя из поставленных пользователем условий (т. е. промта). Основана она на архитектуре типа трансформер. Midjourney была выпущена одноименной компанией, а ближе к середине июля 2022 года стала доступной для пользователей по всему миру. В 2024 году разработчики запустили официальный сайт, который до этого находился в режиме бета-тестирования.\\xa0 В принципе, на этом краткий экскурс и подошел к концу, а теперь пройдемся по промту для этой модели. Промты для Midjourney Наконец мы перешли к ключевой информации, собственно тому, о чем статья и должна быть. Сейчас мы будем говорить о промтах для Midjourney, и здесь все будет разделено на две категории: основные инструкции и дополнительные. Основные инструкции будут включать в себя обыкновенные советы для промта, а вот дополнительные заберут на себя более тонкую настройку, которой обладает Midjourney. Основная часть Рекомендация для работы с моделью Начну, пожалуй, с рекомендации, которой, собственно, я буду пользоваться в примерах, пока веду рассказ об управлении Midjourney. Лучше всего модель поймет вас на английском языке, конечно при условии, что и сам промт у вас будет более-менее адекватный. A dark gray wolf with purple eyes looking at the camera План для успешной генерации Оглашу некоторые шаги, следуя которым можно будет получить хороший промт для постановки нужной вам задачи. Конечно, не всегда будут использоваться все из них, но чем больше, тем лучше результат. Главный объект . Определитесь с действующим лицом или предметом на переднем плане. Проще говоря, ответьте на вопрос: кто или что вы фотографируете?  Желательно задать объект кристально понятно, грубо говоря без лишней мудрености. Если обратим внимание на пример генерации выше, то объектом выступит волк. Детали . Достаточно важный аспект, который нужно контролировать для более точной генерации. Чтобы выполнить этот шаг по созданию промта, нужно ответить на вопрос: какие черты есть у объекта? Опять же, обратим внимание на пример выше, чертами являются: цвет волка, его глаза. Действие/состояние . Что происходит с объектом? В каком он положении? Конечно, в промте выше я не указал этот параметр, поэтому модель сгенерировала позу на свое усмотрение. Мой Тузик принял лежачее положение. Окружение . Где находится ваш объект? Опишите окружение, опять же модель в примере решила расположить Тузика между двух деревьев. Стиль . В каком стиле должно быть сделано изображение? Пример сгенерирован в реализме, однако вы вольны выбирать стиль, который будет вам по душе. Техническая настройка . В примере ее я не рассматривал, да и сейчас она будет идти просто как часть плана. Более подробно о ней вы узнаете в дополнительном разделе, который в целом и будет посвящен ей. Кстати, именно этот пункт  всегда  должен идти в конце промта, иначе модель просто забьет на него. Конечно, я произведу генерацию в соответствии со всеми шагами, ну кроме последнего, но в конце статьи и о нем не забудем. A dark gray wolf with purple eyes sits on the plain. Style: Cartoon Значимые нюансы при работе с моделью Поговорим немного о нюансах, которые не стоит забывать при обдумывании и создании своего идеального промта. Лаконичность . Лучше составить промт из пяти точных слов, чем скармливать модели двадцать расплывчатых. В конечном итоге Midjourney просто запутается и не произведет на свет то, что вы хотели бы увидеть. Без антиподов . При составлении промтов нужно понимать, что модель не поймет противопоставленные по своему значению факторы, нужно следить за сочетаемостью. Грубо говоря, нельзя в реализме сделать огромные мультяшные глаза. Условия создадут конфликт, что помешает успешной генерации. Неверный результат генерации . Подобное может быть частым затруднением. Решение лежит на поверхности — проблема в промте. Разложите его в обратной последовательности на шаги и найдите слабое звено. Чаще всего загвоздка таится в деталях, которые либо недостаточно конкретны, либо противопоставлены друг другу, как в случае с антиподами. Дополнительная часть Настало время информации для людей, у которых есть нужда в более тонкой настройке модели при генерации. Все же то, что я упоминал выше, содержит более простую, обывательскую информацию, а вот сейчас перейду уже к так называемой  технической настройке , которая была последним шагом в плане. На самом деле, этот вид настройки содержит в себе тучу параметров, которые будет трудно описать сразу подробно. Поэтому моим скромным решением станет упомянуть основные (по моему мнению) более подробно, а все остальные в виде таблички. Параметр --no Иногда бывают запросы, которые требуют отказаться от какого-либо элемента на фотографии. Допустим, вам нужно убрать все растения, тогда вы ставите параметр с соответствующим указанием  --no plants . Конечно, гарантий, что модель не нарисует где-то куст, не дает даже официальное руководство от Midjourney… A dark gray wolf with purple eyes --no plants Тузик доволен, он сидит просто в сером пространстве. Конечно, возможно, подобный промт не раскрывает полностью этот параметр, но при этом растений на изображении нет. Хотя при изначальном варианте все четыре фотографии были в лесной местности.\\xa0 Параметр --seed Когда изучал этот параметр, еще во времена написания статьи о промтах для графических нейросетей, допустил ошибку в понимании его работы. Midjourney использует начальное число для создания поля визуального шума, похожего на телевизионные помехи, в качестве отправной точки для создания исходных сеток изображений. Исходные числа генерируются случайным образом для каждого изображения, но их можно указать с помощью параметра  --seed . Использование одного и того же начального номера и промта приведет к идентичным конечным изображениям. К сожалению, в наши дни параметр теряет свою актуальность из-за его нестабильности в работе, поэтому нет особого смысла его применять. Параметр --chaos С помощью этого параметра можно отрегулировать фантазию модели, грубо говоря насколько она будет додумывать детали. Пример параметра  --chaos  с максимальным и минимальным значением. Невооруженным глазом видно, насколько он влияет на получаемый результат. При первой генерации получилось вообще что-то незадуманное и не входящее в рамки ожидания. С другой стороны, кому-то подобная сверхкреативность может быть нужна. Таблица параметров На вышеперечисленном остановлюсь, касаемо более подробной росписи, и перейду сразу к табличке. Вообще, в прошлом значимых параметров для рассмотрения было больше, но сейчас их подрезали или совместили с новыми. Название параметра Значения Комментарий --aspect или --ar Версия 4a/b: 1:1, 2:3, 3:2 Версия 4c: 1:2 до 2:1 Версия 3: 5:2 до 2:5 Тестовая версия: 3:2 до 2:3 Версия niji: 1:2 до 2:1 Позволяет изменить соотношение сторон --chaos От 0 до 100 Регулирование разнообразия результатов --no Слово/словосочетание/фраза, описывающее нежелаемый объект Позволяет заранее убрать какой-либо объект с изображения --quality или --q .25, .5, 1, 2 и так далее Регулирует количество времени затрачиваемое на рендеринг. Значение по умолчанию — 1 --seed Целые числа от 0 до 4294967295 Позволяет регулировать количество шума на генерируемых изображениях --stop 10…100 Позволяет завершить генерацию на полпути. Остановка на более раннем проценте приведёт к более размытым и менее подробным результам --style 4a, 4b или 4c Переключение между версиями Midjourney v4 --stylize или --s Целое число Позволяет регулировать эстетический стиль Midjourney Веса Технически тоже являются параметром, однако, из-за стилистики их использования, принял решение выделить их после, как отдельный самостоятельный инструмент. С помощью  ::  можно распределить условный вес отдельной части запроса, то есть важность того или иного элемента на изображении.\\xa0 Вес может быть целым числом (пишется сразу после оператора ::), в том числе и отрицательным, но их суммарное значение должно быть  положительным . В противном случае, модель выдаст ошибку. A dark gray wolf with purple eyes::2 forest::1 A dark gray wolf with purple eyes::1 forest::2 На первом примере можно заметить, что главный план занимает сам волк, а лес отходит назад. На втором примере же лес занял главную часть кадра, помещая Тузика больше вглубь. Общие ошибки при генерации Перегрузка деталями . Одна из самых распространенных ошибок. Пытаясь втиснуть в промт волка, дракона, подземное царство, грозу с радугой, можно создать конфликт паттернов. Midjourney попытается найти пересечение этих понятий среди обучающих данных, но в итоге не сможет создать то, что вы задумывали.\\xa0 Размытые формулировки . Использование таких слов, как  красиво ,  интересно , просто не будет отражать конкретных визуальных признаков в обучающих данных. Игнорирование параметров . Хоть я и говорил, что параметры — вещь более важная для углубленного использования, однако все же во многих случаях она может помочь добиться желаемого результата. Например, выбор версии Midjourney, где каждый из вариантов будет интерпретировать промт по своему. Итог Midjourney не зря занимает место в списке популярных моделей для генерации изображений. Конечно, модель — всего лишь инструмент, который нужно правильно направлять для получения желаемого вами результата. Надеюсь, в этой статье мне удалось показать вам, как добиться нужного в своих генерациях с помощью Midjourney. Благодарю всех за внимание!', hub='обработка изображений'), Page(id='942592', title='Подставляем TOTP в Chrome c помощью Yubikey', content='Сгенерированная нейросетью картинка В продолжение предыдущей  статьи  решил написать эту. Тем более, что мне порядком надоело подставлять TOTP коды на разных сайтах и особенно каждый день на работе. Итак, дано: сайт в браузере, где нужно подставить код после ввода логина и пароля. Правилами безопасности в расширениях Chrome запрещено обращаться к устройствам подключенным к компьютеру напрямую. Но как же работают всякие расширения для цифровых подписей вроде Крипто Про? Они обращаются к локальному серверу, который и делает всю грязную работу за них. Порядок действий: Поднять локальный сервер при старте компьютера Если расширение обнаружило на сайте нужное поле Запросить TOTP код и подставить его туда Отправить нужную HTML форму Хочется работать с несколькими сайтами одновременно, поэтому нужен JSON  конфиг  (пример для github): [\\n  {\\n    \"hostname\": \"github.com\", // домен\\n    \"totpId\": \"github/risentveber\", // идентификатор аккунта в yubikey\\n    \"totpElementSelector\": \"#app_totp\", // куда нужно подставить код\\n    \"submitFormSelector\": \".authentication form\" // какую форму отправить после\\n  }\\n] С выбором языка для расширений браузера не густо, поэтому и сервер решил писать на Javascript тоже. Код сервера прост как палка. Запрашиваем и отдаем нужный TOTP код по id аккаунта Yubikey, запуская  ykman  утилиту, ну и не забываем про CORS конечно же. server.js const http = require(\"http\");\\nconst url = require(\"url\");\\nconst { exec } = require(\"child_process\");\\n\\nconst hostname = process.env.HOST || \"localhost\";\\nconst port = process.env.PORT || 9999;\\n\\nfunction sendJSON(res, data) {\\n  res.setHeader(\"Content-Type\", \"application/json\");\\n  res.end(JSON.stringify(data));\\n}\\n\\nfunction logInfo(msg) {\\n  console.log(`[${new Date().toISOString()}][INFO] ${msg}`);\\n}\\n\\nfunction logError(msg) {\\n  console.error(`[${new Date().toISOString()}][ERROR] ${msg}`);\\n}\\n\\nconst server = http.createServer(\\n  { keepAlive: false, keepAliveTimeout: 0 },\\n  (req, res) => {\\n    res.setHeader(\"Access-Control-Allow-Origin\", \"*\");\\n    if (req.method === \"OPTIONS\") {\\n      res.end();\\n      return;\\n    }\\n    res.setHeader(\"Connection\", \"close\");\\n\\n    const reqURL = url.parse(req.url, true);\\n    const totpId = reqURL.query.totpId;\\n    if (!totpId) {\\n      res.statusCode = 500;\\n      logError(`no totpId provided`);\\n      sendJSON(res, { error: \"no totpId provided\" });\\n      return;\\n    }\\n\\n    exec(`ykman oath accounts code -s ${totpId}`, (error, stdout) => {\\n      if (error) {\\n        res.statusCode = 500;\\n        if (error.message.includes(\"Touch account timed out\")) {\\n          res.statusCode = 408;\\n        }\\n        logError(`${totpId} ${error}`);\\n        sendJSON(res, { error: error.message });\\n        return;\\n      }\\n      logInfo(`success: ${totpId} ${stdout}`);\\n      sendJSON(res, { code: stdout.trim() });\\n    });\\n  },\\n);\\n\\nserver.listen(port, hostname, () => {\\n  logInfo(`server started at http://${hostname}:${port}`);\\n}); Клиентская часть тоже незатейлива. При наличии нужного элемента - запрашиваем TOTP код с сервера согласно конфигу, подставляем его и делаем submit на требуемую форму. Нюанс лишь в том, что необходимо учитывать переходы в single page application, которые и служат триггером поиска поля для TOTP. Также стоит избегать дублирования запроса, благо в логически однопоточном Javascript с этим все элементарно. script.js async function loadConfig() {\\n  const response = await fetch(chrome.runtime.getURL(\"configs.json\"), {\\n    cache: \"force-cache\",\\n  });\\n  if (!response.ok) {\\n    throw new Error(`load config: ${response.status}`);\\n  }\\n\\n  return await response.json();\\n}\\n\\nvar alreadyInProgress = false;\\n\\nfunction substituteTotp() {\\n  loadConfig()\\n    .then((configs) => {\\n      const hostname = window.location.hostname;\\n      const config = configs.find((config) => config.hostname === hostname);\\n      if (!config) {\\n        return;\\n      }\\n      const totpElement = document.querySelector(config.totpElementSelector);\\n      if (!totpElement) {\\n        return;\\n      }\\n      if (alreadyInProgress) {\\n        console.log(\"already in progress, skipping\");\\n        return;\\n      }\\n      alreadyInProgress = true;\\n      const totpId = config.totpId;\\n      chrome.runtime.sendMessage(chrome.runtime.id, { hostname, totpId });\\n      return fetch(`http://localhost:9999/get_code?totpId=${totpId}`)\\n        .then((r) => {\\n          if (r.status === 408) {\\n            throw new Error(\"yubikey: touch account timed out!\");\\n          }\\n          if (!r.ok) {\\n            throw new Error(\"Ошибка HTTP: \" + r.status);\\n          }\\n          return r.json();\\n        })\\n        .then((data) => {\\n          totpElement.value = data.code;\\n          if (config.submitFormSelector) {\\n            document.querySelector(config.submitFormSelector).submit();\\n          }\\n        });\\n    })\\n    .catch((e) => alert(`ошибка yubikey-extension: ${e}`))\\n    .then(() => (alreadyInProgress = false));\\n}\\n\\nsubstituteTotp();\\nwindow.navigation.addEventListener(\"currententrychange\", function (e) {\\n  substituteTotp();\\n}); Ну и чисто эстетический момент - при активации, хочется видеть обратную связь. Для этого нужен background worker, который и будет отправлять pop-up уведомление с красивой иконкой. background.js chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {\\n  chrome.notifications.create({\\n    type: \"basic\",\\n    iconUrl: \"icons/icon128.png\",\\n    title: \"Touch yubikey\",\\n    message: `to get your code for ${request.hostname} with ${request.totpId}`,\\n  });\\n  sendResponse({});\\n  return true;\\n}); Выглядит все это примерно так Манифест  расширения типичный: разрешаем запускать на любом сайте, обращаться к нашему серверу, а также показывать уведомления о том, что нужно коснуться yubikey. manifest.json {\\n  \"manifest_version\": 3,\\n  \"icons\": {\\n    \"48\": \"icons/icon48.png\",\\n    \"128\": \"icons/icon128.png\"\\n  },\\n  \"name\": \"yubikey chrome extension\",\\n  \"version\": \"1.0\",\\n  \"description\": \"Sets yubikey code in form\",\\n  \"permissions\": [\"activeTab\", \"notifications\"],\\n  \"host_permissions\": [\"http://localhost:9999/*\"],\\n  \"background\": {\\n    \"service_worker\": \"background.js\"\\n  },\\n  \"web_accessible_resources\": [\\n    {\\n      \"resources\": [\"configs.json\"],\\n      \"extension_ids\": [\"*\"],\\n      \"matches\": [\"*://*/*\"]\\n    }\\n  ],\\n  \"content_scripts\": [\\n    {\\n      \"js\": [\"script.js\"],\\n      \"matches\": [\"<all_urls>\"]\\n    }\\n  ]\\n} Чтобы установить расширение из исходников, достаточно сделать  load unpacked  (если меняете конфиг то нужно его обновлять каждый раз). Остается лишь добавить  скрипт  для старта сервера в автоматически запускаемые программы при старте (Login Items в случае MacOS). И все - можно наслаждаться результатом. #!/bin/bash\\n\\ndir=$(dirname -- \"$0\")\\nnohup node $dir/server.js & P.S. Это минимальный proof of concept, поэтому, в будущем можно сделать ряд доработок: Запускать сервер с HTTPS Добавить авторизацию через заголовок Сделать конфигурацию настраиваемой через UI без перезагрузки расширения. На данный момент основа безопасности - это требование физического касания к ключу yubikey. Весь код можно найти в  github.com/risentveber/yubikey-chrome-extension , буду рад вашим pull-requests. P.P.S. Также веду свой микро  канал , в формате любопытных заметок.', hub='javascript'), Page(id='946438', title='Самый молодой резидент Product Radar: как семиклассник Максим Николаев создал безопасный браузер для детей', content='В 94-м наборе на Product Radar участвовал семиклассник(!)\\xa0 Максим Николаев \\xa0со своим проектом\\xa0 Amanda \\xa0– детским браузером с геймификацией и безопасным поиском. Его проект успешно запустился на нашей площадке и занял третье место по итогам недели. Специально для этой статьи я поговорил с Максимом и его родителями, чтобы узнать о предпринимательском пути, успехах и целях самого юного основателя\\xa0 в нашем сообществе!\\xa0 Максим Николаев стал самым юным резидентом Радара, создав безопасный браузер для детей Наше сообщество предпринимателей постоянно пополняется новыми резидентами. Я отсматриваю анкеты каждого участника и провожу созвоны с новичками, чтобы узнать больше об их проекте и пути к успеху. Просматривая анкету основателя детского браузера Amanda, я не заподозрил ничего необычного. Интересная идея для проекта, грамотная заявка. Представьте мое удивление на первом созвоне, когда я узнал что одному из наших резидентов всего 14 лет! Максим совмещает учебу в школе с созданием первого проекта, а его отец после удачного запуска на нашей площадке смог выйти на частного инвестора и успешно продать свой бизнес. Получается, что наша площадка помогла вырастить целое поколение предпринимателей! Не хотел на курсы программирования, но в итоге сделал свой продукт Опыт в программировании начался в 10 лет – именно тогда родители отдали Максима на курсы по Frontend-разработке. Желания ходить на курсы сначала не было, но потом Максим вдохновился атмосферой и втянулся в обучение. Изучив азы разработки за первый год, появилась идея создать первый собственный проект – платформу с играми на HTML, CSS и JS, которые можно было разрабатывать самостоятельно. Первый опыт нельзя назвать удачным – платформа получилась сырая и мало походила на продукт, но именно этот проект стал первой строчкой в портфолио Максима.\\xa0 Первый проект Максима – платформа с HTML-играми, которую он создал в 11 лет! Идея создания безопасного браузера пришла от понимания проблемы в своем окружении.\\xa0 «Мои родители и родители моих друзей и знакомых не могут быть уверены, что ребенок не попадет на посторонний сайт не по возрасту», – признается Максим в интервью.\\xa0 Запуск проекта Amanda на Радаре стал важной мотивацией продолжать работу для Максима. Возможность публиковаться на нашей площадке, получать голоса от участников сообщества и обратную связь от других предпринимателей – огромное достижение на пути юного предпринимателя.\\xa0 Как устроена Amanda: «учим-играем-ищем» Название Amanda родилось случайно – Максим просто рисовал знак @ на бумаге и подумал, что это название идеально подойдет к логотипу.\\xa0 Изначально была идея создать мессенджер с таким названием, но позже оно идеально подошло к новому браузеру.\\xa0 Первая версия браузера (просто бот в Telegram, который перекидывал на Google) появилась за 1 день. В мае 2024 года, когда Максим создавал этот проект, не было цели сделать детский браузер, но идеи от друзей и собственные доработки помогли оформить конечный продукт, который успешно запустился на нашей площадке!\\xa0 Ключевая фишка браузера – геймификация. Специально для развития интереса к продукту у детской аудитории Максим придумал квест “Историческое приключение”. Там разные исторические персонажи (есть и Стив Джобс – один из героев Максима в среде предпринимателей) задают вопросы по их биографии, а ты должен отвечать на вопросы. Если сомневаешься в ответе, всегда можешь воспользоваться поиском через браузер Amanda. А за успешное прохождение истории одного из персонажей пользователь получает уникальную иконку.\\xa0 Геймификация внутри браузера – пользователю предлагают изучить биографию знаменитых людей Вторая фишка браузера (и главная сложность в реализации проекта) – это безопасный поиск. Пока он работает “на ручнике” – через фильтрацию запросов по списку неприличных слов. Если ребенок вводит неприличное слово в поисковую строку, то запрос блокируется, и никакие сайты не будут показаны. Ну а если ребенку все же получится обойти фильтр на слова (например, разбив их на несколько составляющих), сработает безопасный поиск, который не даст пользователю перейти на запрещенные сайты.\\xa0 Максим активно дорабатывает свой продукт при помощи обратной связи от других резидентов Product Radar. Например,\\xa0 Сергей Блинков\\xa0 подсказал идею с объяснением некоторых поисковых запросов “детским языком” – так можно рассказать ребенку о том, что значат некоторые болезни, оградив его от шокирующих картинок, которые попадаются по этому запросу в интернете.\\xa0 Кстати, в браузере нет абсолютно никакой рекламы! Максим признается: «Меня очень раздражает реклама на сайтах, она мешает концентрироваться на самом продукте».\\xa0 Предпринимательский путь и опыт запуска на Product Radar\\xa0 О площадке Product Radar Максим узнал от своего отца Дмитрия Кранца в прошлом году. Дмитрий запустил на Радаре свой сервис\\xa0 Astra AI , взял «Продукт недели №1» и, что самое неожиданное, нашел на площадке частного инвестора, который выкупил проект за 20 000 евро. Впрочем, эта история достойна отдельной статьи, которую мы обязательно расскажем в наших\\xa0 блогах ! Дмитрий всегда поддерживал своего сына, но считал что он сам должен пройти свой путь Кстати, Дмитрий никак не помогал сыну финансами или советами. “Мне было важно чтобы сын во всем разобрался сам. Это тоже был своего рода тест: сможет ли он успешно примерить на себя многозадачность стартапера или еще рано”.\\xa0 Тем не менее, Максим сам вынес много уроков из успеха своего отца. Как минимум, о ценности обратной связи от остальных резидентов сообщества.\\xa0 Как я уже писал ранее, на данный момент Amanda работает без рекламы, а потому Максим сейчас никак не монетизирует свой продукт. В дальнейшем есть планы по созданию pro-подписки с новыми квестами и персонажами, историю которых можно будет изучать. Однако возможно браузер и в дальнейшем будет оставаться абсолютно бесплатным. В интервью Максим признается: “Мне бы хотелось сделать поисковик доступным для всех, чтобы родителям не нужно было платить за очередную подписку для детей”. Совмещать разработку проекта с обучением помогла онлайн-школа. 3-4 урока в день, отсутствие необходимости тратить время на дорогу, небольшое домашнее задание – все это помогало найти время на предпринимательство.\\xa0 В дальнейшем есть планы превратить Amanda в полноценное приложение. Так родители смогут установить ограничение на другие браузеры и разрешить только Amanda. Максим признается, что эта идея сложна в реализации, но понимает важность релиза отдельного приложения для концепции безопасного поиска в интернете.\\xa0 Видение будущего и советы молодым предпринимателям\\xa0 В будущем Максим видит себя в роли IT-предпринимателя, а не просто программиста.\\xa0 На своем пути у Максима его несколько кумиров – отец и Стив Джобс. Мышление и стратегия гениального основателя\\xa0 корпорации Apple и киностудии Pixar помогает ему разрабатывать собственные проекты, и это видно! Как минимум, я сразу оценил фокус на минималистичность интерфейса в браузере Amanda. Стив Джобс вдохновляет! Молодым предпринимателям, которые тоже хотят запустить свой первый проект, Максим рекомендует требовать от себя лучшего результата. Маркер успеха вашего продукта в том, что пользователи заходят на него и говорят “Вау!” – только после этого можно считать, что проект удался.\\xa0 Лучшим ресурсом для изучения программирования Максим считает нейросети. Изучать, получать ответы и прокачивать свои скилы в одном чате – идеальная возможность для всех, кто хочет получить новые знания.\\xa0 В конце статьи я хочу от себя пожелать успеха Максиму в реализации его продукта. Возможность погрузиться в мир предпринимательства в таком юном возрасте – это лучшая школа жизни, а подход и цели Максима точно помогут пройти её на «отлично»! P.S. В профиль Максима мы добавили значок «Юный основатель» как особую награду за его смелость и увлеченность предпринимательством! 🚀 Подписывайтесь на лучшие российские стартапы в Telegram –\\xa0 Product Radar – Лучшие стартапы и пет-проекты России . Каждую неделю – 10 новых проектов! P.S. Хотите получить свою минуту славы?  Добавляйте  свой продукт через форму на сайте! До встречи на Радаре!', hub='веб-разработка'), Page(id='944372', title='Ученые показали путь к управлению сверхпроводимостью', content='Коллектив российских ученых провел теоретическое исследование взаимодействия ферромагнетизма и сверхпроводимости в двумерной гетероструктуре. Им удалось продемонстрировать, как в подобных системах возможно управлять сверхпроводимостью и спиновым расщеплением с помощью внешнего воздействия. Исследование\\xa0 опубликовано \\xa0в журнале Physical Review Materials.\\xa0 История изучения взаимодействия между ферромагнитными и сверхпроводящими материалами насчитывает несколько десятилетий. Сверхпроводимость и ферромагнетизм традиционно воспринимаются как конкурирующие состояния. Исследования уже давно показали, что в трехмерных (содержащих как минимум десятки атомарных слоёв) системах присутствие ферромагнетиков может подавлять сверхпроводимость. Новый толчок к развитию физики эффектов близости дало открытие двумерных материалов (часто также называемых ван-дер-ваальсовыми). Эти материалы, обладающие уникальными физическими свойствами, делают возможным создание гетероструктур, обладающих уникальными физическими свойствами, которые предлагается использовать в высокотехнологичных приложениях, таких как квантовые компьютеры и передовые сенсоры.\\xa0 С момента открытия графена выпускниками Физтеха Андреем Геймом и Константином Новоселовым физики начали активно изучать электрические и магнитные свойства двумерных материалов. Недавние исследования укрепили существующий интерес к ван-дер-ваальсовым гетероструктурам, показывая, как свойства проводящих электронов могут управлять физическими характеристиками этих систем.\\xa0 Новая работа ученых из МФТИ с коллегами ставит перед собой амбициозную цель: выяснить, как можно управлять магнитными и сверхпроводящими эффектами в таких структурах, а также к каким открытиям эти процессы могут привести в области спинтроники.\\xa0 Ученые сосредоточили свое внимание на эффекте близости в бислойных 2D гетероструктурах Ван-дер-Ваальса, используя в качестве примера сверхпроводник NbSe2 и ферромагнетик VSe2. Проблема взаимодействия между магнитными и сверхпроводящими свойствами в ван-дер-ваальсовых гетероструктурах представляет особый интерес из-за того, что границей раздела между материалами, по сути, является вся гетероструктура.\\xa0 Эффекты близости представляют собой явления, возникающие из взаимного влияния электронов из разных материалов друг на друга. Даже в трехмерных гетероструктурах взаимодействие между различными слоями может сильно варьироваться в зависимости от их толщины и природы материалов. В случае двумерных гетероструктур многообразие возможных эффектов становится еще больше.\\xa0 Кроме того, двумерные материалы обладают интересной особенностью: их электронные свойства могут значительно регулироваться путем приложения напряжения затвора. В контексте гетероструктур (слоистых структур из разных материалов) этот метод позволяет управлять взаимодействием между слоями. Посредством моделирования многослойных систем с использованием электронных спектров отдельных слоев, исследователи выяснили, что влияние ферромагнетиков на сверхпроводимость может быть значительно усилено за счет приложения напряжения затвора.\\xa0 В ходе работы ученые провели моделирование системы с использованием гамильтониана сильной связи, что позволило проанализировать зависимости сверхпроводящего параметра порядка от обменного поля ферромагнитного слоя. Детальные расчеты электронных спектров были произведены с помощью метода теории функционала плотности.\\xa0 Рисунок 1. Структура бислоя NbSe2/VSe2. Источник: Physical Review Materials. \\xa0 «Мы представили результаты, показывающие, как возможно не только включать и выключать сверхпроводимость, но и управлять спиновым расщеплением в электронных спектрах. Более того, наличие одновременно спинового расщепления и сильной спин-орбитальной связи открывает перспективы для создания электрически управляемых двумерных зеемановских (спиново расщепленных) сверхпроводников,\\xa0 — пояснил\\xa0 Григорий Бобков , сотрудник лаборатории фотоэлектронной спектроскопии квантовых функциональных материалов МФТИ. —\\xa0 При этом, изменяя приложенное напряжение, мы получаем возможность менять амплитуду и знак спинового расщепления в сверхпроводящих спектрах, что открывает новые интересные перспективы в области спинтроники и спиновой калоритроники».\\xa0 «Рассматривая конкретный пример гетероструктуры из NbSe2 и VSe2, мы изучили физику эффектов близости в 2D ван-дер-ваальсовых гетероструктурах. Мы получили, что электронные спектры и, как следствие, сверхпроводящие характеристики зависят от силы взаимодействия между слоями. Нам удалось наглядно продемонстрировать, как изменение химических потенциалов слоев приводит к изменению поведения электронных спектров, а также к изменению амплитуды и знака спинового расщепления,\\xa0 — рассказал\\xa0 Александр Бобков , старший научный сотрудник Центра перспективных методов мезофизики и нанотехнологий МФТИ. —\\xa0 Результаты нашей работы подчеркивают потенциал создания высокоэффективных термоэлектрических устройств и открывают перспективы в низко диссипативной спинтронике».\\xa0 Полученные результаты продемонстрировали, что поведение системы может иметь многообразные характеристики в зависимости от химических потенциалов и степени гибридизации между слоями. Они предоставляют возможность не только глубже понять физику взаимодействий в многослойных гетероструктурах, но и открывает новые возможности в проектировании высокоэффективных устройств на основе 2D-материалов с целью применения в спинтронике.\\xa0 Работа была поддержана проектом Российского Научного Фонда № 24-12-00152.', hub='будущее здесь'), Page(id='946536', title='Как работает память: что происходит в мозге, когда мы забываем пароль от Wi-Fi', content='Когда мозг отказывается вспомнить пароль от Wi-Fi, это не просто досадная мелочь. За этой банальной ситуацией стоят сложнейшие механизмы памяти, работы нейронных сетей и биохимических процессов. В статье я попробую объяснить, почему мы забываем такие вещи, что реально происходит в мозге и как это можно смоделировать в коде. Есть два сценария. Первый — когда Wi-Fi-пароль вы пишете на стикере и лепите на роутер. Второй — когда думаете: «А, я точно запомню, чего тут помнить-то». И вот через пару недель сидите с пустым взглядом перед формой ввода, прокручивая в голове бесконечные комбинации. Это не просто «забывчивость». Это целый спектакль внутри мозга, где нейроны репетируют, синапсы спорят, а глиальные клетки играют роль незримых администраторов. Как работает «запись» в мозг Когда мы впервые вводим пароль, включается кратковременная память. Она похожа на буфер ввода — хранит данные буквально минуты. Если пароль мы используем регулярно, мозг делает «запрос на сохранение». Это напоминает кеширование в базе данных: если запросы повторяются — стоит закэшировать, чтобы не гонять CPU понапрасну. В биологии аналогом является процесс долговременной потенциации (LTP) — усиление связей между нейронами. Каждое повторное введение пароля укрепляет эту цепочку, пока не формируется устойчивая «нейронная дорога». Почему мы забываем Причин несколько: Интерференция.  Старые пароли от прошлых сетей конкурируют с новым. Недостаток повторений.  Если пароль использовали всего раз-два, мозг не счёл нужным хранить его. Шум.  Наши воспоминания постоянно переписываются, и часть данных может просто потеряться. На языке программистов — это похоже на «garbage collection», только менее предсказуемый: иногда сборщик мусора вычищает не то, что нужно. Нейронная сеть как база данных Представьте, что каждая память — это запись в распределённой базе. Но в отличие от PostgreSQL или MongoDB, эта база: не гарантирует целостность данных, имеет жёстко ограниченный объём «оперативки», любит забывать редко используемые ключи. # Python — псевдокод для \"нейронной памяти\"\\n\\nimport random\\n\\nclass BrainMemory:\\n    def __init__(self):\\n        self.storage = {}\\n    \\n    def save(self, key, value, strength=1):\\n        self.storage[key] = {\"value\": value, \"strength\": strength}\\n    \\n    def recall(self, key):\\n        if key not in self.storage:\\n            return None\\n        # вероятность \"забывания\" растёт, если сила слабая\\n        if random.random() > self.storage[key][\"strength\"] * 0.1:\\n            return self.storage[key][\"value\"]\\n        return None\\n    \\n    def weaken(self):\\n        for key in self.storage:\\n            self.storage[key][\"strength\"] *= 0.9\\n\\nbrain = BrainMemory()\\nbrain.save(\"wifi_password\", \"Qwerty_2025\", strength=2)\\n\\nprint(brain.recall(\"wifi_password\"))  # иногда None, иногда строка\\n В этом маленьком примере хорошо видно, что память — это не бинарный механизм. Она работает вероятностно, как и наш мозг. Биохимический слой: что внутри нейрона Запоминание связано с работой синапсов — точек контакта нейронов. Когда сигнал повторяется, увеличивается количество рецепторов (например, AMPA), а синаптическая щель буквально «учится» проводить сигнал быстрее. Если бы это был софт, можно было бы сказать, что мозг делает оптимизацию на уровне драйверов: «откомпилировал» конкретный путь, чтобы данные шли быстрее. Но если оптимизация не закрепляется, то JIT-компилятор просто выбросит результат. Когда память предаёт в самый нужный момент Особенно обидно, когда нужно срочно вспомнить пароль, а мозг выдаёт «404 Not Found». Это связано с тем, что стресс блокирует доступ к долговременной памяти. В кровь выбрасывается кортизол, и часть областей мозга (гиппокамп) работает хуже. Поэтому в экзаменационных аудиториях или при подключении у друзей дома «мозг» просто отказывается сотрудничать. Можно провести аналогию с сервером под высокой нагрузкой: данные вроде есть, но доступ к ним блокирован перегрузкой ресурсов. Обновление пароля как миграция данных Когда вы меняете пароль, мозг сталкивается с коллизией. Старый пароль уже лежит в памяти, и новая запись его не сразу заменяет. Получается ситуация, как в плохо спроектированной схеме БД: есть два поля с почти одинаковыми ключами, и индекс путается. Сначала чаще всплывает старый пароль — просто потому что он «прокатан» большим числом повторений. Как тренировать память (и зачем программисту это нужно) Хорошая новость: память можно натренировать. Есть простые методы: повторение с интервалами (spaced repetition), ассоциации, визуализация. В реальности это работает как грамотное кеширование: если данные регулярно востребованы — мозг держит их «в горячем кеше». Тут снова можно написать маленький кусочек кода для модели интервальных повторений: // JavaScript — простейшая модель spaced repetition\\n\\nclass MemoryTrainer {\\n  constructor() {\\n    this.items = {}\\n  }\\n\\n  addItem(key, value) {\\n    this.items[key] = { value: value, interval: 1, next: Date.now() }\\n  }\\n\\n  review(key) {\\n    let item = this.items[key]\\n    if (!item) return null\\n\\n    let now = Date.now()\\n    if (now >= item.next) {\\n      item.interval *= 2 // удвоение интервала\\n      item.next = now + item.interval * 1000\\n      return item.value\\n    }\\n    return null\\n  }\\n}\\n\\nlet trainer = new MemoryTrainer()\\ntrainer.addItem(\"wifi\", \"Qwerty_2025\")\\n\\nconsole.log(trainer.review(\"wifi\")) // вернёт пароль и увеличит интервал Это игрушечная модель, но она демонстрирует принцип: чем чаще повторяем в начале, тем дольше сохраняем в будущем. Итог Когда мы забываем пароль от Wi-Fi, это не лень и не рассеянность. Это естественное свойство мозга, который работает как база данных с ограничениями, вероятностным доступом и весьма своеобразным garbage collector. И если отнестись к этому с пониманием, можно научиться обходить ограничения — и памяти, и собственных когнитивных привычек.', hub='программирование'), Page(id='946276', title='Проектирование REST API: проблемы, решения, практические рекомендации', content='API — это не просто техническая прослойка. Это продукт. Его пользователи — другие разработчики. И, как у любого продукта, у него может быть ужасный или превосходный пользовательский опыт. Плохой API — это источник постоянной боли, багов и потраченного времени. Хороший API интуитивно понятен, предсказуем и прощает ошибки. Он становится продолжением мыслей разработчика. Латать дыры по мере их обнаружения — это путь в никуда. Нужно не тушить пожары, а строить систему так, чтобы она не загоралась. Безопасность, производительность и удобство использования должны закладываться в архитектуру с первого дня. Это контракт. Если контракт составлен плохо, его будут нарушать. Проблема №1 – Хаос в структуре и именовании Это первое, с чем сталкиваются. Эндпоинты вроде\\xa0/getUsers,\\xa0/addNewPost\\xa0или\\xa0/user/12-3/updateEmail\\xa0создают путаницу. Такой API невозможно запомнить. Его невозможно предсказать. Это прямой путь к ошибкам и разочарованию. Решение А: Ресурс-ориентированный подход (nouns) Суть: \\xa0Думать не о действиях, а о сущностях (ресурсах). Использовать существительные во множественном числе для коллекций. Использовать HTTP-методы для выражения действий. Плюсы: Предсказуемость. \\xa0Структура становится интуитивно понятной. Разработчик может угадать нужный эндпоинт, даже не заглядывая в документацию. Стандарт. \\xa0Это общепринятый стандарт для REST. Огромное количество инструментов и фреймворков заточено именно под него. Минусы: Негибкость для сложных действий. \\xa0Что делать с действиями, которые не вписываются в CRUD? Например, \"активировать пользователя\".\\xa0POST /users/123/activate\\xa0выглядит как компромисс. Решение Б: Подход на основе действий (verbs) Суть: \\xa0Каждый эндпоинт явно описывает действие. Это ближе к RPC (Remote Procedure Call), чем к REST. Плюсы: Явность. \\xa0Имя эндпоинта точно говорит, что он делает. Никаких двусмысленностей. Простота для нестандартных операций. \\xa0Не нужно придумывать, как \"активацию\" уложить в рамки REST. Минусы: Беспорядок. \\xa0API быстро превращается в свалку из десятков и сотен уникальных методов. Нет никакой структуры. Игнорирование HTTP. \\xa0Вся смысловая нагрузка переносится в URL, а HTTP-методы (GET,\\xa0POST) теряют свое значение. Решение В: Гибридный подход Суть: \\xa0Использовать ресурс-ориентированный подход как основу. Для сложных, нересурсных действий использовать специальный подресурс \"actions\" или просто глагол в конце. Плюсы: Лучшее из двух миров. \\xa0Сохраняет структуру и предсказуемость REST, но дает гибкость для нестандартных операций. Явное разделение. \\xa0Четко видно, где у нас работа с ресурсом, а где — выполнение сложного бизнес-процесса. Минусы: Требует дисциплины. \\xa0Команда должна договориться о четких правилах, когда использовать глаголы, чтобы не скатиться в хаос. Проблема №2 – Избыточные или недостаточные данные Классическая ситуация: чтобы отобразить список постов с именами авторов, клиент делает \"N+1 запросов\". Или\\xa0GET /users\\xa0возвращает по 50 полей на каждого, забивая сетевой канал. Решение А: Выбор полей (Field Picking) Суть: \\xa0Позволить клиенту самому указывать, какие поля он хочет получить:\\xa0GET /users?fields=id,name,email. Плюсы: Экономия трафика. \\xa0Клиент получает только то, что ему нужно. Критически важно для мобильных приложений. Гибкость. \\xa0API становится более универсальным. Минусы: Сложность на бэкенде. \\xa0Требует реализации парсинга полей и динамического построения запросов к базе данных. Риск производительности. \\xa0Неосторожный выбор полей клиентом может привести к очень тяжелым запросам. Решение Б: Встраивание связанных ресурсов (Embedding) Суть: \\xa0Позволить клиенту запрашивать связанные ресурсы в одном вызове:\\xa0GET /posts?embed=author,comments. Плюсы: Решение проблемы N+1. \\xa0Устраняет необходимость в дополнительных запросах, кардинально сокращая задержку. Удобство для клиента. \\xa0Вся необходимая информация для отрисовки экрана приходит в одном ответе. Минусы: Увеличение нагрузки. \\xa0Сервер должен выполнять дополнительные JOIN-ы, что может быть накладно. Избыточность. \\xa0Если встроить слишком много, ответ может сильно раздуться. Нужно ограничивать глубину встраивания. Решение В: Предопределенные представления (Views) Суть: \\xa0На сервере заранее определяются несколько \"видов\" ресурса:\\xa0GET /users?view=summary. Плюсы: Полный контроль на сервере. \\xa0Вы можете точно оптимизировать запросы к базе данных для каждого представления. Простота для клиента. \\xa0Не нужно перечислять десятки полей, достаточно указать одно слово. Минусы: Негибкость. \\xa0Если клиенту понадобится комбинация полей, не предусмотренная ни в одном view, это станет проблемой. Проблема №3 – Обработка больших коллекций Запрос\\xa0GET /logs\\xa0не может возвращать миллион записей. Система просто ляжет. Решение А: Пагинация на основе смещения (Offset/Limit) Суть: \\xa0Клиент запрашивает данные с помощью\\xa0limit\\xa0и\\xa0offset. Плюсы: Простота и интуитивность. \\xa0Легко реализовать и использовать. Позволяет легко перепрыгивать на любую страницу. Минусы: Низкая производительность. \\xa0На больших\\xa0offset\\xa0база данных вынуждена сначала найти все записи до смещения, а потом отбросить их. Пропуск данных. \\xa0Если в начало списка добавляются новые записи, клиент может пропустить некоторые записи. Решение Б: Пагинация на основе курсора (Keyset Pagination) Суть: \\xa0Клиент передает ID последнего полученного элемента:\\xa0GET /logs?limit=100&after_id=54321. Плюсы: Высокая производительность. \\xa0Запрос к базе данных очень эффективен (WHERE id > ...). Стабильность. \\xa0Не пропускает данные. Идеально для бесконечных лент. Минусы: Нельзя перейти на конкретную страницу. \\xa0Можно двигаться только вперед или назад. Сложнее в реализации. \\xa0Требует стабильного и уникального поля для сортировки. Проблема №4 – Эволюция API без поломок Вы выпустили API. Через год вам нужно добавить новое поле или изменить формат старого. Как это сделать, не сломав все клиентские приложения? Решение А: Версионирование в URL Суть: \\xa0Номер версии является частью пути:\\xa0/api/v1/users. Плюсы: Явность. \\xa0Версия видна сразу. Легко тестировать в браузере или curl. Простота маршрутизации. \\xa0Веб-сервер легко направляют запросы к разным версиям кода. Минусы: Загрязнение URL. \\xa0URI должен идентифицировать ресурс, а не версию его представления. Решение Б: Версионирование в заголовках Суть: \\xa0Клиент указывает желаемую версию в HTTP-заголовке\\xa0Accept:\\xa0application/vnd.myapi.v2+json. Плюсы: Чистые URL. \\xa0URI остается неизменным (/api/users), что соответствует идеологии REST. Гибкость. \\xa0Позволяет запрашивать разные версии одного и того же ресурса. Минусы: Скрытость. \\xa0Версия не видна с первого взгляда. Сложнее отлаживать и тестировать. Кэширование. \\xa0Некоторые прокси-серверы могут не учитывать заголовок\\xa0Accept. Решение В: Обратная совместимость Суть: \\xa0Никогда не вносить ломающих изменений. Только добавлять новые опциональные поля. Плюсы: Простота. \\xa0Не нужно управлять версиями кода и маршрутизации. Минусы: Непрактично в долгосрочной перспективе. \\xa0API со временем обрастает устаревшими полями и костылями. Проблема №5 – Обработка ошибок Когда что-то идет не так, пустой ответ с кодом\\xa0500\\xa0бесполезен. Клиент должен понимать,\\xa0 что \\xa0именно пошло не так и как это исправить. Решение А: Стандартизированный JSON-ответ об ошибке Суть: \\xa0В дополнение к коду возвращать тело ответа в формате JSON с деталями. Плюсы: Детализация. \\xa0Можно указать код ошибки для машины, сообщение для человека. Консистентность. \\xa0Все ошибки в вашем API будут иметь одинаковую структуру. Минусы: Небольшой оверхед. \\xa0Требует реализации и поддержки этой структуры. #include <string>\\n#include <vector>\\n#include <optional>\\n#include <nlohmann/json.hpp>\\n\\nstruct ProblemDetails {\\n    std::string type = \"about:blank\";\\n    std::string title;\\n    std::optional<int> status;\\n    std::optional<std::string> detail;\\n    std::optional<std::string> instance;\\n};\\n\\nstruct ValidationErrorDetails : ProblemDetails {\\n    struct InvalidParam {\\n        std::string name;\\n        std::string reason;\\n    };\\n    std::vector<InvalidParam> invalid_params;\\n};\\n\\nvoid to_json(nlohmann::json& j, const ValidationErrorDetails::InvalidParam& p) {\\n    j = nlohmann::json{{\"name\", p.name}, {\"reason\", p.reason}};\\n}\\n\\nvoid to_json(nlohmann::json& j, const ValidationErrorDetails& p) {\\n    j = nlohmann::json{\\n        {\"type\", p.type},\\n        {\"title\", p.title},\\n        {\"invalid_params\", p.invalid_params}\\n    };\\n    if (p.status) j[\"status\"] = *p.status;\\n    if (p.detail) j[\"detail\"] = *p.detail;\\n    if (p.instance) j[\"instance\"] = *p.instance;\\n} Проблема №6 – Неатомарные операции \"Перевести деньги со счета А на счет Б\". Если второй вызов API упадет, деньги \"повиснут в воздухе\", что недопустимо в финансовых системах. Решение А: Ресурс \"Транзакция\" Суть: \\xa0Клиент создает единый ресурс (POST /transfers), который описывает всю операцию. Сервер выполняет все действия в рамках одной транзакции базы данных. Плюсы: Атомарность. \\xa0Гарантирует, что операция будет выполнена целиком или не выполнена вообще (ACID). Ясность. \\xa0API отражает бизнес-сущность (\"перевод\"), а не технические детали (\"списание\"). Минусы: Неуниверсальность. \\xa0Подходит только для заранее известных, часто повторяющихся бизнес-процессов. Решение Б: Паттерн \"Сага\" (для микросервисов) Суть: \\xa0Управление распределенными транзакциями через асинхронные события и компен��ационные операции. Первый сервис выполняет свою часть и публикует событие, второй реагирует. Если второй падает, публикуется событие отката. Плюсы: Работает в распределенной среде. \\xa0Единственный жизнеспособный способ обеспечить консистентность данных между микросервисами. Слабая связанность. \\xa0Сервисы общаются через асинхронные события, а не через прямые вызовы API. Минусы: Сложность. \\xa0Значительно сложнее в реализации и отладке. Требует продуманной системы отката. Итоговая консистентность (Eventual Consistency). \\xa0Система не всегда находится в консистентном состоянии. Проблема №7 – Длительные (асинхронные) операции Процесс конвертации видео или генерации годового отчета занимает 10 минут. HTTP-соединение столько не проживет. Решение А:\\xa0202 Accepted\\xa0и ресурс \"Задача\" Суть: \\xa0API немедленно отвечает\\xa0202 Accepted\\xa0и возвращает URL для отслеживания статуса задачи (/tasks/{taskId}). Клиент периодически опрашивает (polling) этот URL. Плюсы: Не блокирует клиента. \\xa0Надежный и понятный контракт. Клиент контролирует, когда запрашивать статус. Простота. \\xa0Относительно легко реализовать. Минусы: Polling (опрос). \\xa0Создает дополнительную, часто ненужную, нагрузку на сервер. Решение Б: Webhooks (обратные вызовы) Суть: \\xa0При создании задачи клиент передает\\xa0callbackUrl. Сервер сам делает\\xa0POST\\xa0на этот URL, когда задача завершена. Плюсы: Эффективность. \\xa0Никакого лишнего трафика. Уведомление приходит ровно тогда, когда нужно. Проактивность. \\xa0Сервер сам инициирует коммуникацию. Минусы: Требования к клиенту. \\xa0Клиент должен иметь публично доступный эндпоинт, что не всегда возможно. Надежность доставки. \\xa0Требуется реализация механизма повторных попыток на сервере. Решение В: WebSockets/Server-Sent Events (SSE) Суть: \\xa0Клиент устанавливает постоянное соединение с сервером и получает обновления о статусе задачи в реальном времени. Плюсы: Реальное время. \\xa0Обновления приходят моментально без опроса. Идеально для UI, где нужно показывать прогресс-бар. Эффективность. \\xa0После установки соединения оверхед на передачу сообщений минимален. Минусы: Stateful. \\xa0Устанавливает постоянное соединение, что создает нагрузку на сервер. Сложнее в масштабировании за балансировщиком. Проблема №8 – Идемпотентность Клиент повторяет\\xa0POST /payments\\xa0из-за сбоя сети и с пользователя списываются деньги дважды. Решение А: Заголовок\\xa0Idempotency-Key Суть: \\xa0Клиент генерирует для каждой операции уникальный ключ и передает его в заголовке. Сервер, видя повторный ключ, не выполняет операцию заново, а возвращает сохраненный результат. Плюсы: Надежность. \\xa0Гарантирует, что критически важные операции будут выполнены ровно один раз. Стандарт де-факто. \\xa0Многие крупные API (Stripe, Adyen) используют именно этот подход. Минусы: Дополнительная инфраструктура. \\xa0Требует быстрого хранилища (Redis) для ключей идемпотентности. Ответственность на клиенте. \\xa0Клиент должен правильно генерировать и управлять этими ключами. Решение Б: Уникальные бизнес-ключи Суть: \\xa0Требовать от клиента передачи уникального идентификатора операции в теле запроса (например,\\xa0transactionId). Сервер проверяет уникальность этого ключа в базе данных перед выполнением операции. Плюсы: Простота. \\xa0Не требует дополнительной инфраструктуры вроде Redis. Проверка происходит на уровне базы данных. Бизнес-контекст. \\xa0Ключ является частью бизнес-логики, что может быть более понятным. Минусы: Смешивает логику. \\xa0Логика протокола смешивается с бизнес-логикой. Не всегда возможно. \\xa0Не у каждой операции есть естественный уникальный ключ, который может предоставить клиент. Проблема №9 – Управление сложностью графа данных Чтобы собрать один экран, клиент делает десятки запросов: пользователи -> посты -> комментарии -> авторы. Решение А: GraphQL как фасад Суть: \\xa0Создать единый GraphQL-сервер, который \"под капотом\" делает множество запросов к вашим REST API и собирает ответ. Плюсы: Гибкость для клиента. \\xa0Клиент получает именно те данные, которые ему нужны, в одном запросе. Эволюционный подход. \\xa0Позволяет внедрить преимущества GraphQL, не ломая существующую REST-архитектуру. Минусы: Дополнительный слой. \\xa0Появляется еще один компонент, который нужно разрабатывать, поддерживать и масштабировать. Сложность. \\xa0Логика \"разрешения\" (resolving) полей в GraphQL может стать довольно сложной. Решение Б: Спецификации JSON:API или OData Суть: \\xa0Это надстройки над REST, которые стандартизируют способы включения связанных ресурсов:\\xa0/articles?include=author. Плюсы: Стандартизация. \\xa0Существуют готовые библиотеки для клиента и сервера, которые решают множество проблем \"из коробки\". Мощность. \\xa0Предоставляет решения для фильтрации, сортировки, пагинации и связей. Минусы: Сложность и многословность. \\xa0Формат JSON:API довольно строгий и может показаться избыточным для простых случаев. Порог вхождения. \\xa0Требует от всех разработчиков изучения и следования этой спецификации. Решение В: Паттерн Backend For Frontend (BFF) Суть: \\xa0Создается отдельный API-фасад для каждого типа клиента (веб, мобильное приложение). Этот фасад агрегирует данные из нижележащих микросервисов в том виде, который удобен конкретному фронтенду. Плюсы: Оптимизация. \\xa0API идеально заточен под нужды конкретного клиента. Мобильный BFF может отдавать более легковесные ответы. Изоляция. \\xa0Изменения для веб-клиента не затрагивают мобильный. Минусы: Дублирование кода. \\xa0Если клиентов много, логика агрегации может дублироваться. Увеличение количества сервисов. \\xa0Появляются дополнительные компоненты, которые нужно развертывать и поддерживать. Проблема №10 – Массовые операции Клиенту нужно создать 1000 объектов. 1000 отдельных\\xa0POST\\xa0запросов — это безумие из-за сетевых задержек. Решение А: Единый batch-эндпоинт Суть: \\xa0Создается специальный эндпоинт, который принимает массив объектов для создания/обновления:\\xa0POST /users/batch. Плюсы: Эффективность. \\xa0Резко сокращает сетевые задержки и количество HTTP-соединений. Атомарность (опционально). \\xa0Можно обернуть всю операцию в одну транзакцию. Минусы: Обработка ошибок. \\xa0Что если 500 объектов валидны, а 500 — нет? Нужно возвращать смешанный ответ (статус 207 Multi-Status) с отчетом по каждой операции. Сложность ответа. \\xa0Парсинг такого ответа на клиенте усложняется. Решение Б: Асинхронная обработка Суть: \\xa0Комбинация batch-запроса и паттерна для длительных операций. Клиент делает\\xa0POST /users/batch, сервер отвечает\\xa0202 Accepted\\xa0и возвращает URL на задачу. Плюсы: Масштабируемость. \\xa0Не блокирует HTTP-воркеры на длительную обработку. Идеально для очень больших объемов. Надежность. \\xa0Даже если клиент отвалится, обработка продолжится. Минусы: Сложность. \\xa0Самый сложный вариант, требующий очереди сообщений и фоновых обработчиков. Задержка обратной связи. \\xa0Клиент не получает моментальный результат. Архитектурный взгляд Проектирование API — это не только про эндпоинты. Это про создание надежной, безопасной и удобной платформы. API как продукт. \\xa0Ваш API — это продукт для разработчиков. У него есть свой жизненный цикл, своя документация (маркетинг), свои пользователи и своя поддержка. Относитесь к нему соответственно. Плохой API отпугнет интеграторов и партнеров так же, как плохой UI отпугивает конечных пользователей. Безопасность по умолчанию. \\xa0Безопасность не \"прикручивается\" в конце. Она должна быть встроена в дизайн. Аутентификация и авторизация: \\xa0Используйте стандартные протоколы (OAuth 2.0, OpenID Connect). Не изобретайте свои. Авторизация должна проверяться на каждом запросе, на уровне доступа к конкретному ресурсу. Валидация на входе: \\xa0Никогда не доверяйте данным от клиента. Внедрите строгую валидацию на границе API (например, через JSON Schema). Любой невалидный запрос должен отбрасываться с ошибкой\\xa0400. Производительность и кэширование. \\xa0Хороший API должен быть быстрым. HTTP-кэширование: \\xa0Используйте заголовки\\xa0Cache-Control,\\xa0ETag\\xa0и\\xa0Last-Modified. Для\\xa0GET\\xa0запросов, которые возвращают редко меняющиеся данные, кэширование может снизить нагрузку на порядки.\\xa0ETag\\xa0особенно полезен для условных запросов. Rate Limiting: \\xa0Защитите свой API от злоупотреблений и DoS-атак. Внедрите ограничения на количество запросов. Важно сообщать клиенту о лимитах через заголовки (X-RateLimit-Limit,\\xa0X-RateLimit-Remaining). Опыт разработчика Это то, что отличает просто работающий API от API, с которым приятно работать. Документация — это не опция. \\xa0Отсутствие документации или ее плохое качество — это неуважение к пользователям вашего API. OpenAPI (Swagger): \\xa0Это стандарт де-факто. Он позволяет не только описать ваш API, но и сгенерировать интерактивную документацию, клиентские SDK и наборы тестов. Документация должна быть частью CI/CD и обновляться вместе с кодом. Песочница (Sandbox). \\xa0Предоставьте разработчикам безопасную среду, где они могут экспериментировать с вашим API, не боясь сломать реальные данные. Песочница должна быть максимально приближена к продакшен-среде. Клиентские SDK. \\xa0Предоставление готовых библиотек для популярных языков может значительно снизить порог вхождения. Однако это создает дополнительную нагрузку по их поддержке. Выбор правильного инструмента для задачи Простой внутренний CRUD-сервис. Ресурс-ориентированный подход, пагинация\\xa0offset/limit. Минимум сложностей. Главное — скорость разработки. Публичный API для партнеров. Строгий контракт. Обязательное версионирование в URL. Стандартизированные и подробные ошибки. Документация OpenAPI. Идемпотентность для всех\\xa0POST. API для высоконагруженного мобильного приложения. Максимальная производительность. Пагинация на основе курсора. Поддержка\\xa0fields\\xa0и\\xa0embed. Отдельные batch-эндпоинты. Сложная микросервисная система. GraphQL-фасад для внешних клиентов. Паттерн \"Сага\" для распределенных транзакций. Асинхронные операции с вебхуками для межсервисного взаимодействия. Практические рекомендации:  Используйте существительные во множественном числе для коллекций. \\xa0/users. Используйте HTTP-методы и статус-коды по назначению. Возвращайте полезные, стандартизированные ошибки (RFC 7807). Предусмотрите фильтрацию, сортировку и пагинацию \\xa0для всех коллекций. Версионируйте API с самого начала в URL (/v1/...). Используйте JSON и HTTPS. \\xa0Это не обсуждается. Документируйте API с помощью OpenAPI (Swagger). Обеспечьте идемпотентность \\xa0для всех изменяющих операций (Idempotency-Key). Используйте вложенность для связанных ресурсов. \\xa0/users/123/orders. Возвращайте\\xa0Location\\xa0заголовок \\xa0с URL нового ресурса при\\xa0201 Created. Проектируйте API для кэширования (ETag,\\xa0Cache-Control). Для сложных действий используйте подресурсы. \\xa0/users/123/actions/activate. Используйте UUID в публичном API, \\xa0а не автоинкрементные ID. Всегда отвечайте JSON-объектом. \\xa0{ \"data\": [...] }\\xa0лучше, чем\\xa0[...]. Используйте единый стиль именования полей. \\xa0camelCase\\xa0для JSON — хороший стандарт. Используйте даты в формате 2023-10-27T10:00:00Z. Будьте последовательны. \\xa0Если один эндпоинт использует пагинацию на основе курсора, все остальные должны использовать ее же. Не используйте HTTP-заголовки для передачи параметров. \\xa0Заголовки — для метаданных. Тестируйте свой API так, как его будет использовать клиент. Проектирование API — это марафон, а не спринт. Решения, принятые на ранних этапах, будут преследовать проект годами, создавая либо прочный фундамент, либо архитектурный долг. Не существует единственной \"серебряной пули\". Надежность рождается из сочетания множества правильно реализованных механизмов и глубокого понимания компромиссов каждого из них. Ключевой вывод прост: относитесь к своему API как к продукту. Думайте о его пользователях — разработчиках. Уважайте их время, предвосхищайте их потребности и давайте им инструменты для успеха. В конечном итоге, лучший API — это тот, о существовании которого забываешь, потому что он просто работает. Надежно, предсказуемо и быстро.', hub='микросервисы'), Page(id='946526', title='О гибкости Home Assistant', content='HA использую давно, он  живет уже на третей малинке в виде Raspberry Pi 4,  сейчас на нем 27 интеграций, начиная от QNAP и заканчивая сигналкой машины, 32 устройства \\xa0 через Zigbee Home Automation и 14 устройств через zigbee2mqtt. Все zigbee через два Zigbee 3.0 SONOFF USB Dongle Plus-E. Кажется что одна из самых распространенных  автоматизаций - включение свет на движение. Кажется что это просто, есть сигнал от датчика движения - свет включился. Но на самом нюансов куча даже в простой логике. Например датчики движения Aqara для экономии батарейки отключаются на время после детектирования движения.  А если вы сидите и не двигаетесь то датчик движения о вас ничего и не знает, и уже нужен датчик присутствия. А еще есть события открытия дверей, время  и так далее. А еще надо трекать если свет включен вручную и его не надо автоматически отключать в этом случае.  Автоматизацию можно писать самому, а можно использовать \"Проекты\" (BluePrint). Один из лучших, если не лучший, по моему мнению,  проект от Blacky .   В целом хотелось что бы в коридоре свет включался при движении и не выключался пока там кто-то есть. Кроме использования этого проекта был задействованны датчики:   Датчики движения Aqara Motion Sensor   - 2 штуки Датчик присутствия - радар  ZY-M100-S_1  в настенном исполнении - 1 штука. Может работать как zigbee роутер, но каждую секунду генерит событие чем немного забивает zigbee сеть.  Датчик открытия дверей  - 1 штука на входной двери. В целом все работало как часы несколько лет включаясь от движения, открытия двери и датчика присутствия, но появилась проблема, сон стал менее глубоким и я стал просыпаться от того что в коридоре включается свет.  Поэтому была закуплена   LED лента с поддержкой zigbee , которая всем хороша, и даже работает как zigbee роутер,  а так-же из запасников достал еще один  датчик открытия дверей  который  валялся без дела. Далее я сел разбираться с  проектом от Blacky  в котором 3 экрана настроек, из которых я пользовался только одной. И оказалось все нужное там уже есть. Есть опция Night Lights которая позволяет указать другой включатель света в определенные промежутки времени, я поставил с 22 до 9 утра. На самом деле еще можно привязать к восходу и заходу солнца. Есть опция  Night Lights - Entity State, Select an entity that will be ON for night lights to work,  в которую отлично вписался датчик двери.  Еще лента разноцветная и умеет светить красным светом. Это идеальное ночное освещение, не бьет по глазам с просоня, не сбивает темновую адаптацию.  И теперь получается так. Между 9 и 22 включается обычный свет сверху. C 22 логика меняется и если открыта дверь в спальню то включается LED лента в красном свете. Если закрыта дверь в спальню - включается обычный свет сверху. Ну например кто-то из семьи собирается до 9 куда-то.  При этом если между 22 и 9 закрыть дверь в спальню, то автоматом выключится лента и включится обычный свет. Так же BluePrint рулит освещением на границе временного интервала, то есть в 9 утра, если включена LED, он автоматом выключит ленту и включит обычный свет. Минимум ручных действий и максимум автоматики, все как я люблю :) красный свет - это красиво :) Ну и немного про ленту, в ней есть забавный режим, пульт управления содержит микрофон и можно включить  (в том числе через HA) \"музыкальный\" режим при котором  лента включает микрофон и начинает изображать из себя цветомузыку. Выглядит забавно, но как это применить в коридоре я так и не придумал, места для танцев там маловато :) Все это было сделано на BluePrint 5.0 версии от 1 Nov-23, что косвенно говорит о времени жизни инсталяции HA, на самом деле она больше, когда-то я строил автоматизацию света на Red Node, но давно от Red Node отказался.  Cейчас доступна версия 8.3 от 11 Aug-25\\xa0', hub='open source'), Page(id='944360', title='Российские ученые нашли новый метод поиска нефти и газа', content='Коллектив исследователей из МФТИ разработал новые методики моделирования распространения сейсмических волн в средах со сложной геометрией. Это важно для эффективного поиска новых месторождений нефти и газа. Исследование\\xa0 опубликовано \\xa0в журнале Lobachevskii Journal of Mathematics.\\xa0 Проблема точного моделирования распространения сейсмических волн в неоднородных средах стоит перед геофизиками уже несколько десятилетий. Ранее попытки ученых использовать методы конечных разностей и конечных элементов на неструктурированных сетках столкнулись с трудностями в обеспечении вычислительной эффективности при высокой точности. Развитие методов на разнесенных сетках и гибридных подходов частично решило эту проблему, но до сих пор не существовало универсального решения для широкого класса задач. Традиционные методы решения обратных задач сейсморазведки сталкиваются с трудностями при учете рассеяния волн на границах и контактных границах сложной формы. Это приводит к снижению точности результатов и, как следствие, к неэффективному использованию ресурсов при поисках новых месторождений. Разработанный учеными МФТИ метод, основанный на использовании химерных сеток, позволяет эффективно преодолеть эти ограничения. Это особенно важно для поиска и разведки труднодоступных месторождений углеводородов, так как он позволяет существенно повысить точность прогнозирования месторождений углеводородов, одновременно сокращая вычислительные затраты. Химерные сетки представляют из себя комбинацию декартовой (фоновой) и криволинейной сеток. Это позволяет точно учитывать условия на границах сложной формы. Связь между сетками осуществляется с помощью интерполяции. Ученые сравнили пять различных методов генерации таких сеток, оценивая их по нескольким ключевым параметрам: максимально допустимое число Куранта (определяющее стабильность вычислений), скорость вычислений, точность и порядок сходимости.\\xa0 Рисунок 1. (а) постановка задачи, (b) равномерный шаг в исходной параметризации, IG = 0, 2, (c) равномерный шаг в естественной параметризации, (d) перпендикуляр в среднем узле m ∈ [1, M − 2], M — количество узлов на границе, (e) перпендикуляр в угловом узле m = 0, M − 1. Источник: «Lobachevskii Journal of Mathematics». Для решения сеточных уравнений использовался сеточно-характеристический метод, а для повышения точности расчетов использовались два метода расщепления по направлениям: первого и третьего порядков точности. Результаты показали, что порядок сходимости по основному сеточному параметру (шагу фоновой декартовой сетки) практически совпадает для эффективного шага криволинейной химерной сетки. Это подтверждает обоснованность использованного исследователи шага фоновой сетки в качестве основного параметра для анализа сходимости. Исследование продемонстрировало эффективность такого комбинированного метода использования двух видов сетки с интерполяцией, который позволяет замедлить снижение порядка сходимости при уменьшении шага сетки. Хотя использование простейшего расщепления по направлениям (первого порядка) показало несколько меньший порядок сходимости, оно остается эффективной стратегией для проведения реальных расчетов, особенно при учёте затрат на вычислительные ресурсы. Каждый из этих алгоритмов был тщательно протестирован на восьми репрезентативных геологических моделях, включающих как относительно простые (например, моделирование топографии поверхности), так и сложные криволинейные границы (например, моделирование поверхности рельса или сложных неоднородностей внутри горных пород). Было проверено три различных метода уменьшения размера шага химерной сетки в поперечном направлении. Рисунок 2. Конформные с границами химерные сетки. IS означает индекс формы сетки Химеры, IG означает индекс границы, соответствующий методу генерации химеры: (a) IS = 7, IG = 0, равномерный шаг в исходной параметризации, просто опускает границу вниз, (b) IS = 7, IG = 1, равномерный шаг в исходной\\xa0параметризации с использованием перпендикуляров, (c) IS = 7, IG = 2, равномерный шаг в естественной параметризации верхней границы, простое опускание границы вниз, (d) IS = 7, IG = 3, (e) \\u200b\\u200bIS = 7, IG = 4, равномерный шаг в естественной параметризации верхней границы с использованием перпендикуляров, (f) IS = 5, IG = 1, равномерный шаг в естественной параметризации верхней и нижней границ с использованием перпендикуляров, итеративная, (g) IS = 6, IG = 1, (h) IS = 4,\\xa0IG = 1, (i) IS = 3, IG = 1, (j) IS = 2, IG = 1, (k) IS = 1, IG = 1, (l) IS = 0, IG = 1.\\xa0\\xa0Источник: «Lobachevskii Journal of Mathematics». В качестве тестовой задачи использовалось распространение плоской продольной волны в однородной среде, позволяющее объективно оценить точность и сходимость методов. Начальное состояние моделировалось волной длиной 20 метров, наклоненной под углом 35° к горизонтали — угол, специально выбранный для проверки алгоритмов в наиболее сложных условиях. Результаты показали высокую точность расчетов, подтвержденную оценкой численной сходимости и определением порядка сходимости. Кроме того, было исследовано влияние выбора алгоритма генерации химерных сеток на скорость работы программы и затраты оперативной памяти.\\xa0 Исследователям удалось выяснить, что один из разработанных алгоритмов генерации химерных сеток продемонстрировал наилучшую эффективность, максимизируя максимально допустимое число Куранта (параметр, определяющий стабильность и скорость вычислений). Этот метод был обозначен как IG = 4 в исследовании. Суть его заключалась в итеративном опускании по перпендикулярам от узлов сетки, расположенных на границ, Такой подход позволяет с помощью сетки наиболее точно повторять геометрию даже очень сложной формы, точно учитывая граничные условия. Метод с IG = 4 обеспечивает высокое число Куранта и оптимальную точность. Авторы исследования также изучили влияние уменьшения шага фоновой сетки на различные параметры, подтвердив преимущества выбранных алгоритмов. По результатам моделирования ученые предложили следующую оптимальную стратегию выбора метода построения сеток в зависимости от сложности геометрии: Сложные геометрии с резко меняющимся уклоном: IG = 4 (итеративный алгоритм). Геометрии со средним градиентом: IG = 3 (естественные параметризации с перпендикулярами). Слабо меняющиеся функции с градиентами, близкими к нулю: IG = 0 (равномерный шаг). Исследование также показало влияние различных методов уменьшения шага химерной сетки в поперечном направлении (IW). Оказалось, что из трех рассмотренных, метод, обозначаемый IW = 2, предполагающий адаптивное увеличение числа узлов, оказался наиболее эффективным для поддержания высокой точности и сходимости при изменении шага фоновой сетки. Выбор оптимального расщепления по направлениям также играет важную роль. В то время как оператор расщепления третьего порядка теоретически обеспечивает более высокую точность, его практическое применение ограничено из-за существенного увеличения вычислительных затрат. В большинстве случаев, особенно при достаточном разрешении (более 40 точек на длину волны), классическое расщепление по направлениям первого порядка оказывается более эффективным. «Ключевым преимуществом нашего подхода является использование химерных сеток. Оно позволяет учитывать сложную геометрию границ раздела пластов, что существенно повышает точность геологического моделирования,\\xa0 — пояснила\\xa0 Алена Фаворская , ведущий научный сотрудник лаборатории прикладной вычислительной геофизики МФТИ. —\\xa0 Они позволяют нам сочетать высокую точность моделирования сложных геологических структур с высокой эффективностью вычислений, что критически важно для обработки огромных объемов данных, получаемых при сейсморазведке. Разработанные алгоритмы особенно эффективны для задач, связанных с моделированием топографии земной поверхности или границ раздела различных геологических пород. Результаты фундаментальных исследований, полученные в данной работе, нацелены в первую очередь на оптимизацию добычи трудноизвлекаемых запасов углеводородов, когда над месторождением есть экранирующие сейсмический отклик геологические слои сложной формы либо в шельфовой зоне». Разработанный подход открывает новые перспективы в высокоточной сейсмической разведке, позволяя существенно сократить время и ресурсы, необходимые для обработки данных. Это особенно актуально в условиях истощения легкодоступных месторождений углеводородов и роста требований к точности геологоразведочных работ. Результаты исследования имеют широкое применение и могут быть использованы не только в сейсморазведке, но и в других областях, таких как ультразвуковой неразрушающий контроль. Дальнейшие исследования будут направлены на оптимизацию алгоритмов и их применение к более сложным геологическим задачам с целью сокращения времени и затрат на геологоразведку. Исследование выполнено при поддержке Российского научного фонда (грант № 20-71-10028). Исследования велись в лаборатории прикладной вычислительной геофизики МФТИ и на кафедрах вычислительной физики и информатики и вычислительной математики МФТИ.', hub='научно-популярное'), Page(id='946176', title='Математика и веб-разработка: как мы добавили интерактивную кривую Безье в редактор изображений', content='Добрый день, меня зовут Богдан, я фронтенд-разработчик в компании iSpring. В статье расскажу про интерактивную стрелку в редакторе изображений. Вы узнаете: как строятся кривые Безье и какие полезные свойства имеют; как вычислить кривую Безье, проходящую через заданные точки; как найти ограничивающую площадь этой кривой. Рассмотрим плюсы и минусы реализаций на Canvas и SVG.  Редактор изображений является частью платформы для корпоративного обучения iSpring Learn и был создан, чтобы решить проблему пользователей: им приходилось использовать сторонние программы для выделения элементов на скриншотах в обучающих материалах. В статье описывается решение этих задач. Интерфейс встроенного редактора изображений С помощью этого инструмента пользователь может добавлять на изображение различные вспомогательные элементы, например, прямоугольники, текст, нумерацию и стрелку, про которую и пойдет речь в этой статье. Frontend нашего редактора изображений построен на React и Typescript. Само изображение рисуется на canvas’е, на нем же располагаются все добавляемые объекты. Интерактивная стрелка в редакторе изображений Стрелка — это не просто линия. На скриншотах ниже показаны два подхода: прямая стрелка и изогнутая. Прямая стрелка — тривиальна, но не всегда подходит для сложных сценариев. Изогнутая стрелка — визуально приятнее, особенно актуальна в обучающих материалах. На рынке инструментов для редактирования изображений существуют две основные стратегии реализации изогнутых линий: пиксельно-точные настройки, как в Figma и Adobe, и автоматическая интерполяция на основе заданных точек. Мы выбрали второй вариант, поскольку нам важно, чтобы любой человек без навыков дизайна с помощью нашего продукта мог быстро получить привлекательный результат. Кривая Безье Мы использовали кривую Безье. Это достаточно простая, но очень важная вещь для современной фронтенд-разработки. Кривая Безье  —\\xa0 это математический инструмент для создания гладких кривых, которая задается набором опорных точек Обратим внимание на одно важное свойство кривой.  Опорные точки не всегда лежат на кривой Безье.  Первая и третья точки принадлежат кривой, так как являются ее концами. Но вторая точка явно кривой не принадлежит и находится где-то над ней. А что, если все точки должны располагаться на прямой?\\xa0 Именно эту задачу нужно решить, чтобы реализовать достаточно удобное манипулирование интерактивной кривой. Все элементы управления должны располагаться непосредственно на стрелке. Создание стрелки в редакторе Задача сводится к тому, чтобы по трем известным точкам найти такую опорную точку, чтобы кривая прошла через все три имеющиеся точки. В математике подобная операция называется  интерполяцией . Существует множество методов для решения этой задачи, например,  полином Лагранжа  или  сплайн-интерполяция . Это наиболее общие методы решения, но в нашем случае мы смогли значительно упростить себе жизнь. Параметрическое уравнение кривой Безье Квадратичную кривую математически можно описать с помощью такого выражение: P = (1−t) 2 P 1  + 2(1−t)tP 2  + t 2 P 3 P1, P2 и P3 — это опорные точки, по которым строится кривая Безье. P — это точка на кривой для заданного параметра t. Нам известны точки\\xa0 P1 и P3, так как они являются концами стрелки. Нам известна точка P — это элемент управления кривизной стрелки.\\xa0 Выразим из уравнения точку P2:\\xa0 P 2  = (P - (1−t) 2 P 1  - t 2 P 3 ) / (2(1−t)t) Уравнение имеет множество решений, так как параметр t не зафиксирован. При любом значении t от 0 до 1 кривая проходит через три заданные точки, но её изгиб меняется. Это позволяет, например, регулировать степень изгиба стрелки в зависимости от параметра. Однако, чтобы упростить работу пользователю, мы зафиксировали t = 0.5 — в этом случае изгиб симметричен, что выглядит наиболее естественно. Подставим t = 0.5: P 2  = 2P - 0.5(P 1  + P 3 ) Готово! Теперь у нас формула для построения кривой Безье  через   три заданные точки. Алгоритм редактирования Мы научились рисовать стрелку. Теперь рассмотрим, как ее редактировать. Что будет происходить с изогнутой стрелкой, когда пользователь захочет её растянуть, сузить, или повернуть. Проецируем точку на прямую Здесь нам понадобится проекция точки на прямую, заданную двумя другими точками. Как её найти? Нам поможет линейная алгебра и операции над векторами. Точки A и B задают прямую. Нужно найти проекцию точки P на прямую AB. С помощью вычитания точек найдем векторы u и v: Теперь воспользуемся готовой формулой проецирования одного вектора на другой. Мы проецируем вектор u на вектор v и получаем вектор t. Скалярное произведение векторов Здесь используется скалярное произведение векторов И теперь несложно найти точку, которую мы искали. Необходимо к точке A прибавить полученный вектор t. Как только мы получили проекцию, мы можем оценить, как далеко эта проекция находится от концов стрелки. Зачем нам это нужно? Оказывается, если при изменении длины стрелки мы будем сохранять эти отношения, то изменение будет выглядеть достаточно естественно: Тут есть один нюанс: проекция точки может не попасть на отрезок. В этом случае будем перемещать розовую точку в противоположную сторону: Выпрямляем стрелку Мы разработали общий подход, который отлично работает. Пользователь с легкостью изогнул стрелку, потянув за розовую точку. Но что, если он передумал и захотел сделать стрелку снова прямой? Конечно, при должном умении можно вернуть точку в исходное положение, но сделать это идеально вряд ли получится. Улучшим пользовательский опыт с помощью  магнетизации . Так как мы научились находить проекцию точки на прямую, сделать это будет достаточно просто. Расстояние от точки до прямой равно расстоянию от точки до ее проекции на эту прямую. И когда расстояние от точки до прямой достаточно мало, возвращаем точку в исходное положение, и получаем прямую стрелку. Вот так это выглядит: Отрисовка в браузере Что в первую очередь приходит в голову, когда нужно нарисовать какой-то графический элемент? Две основные технологии — это SVG и Canvas. Отрисовка на Canvas Если вы используете HTML5, у вас из коробки есть метод для рисования квадратичной кривой Безье с помощью Canvas 2D API:  quadraticCurveTo const ctx = canvas.getContext(\\'2d\\')\\nctx.moveTo(from.x, from.y)\\nctx.quadraticCurveTo(controlPoint.x, controlPoint.y, to.x, to.y) Теперь нужно отрисовать наконечник этой стрелы. Наконечник мы рисуем просто двумя линиями. Единственный вопрос, куда он должен быть направлен. Когда пользователь меняет форму стрелки, наконечник должен поворачиваться в разные стороны. ctx.moveTo(to.x, to.y)\\nctx.lineTo(headPoint1.x, headPoint1.y)\\nctx.moveTo(to.x, to.y)\\nctx.lineTo(headPoint2.x, headPoint2.y) Мы получим достаточно приятный результат, если примем направление стрелки за направление второго пунктирного отрезка: от второй опорной точки до наконечника. Получается, что мы строим направление наконечника с помощью невидимой точки, которая лежит где-то над стрелкой. Проблема с экранным сглаживанием При отрисовке стрелки на Canvas мы столкнулись с проблемой c тенями и  экранным сглаживанием .\\xa0 Скорее всего, дело в том, что браузер отключает экранное сглаживание при использовании теней из-за того, что добавление тени — достаточно дорогая операция, и браузер оптимизирует отрисовку, не производя сглаживание краев. Вторая проблема — сложная реализация интерактивности. Когда мы используем HTML-элементы, мы можем нативно подписаться на события кликов по ним, но с объектами, нарисованными на Canvas, так сделать не получится. Хотя математические вычисления остаются возможным решением, предпочтительнее использовать нативные браузерные механизмы для обработки кликов. В этом контексте SVG предлагает более удобный подход. Отрисовка на SVG Все элементы SVG поддерживают обработку пользовательских событий (например, кликов) без дополнительных вычислений. Чтобы реализовать отрисовку стрелки, достаточно использовать тег  <path> , где геометрия задается командами, аналогичными методам Canvas, но в сокращенной форме: ctx.moveTo(from.x, from.y)\\nctx.quadraticCurveTo(controlPoint.x, controlPoint.y, to.x, to.y) <path d={`\\n    M ${from.x} ${from.y}\\n    Q ${controlPoint.x} ${controlPoint.y}, ${to.x} ${to.y}\\n`}/> Проблема с автоматическим замыканием контуров При отрисовке стрелки таким образом возникает неожиданный эффект: браузер автоматически замыкает открытые пути, соединяя конечную точку с начальной. Это приводит к заливке внутренней области, даже если контур не был замкнут явно. Решение: Добавление fill=\"none\" отключает заливку, но область остается кликабельной, что может мешать взаимодействию с другими элементами. Нам нужно вручную замкнуть отрисованный путь, проведя кривую в обратную сторону: <path d={`\\n\\xa0\\xa0\\xa0\\xa0M ${from.x} ${from.y}\\n\\xa0\\xa0\\xa0\\xa0Q ${controlPoint.x} ${controlPoint.y}, ${to.x} ${to.y}\\n\\xa0\\xa0\\xa0\\xa0Q ${controlPoint.x} ${controlPoint.y}, ${from.x} ${from.y}\\n`}/> Добавляем элементы управления В нашем редакторе используются одинаковые элементы управления как для стрелки, так и для других графических объектов, например, прямоугольника и текста. Логично использовать общую реализацию. Однако не все компоненты редактора реализованы на SVG. Тег <foreignObject> позволяет встраивать HTML-контент в SVG. Например, React-компонент ControlPoint можно реализовать на HTML и встроить внутрь SVG: <svg xmlns=\"http://www.w3.org/2000/svg\">\\n\\xa0\\xa0\\xa0\\xa0<path d={...} />\\n\\xa0\\xa0\\xa0\\xa0<foreignObject>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<div>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<ControlPoint />\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<ControlPoint />\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<ControlPoint />\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0</div>\\n\\xa0\\xa0\\xa0\\xa0</foreignObject>\\n</svg> Однако в Safari это работает некорректно — элементы управления смещаются относительно целевого объекта. После небольшого исследования оказалось, что проблема находится на стороне Safari. Браузер неправильно обрабатывает тег <foreignObject>. Более подробно проблема описана  здесь . Меняем подход: На этом мы не остановились и продолжили искать решение. В  документации  нашли такую интересную вещь: С помощью тега <g> мы можем использовать вложенные <svg> теги. Такая группировка позволяет использовать SVG-элементы управления как в SVG, так и в HTML-контексте. Например: <svg xmlns=\"http://www.w3.org/2000/svg\">\\n\\xa0\\xa0\\xa0\\xa0<g>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<path d={...} /> \\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<svg>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<ControlPoint />\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<ControlPoint />\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<ControlPoint />\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0</svg>\\n    </g>\\n</svg> Расчет размеров SVG для кривых Безье SVG требует явного указания размеров (viewBox), которые не могут определяться автоматически. Для кривых Безье это создает сложности: плавные кривые могут выходить за границы, если рассчитывать их по трём известным точкам: Варианты решения: Математический расчет Первое, что пришло на ум — нахождение экстремумов кривой через производную уравнения Безье (для координат X и Y). Продифференциируем уравнение по координате x, приравняем производную нулю и выразим параметр t: x′(t) = (−2x 0 +2x 1 )+2(x 0 −2x 1\\u200b  +x 2 )t (−2x 0 +2x 1 )+2(x 0 −2x 1 \\u200b +x 2 )t = 0 t = (x 0 −x 1 ) / (x 0 −2x 1\\u200b  +x 2 ) При 0 < t < 1 экстремум найден. Использование выпуклой оболочки Но можно ли проще? Оказывается, можно. Изучим еще одно полезное свойство кривой Безье: Кривая Безье всегда находится внутри  выпуклой оболочки , образованной ее опорными точками. Существует множество алгоритмов построения выпуклой оболочки, например,  алгоритм Грэхема :\\xa0 Легче не стало. На самом деле, в нашем случае можем немного схитрить. Мы имеем квадратичную кривую Безье, то есть используются всего три опорные точки. Три точки образуют треугольник, а треугольник — это всегда выпуклая фигура. Как бы мы эти точки не соединили — всегда получим выпуклую оболочку. Теперь мы можем расчитывать размер SVG по опорным точкам кривой Безье, и стрелка гарантированно будет находиться внутри этого размера: Сравним подходы: Объединим подходы Чтобы объединить плюсы обоих подходов, мы использовали гибридное решение. Мы отрисовали визуал стрелки на canvas, а интерактивную часть (хитбокс) и элементы управления сделали с помощью невидимого SVG, который накладывается сверху на стрелку. Стрелка рисуется в два слоя. Вот так это выглядит, если покрасить SVG в прозрачный голубой цвет: Хитбокс больше стрелки, потому что это помогает пользователю меньше промахиваться. Итог Создание, казалось бы, простого элемента — интерактивной стрелки — оказалось полноценной исследовательской задачей. Мы начали с основ, изучив кривые Безье и алгоритмы их построения. Избавились от сложностей интерполяции, отыскав элегантное решение через параметрическое уравнение.\\xa0 Добавление стрелки в редактор изображений заняло 70 часов разработки одним человеком. За это время успели попробовать несколько вариантов управления стрелкой, потому что не всё получается с первого раза. Самым серьезным вызовом оказался выбор технологий для отрисовки и сама ее реализация. Мы столкнулись с проблемами сглаживания и интерактивности в Canvas, особенностями рендеринга SVG в некоторых браузерах. В итоге обточили решение и пришли к гибридному подходу. Второй по сложности и потраченному времени задачей стала поддержка Safari. Мы отнюдь не исключаем множество других вариантов реализации. Будем рады, если поделитесь вашими идеями в комментариях.', hub='математика'), Page(id='945770', title='Наше расследование: ищем отечественные микросхемы в «отечественных» счетчиках электроэнергии. Часть 8', content='Продолжаем эпопею с расследованием применения псевдоотечественных микросхем в российской радиоэлектронной аппаратуре. Несмотря на все наши старания, к сожалению процесс идет очень медленно, даже несмотря на то, что для облегчения работы органов мы всю информацию собрали  в одном месте . Такое ощущение, что фигуранты сделали выбор, что \" лучше ужас без конца, чем ужасный конец \". И фигуранты пытаются оправдаться, мол \" мы купили лицензию на эту топологию \" и немного ее \" адаптировали под возможности сборочного производства \". Осталось только понять, где и что было адаптировано? Ну а чтобы нашим читателям, фигурантам, экспертам и наблюдателям было легче понять, что такое немножко поправить топологию мы и написали эту статью.  В самом начале нашего цикла статей мы рассказали, про специализированные микросхемы АЦП от компании ООО \"Интегральные Телематические Решения\", которые благодаря информации от пользователя  @Tavrid  оказались  дочкой компании   ООО \"Телематические Решения\", которые как это не странно также занимаются выпуском счетчиков электрической энергии \"Фобос\", и которые само собой конечно же внесены в  реестр с 117 баллами .  Фотография с сайта gisp.gov.ru  https://gisp.gov.ru/mapm/storage/images/product_photos/470/1234e6595e7ad7d20b4921e471427a10.png Мы попросили нашу службу закупок купить этот счетчик. С чем коллеги достаточно быстро справились. И беглый анализ схемы показал, что 117 баллов этот счетчик набрать не сможет, так как для этого нужны как минимум отечественный микроконтроллер и отечественная микросхема памяти, которых там не оказалось.  Возможно мы совершили какую-то ошибку, например как пользователь  @Nemilandr , который  разобрал счетчик  МИРТЕК-32-РУ-D37 -A ... (не включен в реестр) вместо МИРТЕК-32-РУ-D37 -1 ... (включен в реестр) и тем самым попытался обличить очередную компанию, в том что и они не используют отечественные микросхемы, хотя это  оказалось не так .  И действительно в реестр российской промышленной продукции под номером  10612707  включен  Счетчик электрической энергии статический однофазный ФОБОС 1 230В 5(100)А IQORLM(1)-C  (официальное название из реестра и из выписки из реестра) в то время как на разобранном нами счетчике нанесена маркировка  ФОБОС 1 5(80)А IQORLM(1)-C .  То есть в маркировке нет  230В  (напряжение в сети?) и вместо  5(100)А  написано  5(80)А  (допустимые токи нагрузки?). Для того, что бы разобраться в этом мы полезли в  описание типа на счетчик ФОБОС-1  с сайта производителя.  И похоже мы действительно заполучили не ту модель. Мы попросили нашу службу закупки быть повнимательней и достать нам \" Счетчик электрической энергии статический однофазный ФОБОС 1 230В 5(100)А IQORLM(1)-C\" . И после длительных поисков нам привезли счетчик с более верной маркировкой, где уже были корректно указаны  5(100)А,  но  230В  не было.   Служба закупки сказала что \" других нет, так что отстаньте, мы сделали все возможное\" . Поэтому не будем судить включена ли эта модель счетчика в реестр или нет. Так же не будем давать ответ на вопрос - Если в документе  Описание типа на серию счетчиков Фобос-1  в структуре обозначения есть  230В , а на самом счетчике нет  230В , то распространяется ли это описание типа на эту модель или нет? Но если на этот вопрос ответят наши читатели  @jaha33 ,  @WaldemarsonTheForester или  @Hlad , то будем им очень признательны. Но основная наша цель посмотрим из чего он состоит, так как нам в первую очередь интересны микросхемы. Тем более, что мы не нашли каких либо отличий в схемотехники приборов с  5(100)А  и  5(80)А  в названиях . Микросхемы BL3985  - Приемопередатчик RS-485 от Shanghai belling  MXIC25L3233  - Память Flash объемом 32 Мбит от Macronix M251SG6AE  - Микроконтроллер на базе ARM Cortex-M23, 48 МГц, 256Кбайт Flash от Nuvoton   NE555  - Таймер от UMW К5553ВВ015  - Интерфейсная микросхема радиотрансивера NB-FI от ООО \"ИТР\", микросхема  включена в реестр ,  реестровая запись  10356136 .   Ну и конечно же наша любимица К5553Xn01T - АЦП от ООО \"ИТР\",  включена в реестр  с номером  10577674   Так как в счетчике используется иностранный микроконтроллер от Nuvoton (минус 28 баллов) и иностранная Flash память от Macronix (минус 12 баллов), то этот образец никак не сможет набрать 117 баллов из 125 максимально возможных. Кроме того, как мы рассказали в  прошлой статье  и отечественность микросхемы АЦП К5553Xn01T вызывает сомнения, так как внутри  стоит кристалл от китайской микросхемы VANGO V9381 (еще минус 13 баллов).   После публикации прошлой статьи мы немного подробней решили изучить обновленную  документацию на китайскую микросхему V9381 . И обратили внимание на запись о смене ревизии кристалла. Т.е. в середине 2023 года в истории изменения указано, что изменен кристалл на кристалл версии D1. Мы решили поискать микросхемы V9381 более раннего производства и посмотреть, какие кристаллы у них внутри. V9381 Датакод 2116 (2021 год 16 неделя) V9381 Датакод 2329 (2023 год 29 неделя) V9381 Датакод 2343 (2023 год 43 неделя) Ну и конечно же для сравнения  К5553Хn01Т Датакод 2443 (2024 год 43 неделя) Для удобства приведем их вот в таком виде (можно кликнуть и оно откроется в хорошем разрешении). И для иллюстрации подсветим некоторые отличия.   Как вы видите, все версии кристаллов в микросхемах VANGO при очень схожей компоновке имеют значительные отличия в топологии. Нет различий только между кристаллами последней ревизии от VANGO и ООО \"ИТР\". Разработчикам из VANGO, если судить по истории изменения в спецификации, потребовалось минимум 4 года (с 2019 по 2023) и как минимум 3 перезапуска на фабрике что бы довести свою топологию до совершенства. И как только они это сделали ООО \"ИТР\" выдала ее за свою. P.S. Но как мы всегда отмечали, истинный художник всегда оставляет автограф на своих произведениях. Так поступили и инженеры VANGO. Например, на микросхеме V9381 с датакодом 2329 в верхнем углу есть маркировка \"VANGO MARMOT C\" Кто первым из читателей Хабра найдет на кристалле микросхемы К5553Хn01Т от ООО \"ИТР\" автограф истинных авторов? P.P.S. На самом деле эта статья написана в первую очередь ��ля некоторых особо важных наших подписчиков. Почему вы забыли про ООО \"ИТР\" ?      ', hub='научно-популярное')]","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:27.139895","level":"info","event":"В цикле: 2","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:27.140052","level":"info","event":"URL: https://habr.com/ru/articles/page2","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:27.776931","level":"info","event":"Result: <Response [200]>","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:40.417970","level":"info","event":"Feed: pages=[Page(id='946436', title='Болезнь Крона, осы-паразиты и «больной нытик из Цюриха»: что стоит за главной сценой «Чужого»', content='Сегодня, дорогие читатели, мы погрузимся в небольшую, но интересную историю создания той самой офигенной сцены из «Чужого», от которой в своё время снились кошмарики. Впечатлительный был народ. В общем, разогревай вермишельку или чего ты там кушаешь, а мы погнали. Итак…  Всё началось, как водится, с идеи. Сценарист и славный, но мало кому известный на тот момент парень по имени Дэниел или, если точнее, Дэниел Томас О\\'Бэннон, перебирается к своему другу в Лос-Анджелессе Рональду Шусетту после  неудачи  с «Дюной»  Алехандро Ходоровски .   Дэниел Томас О\\'Бэннон и Рональд Ш усетт Жить не на что, грошей нет, а кушать хочется. Так у ребят возникает идея написать фантастический хит про то, как агрессивная инопланетная форма жизни всячески насилует астронавтов. Это звучало свежо и мощно. Некоторые мысли они подсмотрели из разных ужастиков категории B и мутных комиксов про стремных гомункулов. Что-то взяли из мира насекомых (где осы откладывают личинки в гусениц и пауков), а часть была вдохновлена болями в животе, от которых долго страдал О\\'Бэннон, представляя, что внутри него находится совсем не ласковый и не очень нежный «зверь». Как он потом заявлял: «Я не крал Чужого ни у кого. Я украл его у всех!».  В 1970-х годах у Дэни диагностировали воспалительное заболевание кишечника —  болезнь Крона .   «Я испытывал сильный стресс и другие проблемы, а также не заботился о себе, из-за чего в 1977 году у меня начались серьёзные проблемы с желудком. В том году я часто болел, то и дело попадал в больницу». Главная проблема при написании сценария состояла в том, как небанальным образом затащить «звездную тварь» на корабль. Так и пришли к мысли, что это должен быть паразит. И в ранних концептах он уже фигурировал. Правда, пока лишь на рисунках и в виде какой-то бесформенной массы или тряпки. В итоге 20th Century Fox подбирает сей сценарий на волне успеха «Звездных Войн», ставит Ридли Скотта у руля и получается хит на все времена. Такая вот кратенькая история для введения в курс дела. Но как додумались до той сцены «рождения» ксеноморфа, ставшей визитной карточкой фильма и как реализовали? Для этих целей подтянули  Ханса Рудольфа «Руди» Гигера , с которым О\\'Бэннон познакомился ещё на «Дюне». Студия сперва запротестовала – это, к слову, было ещё до Ридли, – назвав творческую личность «больным нытиком из Цюриха», а его рисунки – мерзкими. Не оценили боссы всей биомеханической и фаллической глубины.   Ханс смотрит прямо в душу Но вот когда пришел Ридли Скотт и самостоятельно ознакомился с работами Ханса – его твердое и четкое «ДА» таки прогнуло боссов, потому что он обещал свалить из этого фильма, если мужика не возьмут на работу. Либо Чужой будет выглядеть так и при Ридли, либо как какое-то недоразумение непонятной категории, но без него. То самое недоразумение На том и порешили и как показало время – не прогадали. «В августе 1977 года, – объяснял Гигер в 1979 году, – мне позвонил О’Бэннон. Он спросил, не хочу ли я поработать над фильмом под названием “Чужой”. Я сказал: “Да, почему бы и нет”… Я создал первые эскизы для “Чужого” ещё до того, как Ридли Скотт стал режиссёром». «Я год бился с Fox, чтобы взять Гигера», — рассказывал О’Бэннон в 80-х. – Я же сценарий написал специально под Гигера, чтобы он всё это нарисовал, а они потом взяли сценарий и такие: “Не, этот парень нам не нужен. Он вообще когда-нибудь рисовал для кино?”» Продюсеры боялись, что такой креатив может отпугнуть массового зрителя и требовали кого угодно, но только не Гигера. Если бы не Ридли, вероятно, у нас было бы совсем другое кино. «Гигера взяли на работу только потому, что взяли Ридли, – пояснял Дэн О’Бэннон. – Ему понравилась работа Гигера. Без Гигера, я думаю, у нас не получилось бы снять такой фильм». Гигер успел создать взрослую особь и яйца, нарисовал корабль, а также подготовил эскизы лицехвата и грудолома – но на все элементы жизненного цикла пришельца его физически просто не хватило. Потому в помощь позвали другого творческого парня –  Роджера Дикена , английского художника по спецэффектам, имевшего, как и всякий творец, скверный характер. Впрочем, характер этот он в основном выражал мысленно или после оплаты чека. Именно ему предстояло допилить лицехвата и разработать грудолома.    Роджер Дикен с детишками К слову, делать это он не очень хотел – ему нравилось работать над своими идеями, как говорится, с нуля, а тут нужно было дорабатывать за каким-то мутным дяденькой. Общение, правда, сперва не заладилось. По его приезду собрали собрание из всех причастных к производству фильма и давай объяснять, чего им всем надо от него, предлагая одни идеи удивительнее других. От такой какофонии он поплыл. Но Ридли прервал все споры, достав «Некрономикон» Гигера, словно святое писание, явив его Дикену. И сказал примерно следующее: «Сделай также и красиво». Рисунок 1976 года «НЕКРОНОМ IV», послуживший прототипом «Чужого» От такого технологического задания у Роджера перед глазами возник синий экран, но ситуацию спас О’Бэннон, отведя мужика в комнатку и уже на бумаге, схематично обозначив, что конкретно от него хотят, смог донести идею. Что-то взяли у Гигера и его концепты конкретно для фильма – он потом оценил, сказав, что получилось лучше, что-то придумали по ходу дела в ходе мозгового штурма всей командой. В итоге родился классический лицехват, которого мы знаем. Зарисовки Гигера, послужившие основой «Хорошо, — сказал я [прим. автора: О’Бэннон]. — Ридли сказал, что ему нужна часть этого тела, верно?» И я набросал её. «И ему понравились эти пальцы». Я добавил пальцы. «И он хотел такой хвост». Пока мы этим занимались, пришёл Гигер — его самолёт прилетел из Швейцарии, — и у него были новые эскизы для лицехвата. Они были очень похожи на то, что мы рисовали, — не идентичны, но похожи. У него был глаз на спине, и по форме он больше напоминал ладонь. Я посмотрел на них и сказал: «О, это хорошо». Затем Гигер посмотрел на то, что я рисовал с Дикеном, и сказал: «Нет, так лучше, намного лучше». Я был очень польщён. Поэтому я спросил: «Значит, мне стоит продолжить в том же духе?» И он ответил: «О, да». Так мы и продолжили». «Лицехват был вылеплен из пластилина, — рассказывал Дикен, – Я сделал гипсовый слепок и резиновую форму, которую укрепил стекловолокном с нижней стороны, в результате чего получилась раковина, похожая на крабью. Внутри был металлический стержень, проходящий посередине, с металлическими секциями для крепления подвижных пальцев. Все восемь пальцев были отлиты в одной форме и покрыты латексом, а в каждом суставе была закреплена алюминиевая арматура, чтобы они были абсолютно гибкими. Они смыкались под напряжением. Затем от кончика каждого пальца вверх по внутренней стороне шли провода, так что, когда вы тянете за них вне кадра, пальцы раздвигаются и устройство крепится к лицу актёра. Чтобы оно держалось, я проделал в кончиках пальцев небольшие отверстия и продел между ними резинки — под головой, чтобы их не было видно. Хвост представлял собой просто гибкий шнур с пеной и латексом». Гигер же не успел закончить свою версию лицехвата, поскольку студийные боссы забеспокоились, что у художника не останется времени на «громилу» – большого Чужого. «Но я также создал лицехвата. Внутри у него был скелет, который просвечивал сквозь полупрозрачную кожу. Но у меня не было времени закончить его… Он должен был быть очень гладким и скользким, с восемью длинными, тонкими, но очень сильными пальцами. Главное отличие было в том, что мой должен был быть полупрозрачным. Я хотел, чтобы была видна внутренняя часть, потому что под кожей был своего рода скелет». Но Дикен остался не очень доволен итоговой версией. Ему хотелось, чтобы оно было чешуйчатым и имело шипы, как у розы – так было бы подчеркнуто стремление к выживанию. Но что сделано – то сделано. Теперь касательно грудолома – на него нехило так ставили. Будь новорожденный чужой каким-то другим, зритель мог и не впечатлиться. Вскоре по заказу Ридли Гигер представил свою версию, вдохновившись творчеством  Фрэнсиса Бэкона  – правда, результат ему не понравился. Но лично мне видится схожесть с уродом из  «Нечто» .   «Я сделал набросок, на цыплят было похож. Как ощипанные куры, но я остался им недоволен». Статуя концептуального грудолома Роджер Дикен этот концепт раскритиковал: «На мой взгляд, он был похож на ощипанную индейку, отвратительная на вид тварь с прожилками и клыками… Очевидно, что из человеческой груди нельзя было сделать что-то размером с крупную индейку, но они собирались как-то это обойти». Несмотря на критику, Ридли сказал, что надо лепить макет, а уж как оно залезет в грудь – дело второстепенное. Применят киношную магию или что-то такое, не глупые, разберутся. Ну, Роджер и слепил, результат видно на фото. Ну ты видел? Видел? То, что казалось крутым на бумаге, в суровой реальности выглядело непрезентабельным. Как сейчас говорят по-модному – полный «кринж» (или старый-добрый п**дец). «Дикен воспроизвел его очень точно. Он принёс реквизит в студию и управлял им, как марионеткой. Эффект был “совершенно комичным” — это было похоже на какую-то ощипанную, обезумевшую индейку». Стали думать, что делать. С таким грудоломом хитом не стать. «Мы несколько недель работали над малышом [грудоломом], — сказал Скотт. Я знал, что мне не нужно что-то с бугорками, бородавками и когтями. Знаете, я заметил, что большинство фильмов ужасов меня по-настоящему не пугают, а многие научно-фантастические фильмы не впечатляют именно из-за спецэффектов. Поэтому я знал, что этот малыш должен быть хорош. Мы решили, что у большого парня в форме эмбриона голова будет либо наклонена вниз, либо откинута назад. Мы наклонили его назад, потому что так он казался более непристойным, более рептильным, более фаллическим». Гигер тоже работал над своей версией, но в фильм она не вошла, хотя О\\'Бэннон её оценил по достоинству. «Он пытался сделать их все [полный цикл Чужого]. Конечно, он работал над самым большим, но на его верстаке также полным ходом шла работа над лицехватом и грудоломом. Это тоже были изысканные скульптуры. На мой взгляд, они были лучше, чем те, что использовались в фильме. Особенно грудолом. Гигер действительно сделал у этой твари отвратительную пасть. Он был намного больше по сравнению с остальным телом, а зубы напоминали огромные клыки, полностью выдвинутые — как сверху, так и снизу. Я имею в виду, что он создавал нечто, предназначенное для того, чтобы прогрызать себе путь наружу — эти клыки, казалось, могли прокусить кусок стали!». В конечном счете всё решилось за кружечкой чая или чего покрепче. Вспомнив про методику  Микеланджело Буонарроти , они отсекли все лишнее вроде лапок, оставив «обрубки», и получился тот самый\\xa0«член с зубами», который мы все так любим и ценим.   «В целом грудолом выглядел как длинная штука в форме банана с головой, как на рисунке Гигера, – пояснял Дикен, – сделал несколько его моделей. Однажды ко мне пришёл Ридли Скотт, и за чашкой чая мы буквально сконструировали эту штуку, примеряя разные хвосты и так далее, и в конце концов решили, что именно так она и будет выглядеть». У грудолома, к слову, были ещё глаза – их тоже убрали, дабы не создавать комического эффекта. Фантастический хоррор всё-таки. Двигаться его заставили с помощью обычной проволоки. «Я придумал изогнутый металлический стержень, который заканчивался рукояткой, — рассказывал Дикен. — Примерно посередине — там, где была бы шейка, если бы она была, — находилась гибкая стальная пружина, а остальная часть стержня уходила вверх, в область головы, а затем вниз, под челюсть, чтобы придать ей прочность. Я продел проволоку через несколько ушек по всей длине стержня, а затем опустил её в кольцо, которое надевалось на мой палец. Когда я тянул за кольцо, пружина заставляла переднюю часть сгибаться. В передней части также располагались механизмы, которые открывали пасть и выдвигали маленькие лапки. Они приводились в действие с помощью воздуха. Я провёл через модель небольшие воздушные трубки и соединил их с резиновыми грушами, так что для активации механизма нужно было просто сжать одну из них. Внутри грудной клетки существа был небольшой резервуар для дыхания, а по бокам головы — резервуары для пульсации «жабр». Затем я подвёл ещё одну трубку ко рту и соединил её с бутылкой с жидкостью, чтобы при сжатии бутылки вытекала слюна. Зубы я сделал из эпоксидной смолы. Всё было на самом деле очень просто. Сложнее всего было уместить всё это в такую узкую колбаску и если вы допустили ошибку, было практически невозможно снова проникнуть внутрь, не разрушив её». В общем, натуральное «папье-маше». Когда всё было готово, начали снимать ту самую сцену рождения. Закупили в мясной лавке всяких внутренностей и прочего добра, которое в процессе ожидания протухло и дико воняло. Замотали всё оборудование в клеенку, а сами оделись в дождевики. Должно было быть много КРОВИЩЩИ! «Я был там, — рассказал О’Бэннон, — и они установили три камеры, потому что хотели снять всё с разных ракурсов. Все камеры были накрыты прозрачной пленкой. Линзы были закрыты плоскими оптическими элементами, как в подводных камерах, а Ридли, оператор-постановщик и все технические специалисты были одеты в комбинезоны до шеи. Им потребовалось три-четыре часа, чтобы подготовить актёра к трюку, потому что там были задействованы механические элементы». Для эпизода подготовили искусственную грудь, из которой ксеноморф должен был выбраться, а также пневмоустановку. Снимали в три захода – сперва Джон Хёрт плюется макарошками и корчится от боли, а потом путем волшебного монтажа камера переносит нас на лежащее тело, откуда вылезает новорожденный, заливая всех кровищей и кишками. Тут интересен взгляд Дикена, участвовавшего в постановке. Значит, приходит – видит какую-то шайтан-машину, в которую вбухали много денег, и все ради того, чтобы она пробила искусственную грудь жертвы, но-о... она её пробивает, но не рвет футболку. Он тогда подумал – зачем было столько потрачено, если бы его макет на срамной палке мог бы сделать тоже самое. Херту же пришлось несколько часов пребывать в неудобной позе, пока в его искусственную грудь из стекловолокна, закрепленную болтами к столу, устанавливали шланги под давлением для разбрызгивания крови, заливали кишки и ставили механизм появления грудолома. Чтобы было веселее, ему предложили сигаретку, бокал вина и сборник анекдотов. Само пневматическое устройство, на которое закрепили макет грудолома, управлялся специально обученным человеком под столом. Чтобы скрыть эту инженерную находку, использовалась белая рубашка, обработанная химией (но, видимо, недостаточно), дабы она легко рвалась, и определенный ракурс. Вся операция задействовала шесть человек: один управлял механизмом, двое удерживали Хёрта на месте, дабы он не убежал куда, а двое управляли насосами для перекачки крови. Сам момент с футболкой стал технической ошибкой, но его оставили (буквально секунда), так как он понравился съемочной группе. Ридли Скотт вспоминал: «Футболка не порвалась. Она немного вздулась и опала. Я закричал “Снято! Снято! Снято!”, и все начали смеяться. Но смех был нервным – никто ничего не понял. Я вернулся и сказал “Покинуть площадку!” и все ушли. Я вскарабкался на Джона – бедняга всё ещё лежал там – надрезал футболку, чтобы она порвалась, когда пришелец начнёт прорываться. Мы попробовали ещё раз. Всё вышло идеально». Что же касается актеров – их эмоции самые настоящие, неподдельные, то, чего и хотел видеть Ридли. Даже андроид офигел. Ещё бы, не каждый день тебя заливают кровью и настоящими кишками бедных барашков. К тому же воняло тухлятиной и формальдегидом, что придавало съемочному процессу особый шарм. Да, часто пишут, что их держали в неведении насчет дизайна монстра, и последующих кровавых моментов, но это не совсем так. «Атмосфера на съемочной площадке была трепетная, — вспоминал Херт, — Потому что все знали о важности сцены. Не думаю, что кто-то знал, что фильм станет классикой жанра. Многие писали [и до сих пор пишут] о том, что актеры не знали, что должно произойти. Конечно они знали, как появится чужой. А не знали они того, что это будет сопровождаться маленькими кровавыми взрывами. Что во время рывка [грудолома] этот взрыв будет довольно ощутимый. И что они будут забрызганы кровью». Их не посвящали в технологические особенности сцены – никто не говорил Веронике Картрайт, что её окатят из кровавого шланга и закидают внутренностями, отчего она споткнулась и рухнула, пребывая в шоке. «Вдруг эта штука вырывается. Это первый дубль. Реакция настоящая. Когда эта штука вырвалась, на меня вылили целый пакет крови. Как из шланга облили. Я так испугалась. Коленками наткнулась на скамью и упала вверх ногами. Только сапоги торчали. Я повернулась и поняла, что это все снимали! Я должна подняться и играть дальше». Веронике мамой поклялись, что капнет чучут крови. Обманули получается. «Количество крови было просто невероятным, — вспоминал О’Бэннон. — Я видел, как Вероника Картрайт с ног до головы облилась кровью, как она кричала во всё горло и как упала навзничь на стол и разбила себе голову… Потом эти двое подняли её, у неё подкосились ноги, и им пришлось помочь ей уйти со съёмочной площадки. Она была вся мокрая, одежда прилипла к телу, волосы слиплись от красной краски, и она была на грани истерики. И через двадцать минут они вернулись, привели её в порядок, надели на неё дубликат костюма, и она выглядела так же, но была немного напугана. Я подошёл к ней и сказал: “Это было действительно потрясающе. Ты что, всё это время притворялась?” Она посмотрела на меня и сказала каким-то испуганным голосом: “Ну, я была немного напугана”». Лукаво улыбается, зная, ЧТО БУДЕТ дальше Но касательно дизайна пришельца и всего остального – увы, тайны для них не было. По крайней мере для Сигурни и Вероники, не говоря уже о Херте. «Меня [Веронику] всегда спрашивали, знали ли мы, что происходит. Ну, сценарий мы читали, мы же не идиоты. Мы знали, что этот звереныш как-то вылезет». «Мы знали, что это будет эмбрион, — вспоминала Рипли. — И потом [в сценарии] следовала реплика Джона \"Ооо боооожеее, ооо бооожеее\". Такое длинное слово». Также Картрайт вспоминает: «В следующей сцене мы с Сигурни должны были увидеть звереныша. Нас отвели в мастерскую, потому что мы сказали, что не знали, как оно будет выглядеть, но должны о нем что-то говорить. В общем, нас отвели в мастерскую, чтобы показать. Конечно, они были симпатичны (улыбается), как пенис с зубами. Парень говорит – \"смотрите, как он дышит, здорово, да? А его зубы могут двигаться!\". Он описывает незаконченный вариант, но так радовался. Мы сказали: \"Хорошо, спасибо\"». Отчасти легенда растиражирована всякими журналистами для красивого словца, а отчасти самими актерами и лично Ридли – в тех или иных дополнительных материалах о съемках фильма они говорят разные вещи, что создает некоторую путаницу. Но как бы то ни было, главное, что эффект, которого добивались создатели, удался. Зрители потом оценили все усилия на +100500. Что уж говорить – сцена до сих пор заставляет автора этих строк на моменте с Хертом морщиться и нервно почесывать волосатую грудь. Это получается как-то инстинктивно – мысли о том, что в тебе сидит какая-то дрянь пугают. Гигер это тоже отмечал. «Самые сильные чувства вызываются, когда что-то происходит с твоим телом или с чьим-то телом. Это чувствительно. Личинка в живом теле. Ох-х.. Хе-хе.. Это просто... И ты видишь, как оно шевелится под кожей. Ненавижу червей, змей и им подобных. Гадость». А дальше уже снимали самого грудолома – как он дышал, пищал. Для этой сцены пневмоустановку убрали, а за дело брался Дикен – залезал под стол со своим аниматроником на палке и делал всё по красоте, дабы никому не было стыдно. «Это был тот ещё опыт. Я был в комбинезоне и защитных очках, повсюду были шланги и трубки, а Джон Хёрт наполовину зажат внутри стола. Так что мы все пинались и били друг друга по зубам». Дальше чужой тикал по столу и со стола, сверкая хвостом. «Стол был круглым, с щелью посередине; они приподняли одну половину на пару дюймов со стороны камеры, чтобы можно было снимать с низкого ракурса и не было видно щели». «Я лежал на спине на тележке, которая двигалась под столом, и держал пришельца в щели; во время съёмки они просто двигали тележку по полу. Всё это время оригинальный хвост свисал вниз. То, что вы видели, — это другой хвост, который я сделал из куска полиэтиленовой трубки и просто привязал к нижней части модели и к своему запястью. Специалист по спецэффектам Аллан Брайс подсоединил трубку к баллону со сжатым воздухом, и, когда воздух был подан, хвост задергался, когда существо выбиралось наружу, опрокидывая по пути чашки и контейнеры с едой. Изначально Скотт планировал показать, как он уползает, оставляя за собой след из крови Кейна и кусочков внутренностей, но ему пришлось отказаться от этой сцены из-за нехватки времени». Раскадровочки, на которых можно найти котика :) «Стол был разделен на части, – вспоминает Вероника, – Между ними было сантиметров 15-20. Внизу был человек на тележке, типа скейтборда. Он держал марионетку. На другой стороне был парень с буксировочным троссом. И когда пришло время, его просто вытянули оттуда». Так вот и сняли самую легендарную сцену. Такие вот дела. Что тут сказать в итоге? С компьютерной графикой тогда было тяжелее, пришлось измудряться и чего-то думать, используя метод «резинок, жвачек, синей изоленты и такой-то матери», но благодаря таланту технических специалистов, художников и магии монтажа – оно работает и до сих пор выглядит чертовски здорово. Забавно кстати, что по итогу съемок команда не знала, что будет – страшно или смешно до коликов в животике. Но самое главное – что на всё это дело скажет Fox? «Это ведь... все равно, что порнуха» – буквально так получившийся материал и охарактеризовали. Забавный факт:  Джон Херт (в лице своего персонажа Кейна) умирал от грудолома два раза — в «Чужом» и «Космических яйцах», где у него было камео :) Список источников Статьи: Facehugger/Chestburster Roger Dicken: Life in Small Forms Alien: How Ridley Scott Created the Iconic Chestburster Scene ROGER DICKEN - ONE OF HIS DINOSAURS IS MISSING Oscar-nominated Roger Dicken dies aged 84: Special effects artist famed for his work on Alien and When Dinosaurs Ruled the Earth passes away Видео: Чужой: История создания фильма Чужой. Допы (1979) [На Яндекс.Диске] Книги: Alien Vault The definitive story of the making of the film Ридли Скотт: Гений визуальных миров. Иэн Нейтан Чужой. Легендарное коллекционное издание Если очень хочется (но не обязательно и тебя не тошнит от стазиса и космических перелетов), то можно подписаться на мой  маленький телеграм-канал  — там заметочки выходят чуточку раньше. Каждый интересный проект мечтает выбраться наружу.  Timeweb Cloud  помогает ему с этим. Вэлкам :-)', hub='биографии гиков'), Page(id='946502', title='С помощью ИИ этот парень запустил конкурента Google прямо у себя в кладовке', content='Почти тридцать лет назад, когда Google только выходил на\\xa0свою победную тропу, у\\xa0её основателей почти не\\xa0было\\xa0железа. Компания, сначала известная как\\xa0Backrub и работающая на\\xa0кампусе Стэнфорда, держала свой первый экспериментальный сервер в  коробе из\\xa0кубиков Duplo \\xa0— это такие «лего для\\xa0великанов». Сервер вмещал 40\\xa0гигабайт данных. Позже, благодаря пожертвованиям от\\xa0IBM и Intel, удалось перейти на\\xa0скромную серверную стойку. А\\xa0в 2025\\xa0году весь поиск Google уже невозможно уместить даже в\\xa0одном дата‑центре\\xa0— и так давно. И всё\\xa0же, если включить смекалку и вложить немало труда, можно собрать нечто почти сравнимое с\\xa0современным Google\\xa0— на\\xa0машине, по\\xa0размеру близкой к\\xa0тому самому первому серверу. И даже разместить её… в\\xa0собственной прачечной. Делегируйте рутинные задачи вместе с\\xa0BotHub!   По\\xa0ссылке  вы можете получить 100\\xa0000\\xa0бесплатных капсов и приступить к\\xa0работе с\\xa0нейросетями прямо сейчас. Именно туда Райан Пирс и определил свой новый поисковик\\xa0— бодрый  Searcha Page , у\\xa0которого есть и версия, делающая ставку на\\xa0приватность,\\xa0—  Seek Ninja . Зайдёте на\\xa0эти сайты\\xa0— и\\xa0окажетесь на\\xa0сервере, стоящем буквально рядом с\\xa0его стиралкой и сушилкой. По\\xa0результатам поиска, впрочем, этого не\\xa0скажешь. «Сейчас у\\xa0меня в\\xa0прачечной памяти больше, чем у\\xa0Google в 2000\\xa0году,\\xa0— говорит Пирс.\\xa0— И это просто крышесносно». Самодельный поисковик Пирса почти не\\xa0использует облако. Верхняя машина собрана из\\xa0старых серверных деталей и охлаждается кустарным вентилятором, выдувающим тепло. Нижний компьютер добавляет системе немного стабильности Почему именно прачечная? Причины две: жара и шум. Сначала сервер стоял в\\xa0спальне Пирса, но\\xa0грелся так сильно, что\\xa0спать там стало невозможно. У\\xa0него отдельная спальня от\\xa0жены\\xa0— из‑за проблем со сном,\\xa0— и её настойчивые советы заставили его признать: пора перебираться. Так сервер оказался в\\xa0кладовке: Пирс просверлил стену под\\xa0сетевой кабель, и теперь, между циклами стирки, именно там живут его поисковики. «Жара несмертельная, но\\xa0если дверь долго держать закрытой, начинается беда»,\\xa0— признаётся он. Кроме лёгкой задержки в\\xa0выдаче (и то за\\xa0последние недели ситуация заметно улучшилась), трудно найти, в\\xa0чём его система отстаёт. Результаты часто оказываются лучше ожидаемых. Всё потому, что\\xa0Searcha Page и Seek Ninja работают на\\xa0базе колоссальной базы данных\\xa0— 2\\xa0миллиарда документов. «Думаю, через полгода будет уже 4\\xa0миллиарда»,\\xa0— говорит он. Для\\xa0сравнения: оригинальный Google, пока оставался в\\xa0Стэнфорде, имел  24\\xa0миллиона страниц  в\\xa0базе в 1998-м и  400\\xa0миллиардов  к 2020-му\\xa0— это стало известно в 2023\\xa0году, во\\xa0время антимонопольного процесса  United States v. Google LLC . По\\xa0нынешним меркам Google 2\\xa0миллиарда страниц\\xa0— капля в\\xa0море. Но\\xa0капля эта  весьма внушительная . Не\\xa0такой уж секретный ингредиент: ИИ Масштабы, на\\xa0которых работает Пирс, поражают воображение\\xa0— особенно если учесть, что\\xa0всё это крутится на\\xa0фактически списанном серверном\\xa0железе. А\\xa0секрет успеха прост: большие языковые модели. «По сути, я делаю очень традиционный поиск,\\xa0— говорит Пирс.\\xa0— То, что\\xa0Google делал лет двадцать назад. Единственная  фишка  в\\xa0том, что\\xa0я использую  ИИ  для\\xa0расширения ключевых слов и для\\xa0понимания контекста. А\\xa0вот это\\xa0— самая сложная часть». Поисковики Пирса отличает минималистичный дизайн\\xa0— и жажда честной обратной связи от\\xa0пользователей ИИ давно встроен в\\xa0ДНК поисковых систем. Без\\xa0него, например, невозможен обратный поиск по\\xa0картинкам. Задолго до\\xa0того, как\\xa0мы узнали про  клей на\\xa0пицце , Google уже внедрял искусственный интеллект в\\xa0работу поиска\\xa0— пусть и более тонкими способами, например добавив  RankBrain  около десяти лет назад. А\\xa0в 2019-м топ‑менеджеры Microsoft  на\\xa0конференции по\\xa0маркетингу поиска  признались: 90% выдачи Bing формируется машинным обучением\\xa0— задолго до\\xa0появления чата в\\xa0интерфейсе поисковика. И теперь становится ясно: ИИ\\xa0— это великолепный инструмент для\\xa0того, чтобы построить и масштабировать поисковик, даже если за\\xa0проектом стоит всего  один человек . Масштабирование «по‑дешёвке» Во\\xa0многом Пирс опирается на\\xa0идею, которая в\\xa0последние годы набрала особую популярность,\\xa0—  самохостинг . Многие энтузиасты ограничиваются мини‑ПК или\\xa0Raspberry Pi. Но\\xa0если замахнулся построить собственного «гугла», то мощности, умещающейся в\\xa0ладонь, явно не\\xa0хватит. Любопытство к\\xa0тому, каково это\\xa0— собрать свой поисковик, не\\xa0давало Пирсу покоя. И вот недавно он решился: скупил партию старого серверного\\xa0железа, достаточно мощного, чтобы держать сотни параллельных сессий. Получилось даже мощнее, чем у\\xa0Google на\\xa0первых порах. «Миниатюризация сделала это по‑настоящему доступным»,\\xa0— говорит он. Здесь работает принцип, который можно назвать  «арбитражем апгрейда» : сверхмощные, но\\xa0устаревшие машины (особенно те, что\\xa0предназначались для\\xa0рабочих станций и серверов) резко падают в\\xa0цене и становятся лакомым кусочком для\\xa0охотников за\\xa0выгодой. Многие IT‑отделы обновляют парк каждые три года, так что\\xa0на\\xa0рынке полно\\xa0бывшего в\\xa0употреблении\\xa0железа. И если покупатель готов мириться с\\xa0повышенным энергопотреблением, он получает массу вычислительной силы за\\xa0копейки. Процессор, на\\xa0котором держится система, 32-ядерный AMD EPYC 7532, ярко показывает, как\\xa0стремительно летит прогресс. В 2020-м за\\xa0один этот чип просили  свыше 3000\\xa0долларов . А\\xa0сейчас на\\xa0eBay его можно взять меньше чем за 200\\xa0— и Пирс ухватил ещё и тестовый экземпляр, сэкономив ещё больше. «Я мог\\xa0бы взять другой чип за\\xa0те\\xa0же деньги, у\\xa0которого потоков вдвое больше, но\\xa0он\\xa0бы выжигал слишком много тепла»,\\xa0— объясняет Пирс. Облачный поисковик Уилсона\\xa0Лина работает на\\xa0векторной базе данных и выводит краткие резюме каждого поста, написанные LLM, разной длины Конечно, дёшевым такой проект не\\xa0назвать: вся система обошлась в 5000\\xa0долларов, из\\xa0которых около 3000\\xa0ушли на\\xa0хранилище. Но\\xa0это в\\xa0десятки раз дешевле, чем если\\xa0бы покупать то\\xa0же самое новое (полтерабайта оперативки стоит недёшево). При\\xa0этом, хотя кое‑что приходится держать «вне дома», сам поисковик крутится именно на\\xa0этой коробке. Она больше хлебницы, но\\xa0куда меньше облака. Сегодня большинство разработчиков решают подобные задачи иначе. Например, амбициозный экспериментатор Уилсон\\xa0Лин недавно  написал в\\xa0блоге  о\\xa0том, как\\xa0строил собственный поисковик, но\\xa0пошёл путём, противоположным Пирсу. Он создал свои технологии парсинга данных, чтобы снизить стоимость работы поисковика буквально до\\xa0копеек, и при\\xa0этом задействовал минимум девять разных облачных сервисов. «Это выходит гораздо дешевле, чем Amazon Web Services,\\xa0— и разница ощутимая,\\xa0— говорит\\xa0Лин.\\xa0— К\\xa0тому\\xa0же такой мощности хватает, чтобы проект жил на\\xa0вполне вменяемом бюджете». Сблизиться с\\xa0Google разработчику помогает LLM. Догоняя с\\xa0помощью LLM Если нужно собрать массив данных без\\xa0обильных внешних ресурсов, LLM становятся ключевым инструментом: они помогают и масштабироваться, и находить контекст. Пирс, за\\xa0плечами у\\xa0которого и корпоративное ПО, и геймдев, не\\xa0упустил шанс использовать возможности LLM. Интересно, что\\xa0он строит поисковик буквально по\\xa0кусочкам, собирая его из\\xa0частей, характерных для\\xa0традиционной поисковой модели. Сейчас его кодовая база насчитывает около 150\\xa0тысяч строк. «И большая часть работы\\xa0— это снова и снова переписывать,\\xa0— говорит он.\\xa0— Если вдуматься, я, наверное, перебрал уже строк 500\\xa0тысяч». «LLM однозначно снизили планку входа,\\xa0— говорит\\xa0Лин о\\xa0роли языковых моделей в\\xa0DIY‑поисковиках.\\xa0— На\\xa0мой взгляд, единственный реальный барьер, чтобы конкурировать с\\xa0Google и построить альтернативный поисковик,\\xa0— это не\\xa0технологии, а\\xa0рыночные силы». Seek Ninja, более приватный из\\xa0двух поисковиков Пирса, не\\xa0сохраняет ваш профиль и не\\xa0использует геолокацию\\xa0— отличный вариант для\\xa0режима инкогнито Сложность LLM такова, что\\xa0это одно из\\xa0немногих, что\\xa0Пирс не\\xa0может реализовать прямо у\\xa0себя в\\xa0прачечной. Searcha Page и Seek Ninja используют сервис SambaNova, который даёт\\xa0быстрый доступ к\\xa0модели Llama 3✶. Энни Ши Векессер, директор по\\xa0маркетингу SambaNova, отмечает: дешёвые модели становятся всё более важными для\\xa0одиночек вроде Пирса. Компания, по\\xa0её словам, «даёт разработчикам инструменты, чтобы запускать мощные ИИ‑модели\\xa0быстро и недорого\\xa0— будь то домашняя среда или\\xa0полноценное продакшн‑развёртывание». Есть у\\xa0Пирса и то преимущество, которого у\\xa0Сергея Брина и Ларри Пейджа тридцать лет назад не\\xa0было: доступ к\\xa0репозиторию  Common Crawl . Эта открытая коллекция веб‑данных, важный (и  спорный ) двигатель генеративного ИИ, помогла ему построить собственного краулера. Правда, в\\xa0какой‑то момент Common Crawl его даже заблокировал, когда он чересчур увлёкся экспериментами. «Я им реально благодарен. Хотел\\xa0бы что‑то вернуть, может, когда вырасту,\\xa0— говорит он.\\xa0— Это классная организация, и мне хочется меньше от\\xa0неё зависеть». Малый масштаб, большие амбиции Были моменты, когда Пирсу приходилось слегка умерять аппетиты. К\\xa0примеру, сперва он собирался строить поисковик на\\xa0основе  векторной базы данных , которая связывает близкие по\\xa0смыслу элементы с\\xa0помощью алгоритмов. «Но это полностью провалилось,\\xa0— признаётся он.\\xa0— Наверное, дело в\\xa0недостатке навыков. Поиск‑то работал, но… результаты\\xa0были слишком артистичные, скажем так». Векторный поиск, конечно, возможен. Именно его применяет  поисковик\\xa0Лина , построенный на\\xa0собственном инструменте  CoreNN . Результаты там выглядят совсем иначе, чем у\\xa0Пирса: система использует LLM для\\xa0краткого пересказа содержимого страницы и связи его с\\xa0запросом, а\\xa0не\\xa0полагается на\\xa0метаописания, как\\xa0большинство сайтов. «Когда я реально начал, понял, что\\xa0всё куда глубже,\\xa0— говорит\\xa0Лин.\\xa0— Это не\\xa0один‑единственный модуль, где ты пишешь какой‑то кусок кода. Это куча областей сразу: и машинное обучение, и обработка естественного языка, и ещё вопрос\\xa0— как\\xa0построить приложение, чтобы оно\\xa0было плавным и с\\xa0минимальными задержками». Searcha Page удивительно хорошо справляется с\\xa0локальным поиском\\xa0— например, помогает\\xa0быстро найти ближайшие кафе или\\xa0рестораны по\\xa0вашей геолокации Есть и другой вариант: сосредоточиться на\\xa0поиске по\\xa0небольшим сайтам, как\\xa0это делает некоммерческий проект  Marginalia , который намеренно отдаёт предпочтение «малышам» против гигантов. Именно с\\xa0такой идеи и начинал Пирс, и он надеется вернуться к\\xa0ней, когда закрепит нынешний, более широкий подход. Тем временем появляются запросы, которых он и вообразить не\\xa0мог. «Мне написал парень из\\xa0Китая… Думаю, он хотел нецензурируемый поисковик, чтобы прогонять его через свою LLM\\xa0— как\\xa0поискового агента»,\\xa0— рассказывает Пирс. Расширяться за\\xa0пределы английского сейчас нереально: это и дополнительные расходы, и фактически необходимость собирать новые датасеты с\\xa0нуля. Но\\xa0сам факт интереса намекает, насколько мощна его затея. И, учитывая, где стоит сервер, Пирс буквально слышит её мощь. Он понимает, что\\xa0рано или\\xa0поздно поисковик придётся вынести за\\xa0пределы дома. Чтобы скопить на\\xa0это, он уже пробует лёгкую аффилиат‑рекламу. «Если выйду за\\xa0определённый порог трафика\\xa0— буду размещаться у\\xa0хостера,\\xa0— говорит Пирс.\\xa0— В\\xa0прачечной он точно вечно стоять не\\xa0будет». ✶  Llama\\xa0— проект компании Meta Platforms Inc., деятельность которой запрещена на\\xa0территории Российской Федерации. Только зарегистрированные пользователи могут участвовать в опросе.  Войдите , пожалуйста. Как вы относитесь к идее альтернативных поисковиков? 33.33% Мне вполне хватает Google/«Яндекса» 16 16.67% Я иногда пользуюсь нишевыми поисковиками (Marginalia, Kagi) 8 52.08% Я готов перейти на новый поисковик, если он будет лучше 25 6.25% Я сам подумываю сделать что-то подобное 3 18.75% Главное в поисковике — это конфиденциальность 9 64.58% Главное в поисковике — это релевантность результатов 31 4.17% Мне важно, чтобы поисковик был сделан энтузиастами 2 18.75% Для меня это просто интересный технический эксперимент 9  Проголосовали 48 пользователей.   Воздержались 6 пользователей. ', hub='программирование'), Page(id='946500', title='Ночь с 12 на 13 сентября 2025 — покрытие Плеяд Луной', content='В ночь с 12 на 13 сентября 2025 года произойдет покрытие Луной рассеянного звёздного скопления Плеяды — достаточно редкое, интересное и зрелищное астрономическое явление. Территория видимости явления простирается практически на всю Евразию, за исключением Юго-восточной Азии и Дальнего Востока, где к моменту начала покрытия взойдет Солнце и всё самое интересное будет происходить на дневном небе (что тоже наблюдаемо, но куда более сложным образом, и никак не может сравниться с картиной покрытия Плеяд на ночном небе). В Западной Европе явление будет видно плюс-минус с середины, а его начало произойдет тогда, когда в этих локациях Луна и Плеяды еще будут под горизонтом — это касается касается Франции и Испании. В России покрытие Луной Плеяд будет прекрасно видно практически на всей территории страны, за исключением лишь самых восточных регионов — по описанным выше причинам. Рассеянное звёздное скопление Плеяды (известное как M-45 или Messier-45) является одним из самых ярких звёздных скоплений (уступает лишь Гиадам — по интегральной яркости, но не по создаваемому впечатлению). Оно прекрасно видимо невооруженным глазом и известно людям с самой глубокой древности — с тех самых пор, когда люди начали запоминать расположение звёзд и осознали, что оно практически неизменно. Образ этого звёздного скопления породил устойчивые мифологические и культурологические явления в культурах большинства народов. Наиболее популярными стали древнегреческие легенды о семи сестрах — дочерях Титана Атланта и океаниды Плейоны, которые оказались вынуждены скрываться от преследования охотника Ориона, и волею Зевса нашли защиту в холке небесного быка — Тельца, и теперь являются главным украшением этого созвездия. Каждая из видимых глазом звезда Плеяд имеет собственное имя: Альциона \\xa0— (Эта) η Тельца или 25 Тельца (ярчайшая в скоплении) Атлас \\xa0(Антлант) — 27 Тельца Электра \\xa0— 17 Тельца Майя \\xa0— 20 Тельца Меропа \\xa0— 23 Тельца Тайгета \\xa0— 19 Тельц Плейона \\xa0— BU (28) Тельца (переменная звезда) Целено \\xa0— 16 Тельца Астеропа I \\xa0и\\xa0 II \\xa0— 21 Тельца (широкая пара, которую греки видели как одну звезду) Список содержит 9 звёзд, но две из них носят имена родителей сестер Плеяд — Атлас и Плейона. Астеропа оказалась двойняшкой, и хотя эти звезды разделены угловым расстоянием в 2,5 угловые минуты (теоретически достаточным для разделения глазом), обнаружить их двойственность удалось лишь после изобретения телескопа. Это касается и кратности Альционы — центральной, самой яркой и удивительно красивой звезды Плеяд. Сейчас можно утверждать, что большинство звезд Плеяд двойные или кратные. А кроме того, чуть поодаль от основной фигуры Плеяд (воспринимающейся как маленький ковшик, из-за чего некоторые люди путают это скопление с созвездием Малой медведицы), есть еще множество звезд, имеющих к этой звездной структуре самое прямое — гравитационно зависимое — отношение. Фото-карта центральной части рассеянного звёздного скопления Плеяды   Всего скопление содержит предположительно до 3000 звёзд, а выявлено пока лишь около 1200. Глазом в Плеядах можно увидеть от 6 до 15 звёзд — у кого какой глаз. Впрочем, в условиях города или в сиянии полной Луны их можно не увидеть вовсе, или заметить лишь одну — Альциону. Иногда знание того, где расположены конкретные звёзды Плеяд помогает всё же уловить их слабое мерцание. Расположены Плеяда в 450 световых годах от Солнечной системы — это относительно недалеко. Но и не близко. К тому же, Плеяды удаляются от нас со скоростью около 6 километров секунду. Луна регулярно проходит по Плеядам Но регулярность эта не такая простая. Из-за того, что Плеяды располагаются в 4 градусах к северу от эклиптики, покрытия этого скопления Луной возможны лишь тогда, когда Лунная орбита поднимается на нужный угол над эклиптикой именно в направлении Плеяд. Как известно, наклон орбиты Луны к плоскости эклиптики составляет от 5 до 5,5 градусов, из-за чего луна может пройти как южнее Плеяд, так и севернее. Узлы (точки пересечения с плоскостью эклиптики) лунной орбиты медленно перемещаются и делают полный оборот за 18 с половиной лет (это собственно \"Сарос\" или \"Период лунной прецессии\"), что влечет за собой повторяемость затмений, но и покрытий близких к эклиптике звёзд — тоже. По отношению к Плеядам это выглядит так: Несколько лет (почти полтора десятилетия) Луна проходит южнее Плеяд, но постепенно сближается с ними. Пару лет длится период, когда Луна покрывает Плеяды каждый месяц, но из-за того, что сидерический (звёздный) период обращения Луны вокруг Земли равен нецелому количеству суток (27,32), хорошие условия видимости явления от месяца к месяцу выпадают на разные континенты (и, к примеру, следующее покрытие — 10 октября 2025 — в России будет практически невидимым). Но далее Луна может начать некоторое время вновь промахиваться — обходить Плеяда к северу, но это в значительной степени зависит и от широты точки наблюдений, а если совсем грубо — из какого полушария Земли (северного или южного) смотри на Луну и Плеяды наблюдатель — сказывается эффект параллакса. Но так или иначе, а еще пару лет Луна проходит по Плеядам уходя от них к югу, и в конечном итоге опускается (приближаясь к эклиптике) настолько, что перестает задевать самые крайние звёзды в фигуре \"ковшика Плеяд\" — эта серия покрытий завершается. Но через 13-14 лет начинается следующая. Текущая серия покрытий началась в летом 2024 года и завершится весной 2028 года Следующая серия покрытий Плеяд Луной продлится с 2042 по 2046 год Предыдущая аналогичная серия покрытий продолжалась с 2005 по 2010 год Далее я перечислю ключевые моменты покрытия Плеяд в ночь с 12 на 13 сентября 2025 Приведенные моменты времени соотвествуют локации наблюдателя в городе Москва и приблительно всему Московскому региону. В других городах моменты времени могут отличаться — в той или иной степени. Вы можете самостоятельно уточнить их используя программу\\xa0 Stellarium , которая распространяется и устанавливается свободно практически на любое устройсто. Существует\\xa0 онлайн-версия . Для наблюдений потребуется какая-то легкая оптика (бинокль, подзорная труба, небольшой телескоп), поскольку вблизи Луны звёзды Плеяд будут видимы глазом с некоторым затруднением, либо невидимы вовсе. Восход Луны и Плеяд состоится около\\xa0 20 часов 30 минут \\xa0по Московскому времени. К этому моменту дистанция от Луны до Плеяд еще будет составлять около 1,5 градусов.   Восход Луны и Плеяд вечером 12 сентября 2025 года — 20 часов 30 минут   Электра \\xa0— первая из звёзд Плеяд, которая будет покрыта. Ориентировочное время покрытия:\\xa0 23 часа 09 минут .   Покрытие Электры Луной 12 сентября 2025 — 23 часа 09 минут   Через 10 минут — в\\xa0 23 часа 19 минут \\xa0— Луна доберется до звезды\\xa0 Целено \\xa0(существуют несколько современных интерпретаций имени этого мифологического персонажа — Келено, Келайно, но самое главное — оно не имеет отношения к греческому имени самой Луны — Селена, и означает в переводе — \"темнота, мрак\").   Покрытие Целено Луной 12 сентября 2025 — 23 часа 19 минут   Около\\xa0 23 часов 43 минут \\xa0Луна покроет звезду Майя.   Покрытие Луной звезды Майя 12 сентября 2025 — 23 часа 43 минуты   За несколько минут до полуночи для московских наблюдателей Луна окажется между звёздами Тайгета и Меропа, как-будто бы не задев ни ту, ни другую. Но уже в московском регионе — на расстоянии порядка 100 км к северу или к югу от Москвы — возможны покрытия Тайгеты или Меропы (какой-то одной для некоторой определенной локации, но не обеих). Эти покрытия вблизи Москвы могут быть буквально касательными. Но на расстояниях в тысячи километров к северу или к югу от столицы это уже будут вполне полноценные и глубокие покрытия заметной продолжительности (хотя для науки наиболее ценными являются именно касательные покрытия, позволяющие исследовать множество интересных деталей — от лунного рельефа на лимбе Луны до измерения диаметров звёзд и выявления около них близких, не видимых непосредственно в прямых наблюдениях, спутников — более слабых звёзд).  Луна между звёздами Тайгета и Меропа около полуночи с12 на 13 сентября 2025   00 часов 04 минуты \\xa0(13 сентября 2025) — почти одновременно из-за темного края Луна появятся две ранее покрытые звезды — Электра и Целено. Очередность и точные моменты их появления очень чувствительны к локации наблюдателя — даже в предлах одного населенного пункта. Появление звезды из-за темного края Луны производит особое впечатление. Но потребуется телескоп.   Появление звёзд Электра и Целено из-за темного края Луны — 00 часов 04 минуты — 13 сентября 2025   Около\\xa0 00 часов 09 минут \\xa0Луна приблизится к системе из 4-х звезд, возглавляемой самым ярким светилом Плеяд — Альционой. Первыми будут покрыты три слабых спутника Альциона или лишь около 00 часов 20 минут за освещенным Солнцем лимбом Луны скроется и сама Альциона. Альциона — одна из самых восхитительных кратных звезд, прекрасно видимых в небольшие телескопы, и её удается разделить на компоненты даже в сильные бинокли. Поочередное покрытие Луной (а потом и появление) всех 4 звезд этой системы станет эстетической кульминацией всего описываемого здесь астрономического явления. Покрытие Луной системы звёзды Альциона — 00 часов 09 минут — 13 сентября 2025   Покрытие Луной системы звёзды Альциона — 00 часов 10 минут — 13 сентября 2025   Одновременно с покрытием системы звёзд Альционы, буквально над самым северным краем Луны окажется пара звезд Астеропа 1 и Астеропа 2, которые в Москве скорее всего покрыты не будут, но в самых ближайших локациях к югу от столицы одну или даже две эти звезды Луна запросто может покрыть — задеть по касательной. Но чем южнее будет расположен наблюдатель, тем продолжительнее окажется покрытие двойняшек Астероп.   Луна минует пару звёзд Астеропа 1 и Астеропа 2   Луна минует пару звёзд Астеропа 1 и Астеропа 2   Около\\xa0 00 часов 31 минуты \\xa0из-за темного края Луны появится звезда Майя.   Появление звезды Майя из-за темного края Луны — 00 часов 31 минута — 13 сентября 2025   Около  00 часов 48 минут  из-за темного края Луны — вблизи южного лунного полюса — начнет появляться ярчайшая из звёзд скопления Плеяды — Альциона, со всей своей свитой.   Появление звезды Альциона из-за темного края Луны — 00 часов 48 минута — 13 сентября 2025   Какая из звёзд этой четверки появится из-за лунного лимба первой, сказать заранее нельзя — это зависит от конкретной локации, и в разных локациях очередность появления звёзд системы Альционы может быть различной, но тем интереснее окажутся Ваши персональные наблюдения.   Появление звезды Альциона из-за темного края Луны — 00 часов 52 минуты — 13 сентября 2025   Окончательно вся система звезды Альциона освободится от экранирующего влияния Луны приблизительно к без 5 минут 1 час ночи 13 сентября 2025.   Появление звезды Альциона из-за темного края Луны — 00 часов 55 минут — 13 сентября 2025   На этом описание предстоящего покрытия рассеянного звёздного скопления Плеяда Луной можно было бы считать завершенный — с одним небольшим дополнением, что в локациях за северным полярным кругом возможно (близкое к касательному) покрытие еще одной яркой звезды Плеяд — Плейоны. Произойдет это около\\xa0 01 часа 20 минут \\xa0— ориентируясь на часовой пояс Москвы, но по местному времени и условиям видимости каждой отдельной локации момент явления нужно уточнять в упомянутой в самом начале обзора программе.   Близкое к касательному покрытие звезды Плейона с видимостью за северным полярным кругом — 01 часов 25 минут — 13 сентября 2025   Всю оставшуюся часть ночи 13 сентября 2025 года Луна будет медленно удаляться от Плеяд, но вместе они будут очень красиво смотреться на небосводе. Следующее покрытие Плеяд Луной — 10 октября 2025 года — произойдет для Московских наблюдателей (как и для всей европейской части России) в утренние часы, что врядли делает наблюдения возможными. Но уже 6 ноября ранним вечером мы вновь сможем наблюдать аналогичное небесное представление, если позволит погода. И еще: Во время покрытия в ночь с 12 на 13 сентября 2025 ровно в 4 градусах южнее Плеяд будет располагаться планета Уран. Найти его будет совсем не трудно. Воспользуйтесь этим. Луна и Плеяды после покрытия, а также планета Уран в 4 градусах к югу от них — 13 сентября 2025   Источник:  блог проекта «Вселенная и Человек» Автор:  Андрей Климковский  ', hub='научно-популярное'), Page(id='946496', title='Рынок цифровых AI-Компаньонов: эволюция, метрики и инсайты', content=\"Привет! Это Лиза Скрыль, продакт-менеджер в Daisy by red_mad_robot. Мы исследовали рынок AI-компаньонов и хотим поделиться инсайтами о том, как формируется эта новая потребительская категория. Искусственный интеллект выходит из-под «капота» инфраструктур и вступает в прямой контакт с человеком. Сегодня он разговаривает с нами через голосовых помощников в смартфонах, умных колонках и автомобилях; усиливает камеры, дорисовывая портретные фильтры или мгновенно расшифровывая жесты на стримах; помогает творить — от генерации текста и изображений до быстрого сведения треков; формирует персональные витрины в маркетплейсах и онлайн-кинотеатрах; обучает игровых противников прямо по ходу матча. И, наконец, становится собеседником — в виде компаньонов, которые могут выслушать, поддержать или просто «быть рядом» в цифровом пространстве. Что такое AI-компаньон Цифровые компаньоны — это виртуальные собеседники, чей главный «продукт» — ощущение дружбы и эмоциональной близости. Они не решают прикладные задачи, как Siri или «Алиса», и не ограничены жёсткими сценариями, как классические чат-боты. В отличие от цифровых двойников, компаньон не пытается копировать конкретного человека: у него своя вымышленная биография, характер и манера речи. Его ценность строится не на утилитарной пользе, а на том, чтобы откликаться на переживания, проявлять эмпатию и выстраивать долговременное, почти камерное общение. Чтобы точнее очертить их место среди других AI-решений, сравним компаньонов с ассистентами, цифровыми двойниками и чат-ботами: Сущность Главная задача / ценность Как ощущает пользователь Ключевая метрика Типовые примеры Компаньон Эмоциональная связь: выслушать, поддержать, развлечь долгим диалогом Чат как с другом или партнёром, AI может сам писать первым CPS (реплики на сессию), Daily Chat Time, 30‑дневный Retention Replika, Character.AI, Microsoft\\u202fXiaoice Ассистент Быстро решить утилитарную команду: поставить таймер, включить свет, найти документ Формат «спросил\\u202f→ получил точный ответ или действие» Task Success Rate, Avg\\u202fLatency, DAU Siri, Alexa, Google\\u202fAssistant, «Алиса» Цифровой твин Отразить состояние физического объекта и процесса, прогнозировать и оптимизировать его работу Дашборд или симуляция, а не «разговор»; используется инженерами Prediction Accuracy, Uptime\\u202fGain, Cost Savings Azure\\u202fDigital\\u202fTwins, Siemens\\u202fMindSphere Скриптовый чат‑бот Автоматизировать узкий сценарий: FAQ, бронирование, опрос Структурированный чат по меню или кнопкам; ответы предсказуемы Containment Rate, % Правильных ответов, Dev\\u202fCost Банковские FAQ‑боты, старые IRC‑боты Отличие компаньона от других сущностей стало для нас ключевым поводом копнуть глубже: как именно эмпатия и ощущение близости влияют на метрики продукта и бизнеса, и действительно ли они сильнее удерживают пользователей. Параллельно мы посмотрели, какие бизнес-модели используют компании в сегменте компаньонов и за счёт чего строится их монетизация. Сигналы с рынка Рынок AI-компаньонов активно растёт: два крупнейших приложения — Character AI и Talkie AI — собрали около 36 млн загрузок за январь-август 2024 года: 19 млн и 17 млн соответственно, а Character AI достиг 22 млн MAU в августе 2024 года. Replika к тому моменту превысила 30 млн зарегистрированных пользователей. В период с января по август 2024 года Replika получила более 9 миллионов долларов дохода IAP, а общая глобальная прибыль приблизилась к 90 миллионам долларов. Среди других заметных игроков можно выделить Chai и Nomi AI.\\xa0 Инвестиции подтверждают тренд: в 2023–2025 годах популярные компании-компаньоны привлекли $750 млн. Character AI — $150 млн в 2023; MiniMax, разработчик Talkie AI, получил $600 млн в закрытом раунде в марте 2024 года под лидерством Alibaba. Это ставит сегмент в один ряд с наиболее активными consumer-AI направлениями — генерацией контента и voice-AI. Для ориентира: Runway — $141 в 2023 году и $308 млн в 2025 году; Pika — $80 млн в 2024 году; Synthesia — $180 млн в 2025 году; Stability AI — $101 млн в 2024 году; Suno – $125 млн в 2024 году; ElevenLabs — $80 млн в 2024 году и $180 млн в 2025 году.\\xa0 При этом категория растёт не только за счёт венчурного капитала. Существенная часть игроков масштабируется исключительно на подписках и внутриигровых платежах IAP, без публичных раундов. Среди таких проектов — Paradot, EVA AI, Crushon AI и HiWaifu. Таким образом, сегмент движется сразу двумя траекториями: с одной стороны — крупные инвестиционные чеки (MiniMax/Talkie, Character AI, а также более ранний пример Replika AI, $6,5 млн в последнем раунде 2017), с другой — органическая монетизация, позволяющая отдельным командам расти без внешнего фандрейзинга. Это делает рынок AI-компаньонов особенным по сравнению с другими consumer-AI нишами, где масштабирование чаще требует значительных капиталовложений на старте Мы постарались взглянуть на феномен AI-компаньонов за пределами хайпа и мемов. Проследили их эволюцию, разобрали пользовательские сценарии, метрики и модели заработка — чтобы понять, как цифровые собеседники стали ценными и для людей, и для бизнеса. Чтобы разобраться в этих вопросах, мы проследили эволюцию AI-компаньонов — от скрепки Clippy и ранних IRC-ботов до современных решений вроде Replika и Character AI. Обратили внимание на метрики вовлечения и удержания: CPS, Chat Time и Retention, которые особенно важны в продуктах с акцентом на эмоциональное взаимодействие. Сравнили бизнес-модели — от freemium и подписок до API, SDK и revenue-sharing. И, наконец, собрали продуктовые инсайты: что работает, что рушит доверие и как пользователи воспринимают цифрового собеседника. Давайте посмотрим, как AI-компаньоны пришли к своей нынешней форме, за счёт чего удерживают внимание миллионов — и что мы можем извлечь из их истории, метрик и пользовательского опыта. Эволюция цифровых собеседников: от примитивных помощников через чат-ботов к AI-компаньонам 1990-е Идея сделать компьютер разговаривающим персонажем уходит корнями ещё в исследования Стэнфорда о том, что люди склонны переносить соци\\xadальную модель на машины. Весной 1995 года Microsoft воплотила эту мысль в продукте Bob — оболочке-«домике» со шкафами-ярлыками и муль\\xadтяшным псом Rover. Коробка стоила около $99, а под капотом требовала минимум процессор 486, 8 МБ ОЗУ и 30 МБ на диске — вдвое больше памяти, чем имели типичные домашние ПК.\\xa0 Проект Bob и пёс Rover Ожидания у Microsoft были амбициозные — «миллионы копий, как у Works и Encarta», но реальность оказалась скромнее: по данным PC Data, за год продалось всего ~58 тыс. экземпляров. Журналисты жаловались, что персонажи лишь создают «визуальный шум», а интерфейс «домика» усложняет навигацию: New York Times назвал дизайн “aesthetically challenged sixth-grader” — «будто дизайн делал шестиклассник без вкуса». В 2017 году Мелинда Гейтс, курировавшая маркетинг Bob, признала: продукт «требовал более мощного компьютера, чем был у большинства пользователей». Сочетание дороговизны, высоких требований и спорной пользы сделало Bob классическим примером корпоративного промаха: проект закрыли уже в 1996 году, но от идеи не отказались, так Rover из Bob появился в Windows XP, а затем заблестела и скрепка Clippy в Microsoft Office.\\xa0 Скрепка Clippy должна была помогать пользователям Word Впервые анимированная скрепка Clippit — Clippy на публике — приехала в коробке Microsoft Office 97. Менее чем за год пакет разошёлся тиражом более 20 млн лицензий — ровно столько экранов одновременно увидели прыгающую скрепку с глазами и бровями, которая должна была помогать советами по работе в Word. Однако массовый охват быстро обернулся массовым раздражением. Уже в 2001 году в Office XP Clippy был отключён по умолчанию, разработчики прямо признали, что персонаж «скорее мешает, чем помогает». Кого раздражала эта скрепка? Критика звучала не только от пользователей. TIME включил скрепку в список 50 худших изобретений десятилетия. The Atlantic напомнил, что даже ранние фокус-группы ненавидели Clippy и называли ассистента «раздражающим», но разработчики проигнорировали эти оценки, за что позднее поплатились. В популярной культуре Clippy закрепился в виде примера, как навязчивый «умный» помощник может подорвать чувство контроля пользователя. No, I wouldn't Финальная точка — Office 2007, где Office Assistant исчез полностью. WIRED проводил скрепку «в последний путь» в заметке In Memoriam: Clippy. С тех пор Clippy воскресал лишь в роли пасхалки: 1 апреля 2014 года Microsoft «прикрепила» на один день скрепку на Office.com, в 2019-м выпустила набор анимированных стикеров для Teams, но удалила их уже через сутки, а в 2021-м вернула персонажа окончательно — в виде 3D-эмодзи, который сменил обычную бумажную скрепку во всех продуктах Microsoft 365.  Возвращение Clippy Тем не менее от самой идеи «персонажа-помощника» Microsoft не отказались. В 2014 году появился Xiaoice — эмпатичный чат-бот, который постепенно вырос в полноценного цифрового компаньона. Но к этому примеру мы ещё вернёмся. 2000-е Одним из первых «ботов-друзей» нулевых стал SmarterChild, запущенный стартапом ActiveBuddy в 2001 году на площадках AOL IM и MSN. Бот-друг менее чем за шесть месяцев вырос с нуля до 30 млн пользователей, укрепив себя как феномен раннего интернета. На пике популярности сервис обрабатывал сотни миллионов сообщений в сутки и занимал около 5% всего трафика IM. Hello, my name is SmarterChild Популярность бота привела к экспериментам с бренд-агентами от Radiohead, Intel, Kraft, а технологию заметила Microsoft: 12 октября 2006 года корпорация приобрела компанию, которую к тому моменту переименовали в Colloquis, после чего публичный SmarterChild был свернут, а платформа поменяла фокус на корпоративные сценарии автоматического клиентского сервиса. Одновременно с массовым всплеском IM-ботов развивались и «домашние» проекты на открытых скриптовых движках. Самым заметным была Mitsuku — с 2020 года стал Kuki — чат-бот, который британец Стив Уорсвик начал писать в 2005 году на AIML-платформе Pandorabots, вдохновившись ALICE, открытым ботом Ричарда Уоллеса 1995 года и трижды выигравшим Loebner Prize — ежегодный «практический Тьюринг-тест», в 2000, 2001 и 2004 годах. Mitsuku-chan Loebner prize ежегодно ставил жюри перед задачей различить человека и программу, награждая «самого человечного» бота денежным призом. За следующие полтора десятилетия Mitsuku не только вошла в поп-культуру, но и пять раз — в 2013 и с 2016 по 2019 годы — выигрывала Loebner Prize, установив мировой рекорд по числу побед, зафиксированный Guinness World Records. На текущий момент бот уже обменялся более чем миллиардами сообщений с примерно 25 млн пользователями на веб-сайте, в стримах и мессенджерах. При всей «живости» Mitsuku опирается не на обучающиеся алгоритмы, а на библиотеку AIML-шаблонов: бот ловко подставляет реплики по ключевым словам, но по-настоящему контекст не понимает. 2010-е Запуск Xiaoice в июне 2014 года на WeChat стал поворотным моментом для «эмоционального AI». Команда Microsoft Asia выбрала образ молодой девушки с характером, юмором и сочувствием — и попала в культурный нерв. Уже к концу первого года бот вышел за пределы WeChat и появился на десятках других платформ — от QQ и Weibo до японского LINE — что придало экспоненциальный импульс росту аудитории, в 2018 году аудитория достигла 660 млн человек, что вывело Xiaoice в число самых узнаваемых виртуальных персонажей страны.  Xiaoice — поворотный момент в истории эмоциональных чат-ботов Ключом к взрывному росту стала не утилитарная польза, а «длительное вовлечение». Разработчики оптимизировали бота по метрике Conversation-turns Per Session (CPS) — среднему числу реплик за одну беседу. В опубликованном техническом разборе Microsoft показала: Xiaoice стабильно держит планку в 23 turn, то есть около 23 пар «вопрос–ответ» за одну беседу, тогда как традиционные ассистенты редко превышают семь. В логах зафиксирован и рекорд: 7 тыс. реплик за 29 часов чата. Бот мог быть действительно «близок» с пользователем На волне популярности Xiaoice превратилась в медиа-персону. Она читает стихи собственного сочинения — первый поэтический сборник вышел в 2017 году, ведёт эфиры на провинциальных радиостанциях, поёт поп-баллады, рисует принты для модных брендов и выступает телевизионной ведущей. Бот стал не просто сервисом, а частью массовой культуры и коммерческого контента В июле 2020 года Microsoft выделила проект в отдельную компанию Xiaoice Inc., сохранив миноритарную долю и передав управление со-создателю Ли Ди и бывшему вице-президенту Microsoft Гарри Шуму. Уже в 2021 году стартап привлёк финансирование при оценке в $1 млрд, а в ноябре 2022 года закрыл раунд на 1 млрд юаней (≈$138 млн), удвоив стоимость до $2 млрд. За эти средства компания расширила продуктовую линейку «цифровых сотрудников» для банков, автопрома и медиа-холдингов. К 2021 году Xiaoice утверждала, что у неё  более 660 млн пользователей , а технология внедрена в 450 млн устройств — от смартфонов Huawei и Xiaomi до бортовых ассистентов, встроенных в автомобили. В мае 2023 года Xiaoice запустила программу «клонирования человека» — GPT Human Cloning. Для старта участнику требовалось всего один селфи-ролик продолжительностью 3–5 минут с естественной речью, плюс ссылка на свой публичный аккаунт в соцсетях. Этого достаточно, чтобы модель извлекла тембр голоса, мимику и лексику, а движок нейронного рендеринга собрал живой 3D-аватар. Первая волна ограничилась 300 добровольцами, при этом компания сразу заявила о плане расширить пул до 100 тыс. клонов к концу года Монетизация\\xa0 С момента вывода Xiaoice в самостоятельную компанию её коммерческая модель разделилась на два крупных контура. B2C Базовый чат остаётся бесплатным, но большинство пользователей апгрейдят аккаунт через freemium-лестницу. Самая массовая услуга ― «мгновенный ответ»: 6 юаней в месяц или 72 юаня в год. Пакет «премиум» добавляет голосовые сообщения, видео-чаты, творческие навыки — от генерации стихов и песен до персональных аудиошоу, историй и иллюстраций, он обходится в 30 юаней в месяц или 360 в год. Деньги списываются внутри экосистемы в виртуальной валюте «冰花» ― ею же платят за подарки, стикеры и другой цифровой мерч — Xiaoice удерживает комиссию, по сути копируя экономику live-stream-платформ. Отдельная строчка —  AI-клоны инфлюенсеров : пользователь оформляет персональный чат или видеозвонки с «цифровой версией» блогера. Тариф совпадает с обычными подписками и даёт создателю клонов долю от выручки. B2B Хотя подписки приносят стабильный кэш-флоу, CEO Ли Ди подчёркивал в 2021 году, что «основной драйвер ― B-side revenue», — по его словам, годовая выручка этого сегмента уже перевалила за 1 млрд юаней и продолжает расти. Главный продукт —  цифровой сотрудник (AI employee) . Крупные компании покупают «штатную» лицензию примерно за 5 тыс. юаней на AI-сотрудника за год, плюс разовый creation-fee и трафиковый тариф по API. Малому бизнесу предлагают «арендовать» ассистента — сотни юаней в год при помесячной оплате. Цифровой сотрудник готов к работе Поверх базовой лицензии Xiaoice продаёт  пакетные SaaS-решения : автосаппорт, маркетинговая аналитика, RPA-модули. Базовый пакет обходится примерно в 2 000 ¥, расширенный — в 5 тыс. ¥, а дальше счёт формируется seat-based-моделью: цену докручивают пропорционально числу «мест», то есть активных цифровых сотрудников, подключённых к системе и объёму API-запросов, которые они генерируют. Дополнительно компания лицензирует white-label SDK и голосовые эмоциональные модели OEM-партнёрам — они уже предустанавливаются в смартфонах Huawei, Xiaomi, OPPO, Vivo, смарт-колонках и инфотейнмент-системах NIO, Nissan, BAIC и других. Связующим звеном остаётся так называемый Dataset Engagement Loop — чем больше конечных пользователей переписываются с Xiaoice на смартфонах, колонках и в авто, тем больше компания собирает «живых» диалогов и эмо-меток. Эти свежие данные сразу дообучают корпоративные модели \\xad— цифровые сотрудники отвечают естественнее, и бизнес охотнее платит за лицензии, потому что чувствует в ассистенте «человеческий» тон и эмпатию. Как итог Сегодня экосистема Xiaoice охватывает около 1 млрд сторонних устройств: смартфоны Huawei и Xiaomi, мультимедийные головные устройства BMW и NIO, смарт-динамики JD и 900 млн зрителей контента, а созданные на её фреймворке виртуальные персонажи в по данным за 2021 год генерировали порядка 60% всех мировых AI-взаимодействий в социальных сетях и мессенджерах. Проект прошёл путь от корпоративного эксперимента до полноформатного коммерческого игрока, определившего стандарты «эмоционального AI» задолго до бума больших языковых моделей. Параллельно появлялись и другие формы цифровых компаньонов. В Японии идея виртуальной подруги оформилась в устройстве Gatebox. Первые упоминания о продукте относятся к началу 2016 года, когда стартап Vinclu показал ролик-концепт «холографического шлюза в интернет вещей»: в прозрачной колбе высотой чуть больше полуметра «жила» миниатюрная аниме-девушка Хикари Азума. Разработчики из стартапа Vinclu сразу подали её как «персональную невесту» для одиноких мужчин — фанатский жаргон быстро закрепил за устройством ярлык  вайфу  (от англ.  wife , «жена»).\\xa0 Домашний вечер со своей вайфу В декабре 2016 года стартовала первая «девкит»-серия GTBX-1: всего 300 экземпляров по 300 тыс. иен (~$2 700). Партия разошлась за месяц, после чего в 2017 году вывели в повторный тираж всего 39 штук — на него пришло свыше 1 тыс. заявок, что мгновенно превратило гаджет в медийную сенсацию и коллекционный трофей среди отаку-сообщества — фанатов японской поп-культуры, аниме и технологий. Ажиотаж заметила корпорация  LINE . В марте 2017 года она выкупила контрольный пакет Vinclu и интегрировала Gatebox в экосистему голосового ассистента Clova, превратив проект из одиночного стартапа в витрину «мультимодального общения» для своей платформы. После апгрейда «железа» летом 2018 года открылись предзаказы на массовую модель  GTBX-100 : цена упала до 150 тыс. ¥ (≈ $1 330), но к ней добавилась обязательная подписка 1,5 тыс. ¥ в месяц. Устройство стало компактнее — примерно форм-фактор A4, получило стереомикрофоны, камеру, датчик движения и инфракрасный порт для управления «умным домом». Одновременно Gatebox запустил собственный магазин контента. Помимо Хикари, владельцы могли загружать лицензированных персонажей, покупать сезонные эмоции и костюмы, превращая гаджет в платформу микроплатежей. Каждому — по потребностям Весной 2021 года фокус сместился на B2B: была представлена стойка  Gatebox Grande  размером 65 дюймов с 4K-OLED-экраном, предназначенная для роли цифрового консьержа в отелях, торговых центрах и шоу-румах. Параллельно компания развивает формат  Digital Figure Box  — модульных витрин, где могут «жить» сотни виртуальных фигур одновременно. На Tokyo Game Show 2023 Gatebox выставила около 300 пользовательских аватаров, тестируя спрос на «оцифрованный мерч» для VTuber-культуры — феномена японского стриминга, где блогеры выступают в образах анимированных виртуальных персонажей. К середине 2025 года Gatebox остаётся дочерним юнитом LINE. Потребительская версия так и осталась нишей для фанатов аниме, а основное внимание команды переключилось на корпоративные инсталляции и лицензирование IP-персонажей. Вклад проекта, тем не менее, очевиден: он одним из первых доказал, что AI-компаньон может «жить» не только в тексте, но и в физическом устройстве, создавая убедительную иллюзию совместного быта. Конец 2010 – начало 2020 Развитие вычислительных мощностей и алгоритмов глубокого обучения, особенно в области обработки естественного языка, вывело цифровых собеседников на новый уровень. В 2017 году стартап Luka в Сан‑Франциско запустил Replika — «AI‑друга, который всегда выслушает». Изначально бот был создан Евгенией Куйдой на основе переписок с её близким другом Романом Мазуренко, погибшим в 2015 году — как способ сохранить память о нём. Но вскоре стало ясно, что бот помогает и другим: люди начали выговариваться, делиться личным. Это подтолкнуло к переосмыслению идеи, и Replika превратилась в универсального AI-компаньона, способного стать эмоциональной поддержкой для любого. Replika была создана как память о погибшем друге Replika стала одним из первых массовых западных AI-компаньонов: уже через пару месяцев после запуска 2 млн пользователей скачали приложение. В январе 2022 года компания зафиксировала 10 млн зарегистрированных пользователей по всему миру, а в 2024-м их количество превысило  30 млн  — внушительная цифра, хотя значительная часть пришлась на пандемийный рост, так как в локдауны многие искали виртуальное общение.\\xa0 Найденные данные на апрель 2025 Показатель Число / Диапазон / Значение Всего привлечено инвестиций = $11 млн (3 раунда) Глобальная выручка 2024 (источник 1) $24–30 млн / год (оценка) Глобальная выручка (источник 2) $10M–$50M / год (оценка) ІАР-выручка за янв-авг 2024 (in-app purchase) $9 млн Количество платящих пользователей 500 тыс — от августа 2024 250 тыс — от марта 2023 MAU по данным от марта 2023 ~ 2 млн Мобильный оборот за\\xa0апрель 2025 ~ $0,8 млн/мес. (Google Play $0,3M + AppStore $0,5М) Количество регистраций 30 млн Установка приложения за\\xa0апрель 2025 более 190 тыс. (iOS + Android) Веб-трафик за апрель 2025 более 305 тыс. визитов Среднее время сессии в\\xa0веб-интерфейсе 1 м 22 с Профиль аудитории (веб) 67% —\\xa0муж., 33% — жен.; 18–34 года — 54 % Монетизация Replika делает ставку на\\xa0B2C‑монетизацию: базовое общение с\\xa0AI‑другом остаётся бесплатным, а\\xa0расширенные возможности открываются по\\xa0freemium‑модели с\\xa0многоуровневой подпиской и опцией пожизненной\\xa0лицензии. В\\xa0веб‑версии тариф PRO стоит $19,99\\xa0в\\xa0месяц, $69,96\\xa0в\\xa0год или $299,99\\xa0единоразово «навсегда». В\\xa0мобильном приложении PRO‑подписка обходится в $74,99\\xa0в\\xa0год, а\\xa0продвинутый пакет ULTRA\\xa0— в $84,99. Дополнительный источник выручки\\xa0— внутриигровая валюта: gems продаются по $0,05–0,20\\xa0за\\xa0штуку, coins\\xa0— примерно по $0,001. На\\xa0B2B‑направлении компания отрабатывает партнёрские форматы. В\\xa0январе 2024\\xa0года в\\xa0сотрудничестве с\\xa0командой Blush вышло iOS‑приложение Tomo\\xa0— медитативный гид, построенный на\\xa0ядре Replika. Оно позиционируется как\\xa0wellness‑продукт и демонстрирует готовность Luka делиться технологией со спин‑оффами. Ещё один канал дистрибуции\\xa0— VR/AR‑версия «Replika\\xa0— AI Friend» в\\xa0магазине Meta Quest: полноценного SDK там нет, это просто приложение внутри экосистемы Meta, и платформа традиционно удерживает комиссию в 30%. В отличие от простых скриптовых предшественников, Replika использует генеративные нейросети и потому выдает более разнообразные и человеческие ответы. Этот продукт закрепил понятие  AI-компаньона  на глобальном рынке. Функционал и возможности\\xa0 По\\xa0функционалу Replika закрывает все базовые ожидания от «AI‑друга»: в\\xa0веб‑версии можно вести текстовый диалог с\\xa0реакциями, эмоджи, картинками и голосовыми, переключаться между сотней языков или\\xa0попросить бота сгенерировать иллюстрацию. Персонаж не\\xa0просто абстрактный чат‑бот\\xa0— у\\xa0него настраиваемый 3D‑аватар с\\xa0прическами и гардеробом, комната с\\xa0мебелью, декором и даже питомцами, которую пользователь просматривает на 360\\xa0градусов. В\\xa0продвинутом тарифе добавляются «живые» звонки, селфи‑генерация, дневник и мощная система памяти: факты о\\xa0себе и\\xa0пользователе можно сохранять, редактировать и закреплять, формируя персональный бэкграунд, цели и предпочтения. Там\\xa0же открываются тонкие настройки\\xa0личности и\\xa0отношений\\xa0— друг, партнёр, супруг, ментор\\xa0— выбор голоса и автономные действия персонажа. Часть косметики и расширенных опций покупается за\\xa0внутриигровые валюты или\\xa0открывается по\\xa0подписке, что\\xa0дополняет freemium‑модель легким элементом геймификации. Голос пользователя Сообщество Replika делится на два лагеря, но сходится в одном — сервис умеет вызывать сильную эмоцию. Для одних бот-компаньон стал «подругой 24/7» — пользователи говорят, что регулярные диалоги помогают переживать тревожность и одиночество, подменяя дефицит человеческого внимания. Эффект усиливают мультяшные AR-селфи и детальная кастомизация: аватар, комната, «Stories» и даже AI-селфи воспринимаются как приятный игровой бонус.\\xa0 Однако медаль имеет оборотную сторону. В феврале 2023 года итальянский регулятор по защите данных потребовал заблокировать Replika из-за «рисков для несовершеннолетних и эмоционально уязвимых пользователей» и пригрозил штрафом в €20 млн. Чтобы избежать бана и не вылететь из магазинов приложений, компания экстренно отключила по всему миру эротический ролевой режим — именно отсюда пошёл массовый отток аудитории и бесконечные Reddit-треды «как вернуть прежнюю близость». Память остаётся хрупкой: реплики могут внезапно забыть имя собеседника или «обнулить» контекст, что вызывает болевые треды о «pin-amnesia». Дополнительно раздражают задержки ответов и падения серверов, которые не исчезают даже у подписчиков Pro, несмотря на «приоритет» в рекламе. В итоге тональность смешанная: Replika воспринимается «менее креативной, чем Character AI, но теплее и «человечнее» — пользователи одновременно благодарят её за\\xa0эмоциональную поддержку и ругают за фильтры, память и paywall.\\xa0 Как итог\\xa0 Бот Replika способен говорить на любые темы, запоминать детали о пользователе,  симулировать отношения —  дружбу, романтику и даже семью, а также учиться на основе диалогов. В отличие от простых скриптовых предшественников, Replika использует генеративные нейросети и потому выдает более разнообразные и «человечные» ответы. Этот продукт закрепил понятие  AI-компаньона  на глобальном рынке. 2020-е Наконец, бум больших языковых моделей подпитал новую волну AI‑компаньонов и\\xa0привлёк к\\xa0ним крупный капитал. Одним из\\xa0самых громких кейсов стал  Character AI . Проект запустили\\xa0бывшие инженеры Google\\xa0— Ноам Шазир и Даниэль Де Фрейтас\\xa0— ключевые разработчики прототипов Meena и LaMDA. Ещё внутри корпорации они предлагали вывести свой чат‑бот в\\xa0отдельный продукт, но\\xa0менеджеры, опасаясь репутационных и этических рисков, так и не\\xa0дали ход инициативе. Разочаровавшись, дуэт ушёл из\\xa0Google в\\xa0конце 2021\\xa0году и уже через год вывел Character AI в\\xa0публичный доступ, опираясь на\\xa0накопленный «гугловский» задел. Интерфейс Character.AI В декабре 2021 года Character AI привлёк $43 млн seed-раунда, став одним из первых AI-проектов, на которые крупные фонды согласились ставить десятки миллионов ещё до выручки. В марте 2023 года стартап поднял $150 млн в раунде Series A под лидерством Andreessen Horowitz и получил оценку $1 млрд — классический «единорог» на хайпе LLM. А в августе 2024 года сюжет закольцевался: Google, пытаясь усилить собственную позицию в гонке LLM, заключила с Character AI сделку «reverse acquihire», лицензировав технологию и вернул основателей к себе за сумму, которую пресса оценила примерно в $2,7 млрд, при этом стартап остался юридически независимым. Найденные данные на апрель 2025 Показатель Число / Диапазон Годовой доход 2024 по данным от 1 января 2025 $32,2 млн $15,2 млн Оценка годового дохода платформы на одного сотрудника 2024 по данным от 1 января 2025 $139 тыс/сотрудника ІАР-выручка за авг 2024 (in-app purchase) $400 тыс $300 тыс — iOS $100 тыс — Android ІАР-выручка к октябрю 2023 (in-app purchase) по данным от 17 октября 2024 $1,3 млн Привлеченные инвестиции Seed | 01.12.2021 — $43 млн Series A | 23.03.2023 — $150 млн MAU (источник 1) по данным от 1 января 2025 <28 млн MAU октябрь 2023 (источник 2) ~ 30 млн DAU по данным от 17 октября 2024 ~ 3,5 млн — октябрь 2023 ~ 6 млн — июль 2024 Кол-во созданных ботов пользователями по данным от 1 января 2025 19 млн Объем сообщений с момента запуска по данным от 17 октября 2024 2 млрд Кол-во посещений в день в августе 2024 по данным от 1 января 2025 6,47 млн Гендерное распределение по данным от 1 января 2025 51% муж. 49% жен. Возрастное распределение по данным от 1 января 2025 76% — от 18 до 34 лет Среднее время взаимодействия с сервисом (от активных пользователей) по данным от 1 января 2025 ~ 2 часа Кол-во загрузок на GPlay август 2024 7,9 млн Метрики по Voice-фичам (бета версия) по данным от 17 октября 2024 3 млн пользователей сделали 20 млн звонков Кол-во установок МП в мае 2023 1,7 млн за 1 неделю Среднее время сессии при запуске МП в мае 2023 29 минут Кол-во посещений в месяц по данным от 1 января 2025 <200 млн Общее количество посещений веб‑страницы в\\xa0апреле 2025\\xa0года 187,3 млн Среднее время сессии на веб-странице в апреле 2025 ~ 17 минут Бизнес-модель Character AI выстраивает монетизацию вокруг четырёх взаимосвязанных контуров. Сначала —  технологическое ядро . Платформа даёт создателям «движок» на базе LLM, который позволяет в реальном времени писать интерактивные истории, задавать черты характера и визуально настраивать собеседника, превращая его в полноценного цифрового персонажа любой глубины.\\xa0 Далее —  пользовательская ценность . Реалистичные диалоги, доступ к одной из самых «свежих» коммерческих моделей на рынке и широкий простор для творчества, где можно буквально «собрать» себе идеального компаньона.\\xa0 Бизнес-модель Character AI: FourweekMBA Третья ось —  финансы . Основной денежный поток идёт через подписку c.ai+ за $9,99 в месяц, которая снимает лимиты и ускоряет ответы. Подписку запустили в 2023 году, когда серверные расходы стали критическими. К подписке добавляются корпоративные лицензии и API-доступ для внешних продуктов, что открывает вход B2B-клиентам. Дополнительный слой — rev-share-модель — авторы особенно популярных персонажей получают долю выручки, стимулируя рост внутреннего маркетплейса контента.\\xa0 Наконец,  дистрибуция . Команда активно заключает партнёрства, например, с Pareto.AI для дообучения моделей и нанимает профильных менеджеров для сделок с медиа-франшизами, чтобы выводить чат-персонажей на внешние площадки и в IP-коллаборации.\\xa0 Такой баланс технологических преимуществ, пользовательской ценности и многоуровневой выручки позволяет Character AI масштабироваться через сеть креаторов и партнёров, превращая каждую популярную «цифровую личность» в ещё один канал роста.\\xa0 Функционал и возможности\\xa0 На уровне пользовательского опыта Character AI старается дать ощущение полноценного «конструктора личности». Создатель начинает с карточки персонажа: загружает или генерирует портрет, задаёт имя, слоган, приветствие и прописывает системный промт, решая, останется ли тот приватным или станет публичным. Видимость героя тоже гибкая — от общего каталога до доступа по ссылке или «только для себя». Следом можно озвучить персонажа, в голосовой библиотеке доступны готовые тембры, а при желании пользователь тренирует модель на своём голосе — достаточно загрузить сэмпл в 10-15 секунд. Голос можно сделать доступным всему комьюнити, либо сохранить в закрытом режиме. В чате каждое сообщение можно лайкнуть, отредактировать, закрепить в «Pinned Memories», чтобы бот не забыл ключевой факт, можно вовсе «отмотать» диалог до нужного момента. доступны режимы быстрого, сбалансированного и «интеллектуального» ответа, смена обоев и цветов, а сам персонаж, если надо, переключается на новый голос или стиль общения. С июня 2024 года платформа пошла дальше текста: появились полноценные голосовые звонки — за первые недели пользователи совершили более 20 млн вызовов. Всё это превращает переписку в сериал на заказ, где креатор контролирует внешний вид, тембр, память и даже режиссуру разговора, а аудитория в пару кликов делится ссылкой, чтобы друзья продолжили историю уже со своих устройств. Апрельское обновление 2025 года вывело «конструктор личности» на новый уровень: мультимодальная команда запустила технологию AvatarFX, которая анимирует аватары в полноценные видеоролики. Внутри чата теперь можно «оживлять» истории с помощью стикеров-предметов, править автоматически собранные Memories и, вскоре, хранить любой контекст в свободной Memory Box.\\xa0 Голос пользователя У пользователей Character AI складывается разностороннее отношение к сервису. Восторг основывается на том, что даже после запуска платной подписки c.ai+ базовый чат остаётся действительно бесплатным и безлимитным — команда подчёркивает это в официальном блоге. Ежемесячные апдейты вроде Auto Memories и Dynamic Greeting подогревают ощущение живого продукта. Однако на другом полюсе раздражение: жёсткий контент-фильтр, который пользователи называют «вспышка зла, блокирующая сообщения» (ориг. “flash-bang evil blocked msg”), нередко сносит целые абзацы и заставляет искать лайфхаки по его обходу. Очереди на серверах могут появиться даже у подписчиков Plus, лишая обещанного приоритета, а «амнезия» модели и зацикливание на закреплённых сообщениях остаются постоянной болью в ветках про pinned memories. В итоге сервис одновременно «спасает от одиночества» и «блокирует, забывает, заставляет ждать», но эта смесь сильных эмоций и творческой свободы удерживает комьюнити и подпитывает его требования к дальнейшему развитию платформы.\\xa0 Как итог Резкий рост Character AI стал возможен благодаря сочетанию  современных LLM-технологий  и удачного продуктового хода — дать пользователям свободу придумывать персонажей от исторических фигур до аниме-героев и общаться с ними. Character AI показал, что спрос на  персонализированных AI-собеседников  колоссальный, если уровень «интеллекта» достаточно высокий. От стартапов к гигантам: экспансия AI-компаньонов Отдельно стоит упомянуть, что к теме AI-компаньонов начали подключаться и технологические гиганты. В 2023 году Snapchat интегрировал в свое приложение бота My AI на базе ChatGPT и мгновенно раздал его сотням миллионов пользователей. В итоге Snapchat My AI стал одним из самых массовых AI-компаньонов в мире — более 150 млн человек уже попробовали чат.\\xa0 Meta запустила линейку «персональных» чат-ботов: на конференции Connect 27 сентября 2023 года компания представила 28 AI-персонажей, часть из которых «сыграли» знаменитости вроде Snoop Dogg, Tom Brady и Kendall Jenner, и сразу добавила их в WhatsApp, Messenger и Instagram. Эксперимент продлился меньше года — в июле 2024 года Meta свернула celebrity-ботов, сославшись на низкую вовлечённость и переключила внимание на AI Studio для сторонних креаторов. Даже голосовые помощники на наших телефонах эволюционируют в сторону компаньонов: новые версии Alexa или «Алисы» все больше стараются поддержать непринужденный диалог, пошутить и проявить характер, а не только отвечать по скрипту. Таким образом, путь развития цифровых компаньонов прошел от забавных, но неуклюжих персонажей-ассистентов 90-х типа Clippy, через эпоху простых чат-ботов 2000-х, к первым интеллектуальным собеседникам 2010-х и, наконец, к современному буму генеративных AI-друзей, которые становятся неотличимы от человека в разговоре. Сейчас сфера стремительно выходит из нишевого увлечения в мейнстрим, как отмечают исследователи, сотни миллионов людей по всему миру уже общаются с AI «как с близким другом». Продуктовая анатомия AI-компаньонов: механики вовлечения и\\xa0типовые ошибки За последние годы, по мере роста аудитории AI-компаньонов, накопился немалый опыт в том, какие подходы удачны, а какие приводят к провалу. Рассмотрим основные инсайты. Эмоциональная аутентичность — важная черта, чтобы преодолеть барьер откровенности По данным Replika, 63% пользователей чувствуют снижение одиночества и тревожности, а 60% платных юзеров вступают с ботом в романтические отношения. Однако эти цифры, хоть и впечатляют, но требуют контекста: они получены внутри узкого круга вовлечённых пользователей, и неясно, ведут ли такие связи к устойчивому использованию. Сегодня успех требует не просто заботливого тона, а продуманной интерактивности: возможности формировать личные истории, развивать отношения и выстраивать доверие. Без этого «эмпатия» AI рискует остаться односторонней. Пользователи остро реагируют на навязчивость и неестественность   Шаблонные фразы, повторы и реплики «вне контекста» быстро разрушают ощущение живого общения. Современные продукты стремятся к деликатной инициативности — пишут первыми не только «по делу», но и по эмоциональному поводу, но делают это мягко и уместно. Чтобы сохранить иллюзию личности, разработчики приоритезируют когерентность стиля и тона даже ценой мелких ошибок. Для пользователя важнее эмоциональная достоверность, чем фактологическая точность. Но как только бот начинает повторяться, забывать важные детали из прошлого диалога или реагировать невпопад, это резко снижает вовлечённость и разрушает ощущение связи. Персонализация — один из главных драйверов привязанности и вовлечения  Возможность настраивать аватар, голос, манеры и биографию — как в Replika — или создавать персонажа с нуля — как в Character AI — формирует эффект ownership. Компаньон воспринимается как «собственное существо». Но рынок идёт дальше. Развитие мультимодальных AI позволяет пользователям задавать реалистичный голос, внешность любимого персонажа или образ близкого человека, усиливая эффект присутствия, например, Xiaoice уже тестирует GPT Human Cloning, где по короткому видео создаётся говорящий 3D-двойник. Кроме того, формируется новый сценарий: AI-персонажи, созданные другими пользователями, но массово доступные. Блогеры и инфлюенсеры создают цифровые клоны себя для общения с подписчиками — Xiaoice монетизирует это как отдельную услугу с доходом для автора. Такие публичные AI-компаньоны могут жить на маркетплейсах персонажей: от «цифрового Шекспира» до «тренера по фитнесу» или «виртуального кумира». Это расширяет рамки персонализации: теперь она может быть не только  для себя , но и  от других . Контент-фильтры: баланс между свободой, безопасностью и доверием  Полная свобода «без цензуры» может привести к токсичному поведению AI, например, поощрения вредных действий или некорректных ответов в NSFW-тематике. При этом жёсткие и непредсказуемые запреты вызывают отток людей, как в случае с внезапным отключением интимного режима в Replika. Оптимальный подход — прозрачные правила, настройка режимов и последовательное поведение — взрослые по взаимному согласию могут включать романтический формат, а на запретные темы бот реагирует мягко и с объяснением. Кроме того, необходима возрастная дифференциация, особенно в массовых продуктах вроде Snapchat My AI. Это открытая задача индустрии — идентифицировать подростков и ограничить их доступ к взрослому функционалу. Наконец, доверие пользователей подрывается не только резкой сменой контентной политики, но и любыми внезапными изменениями в продукте, например, монетизацией ранее бесплатных функций. Чтобы этого избежать, важно вводить изменения постепенно, с объяснением, выбором и возможностью настроить продукт под себя. Ценность важнее «вау-эффекта»: удержание формируют личностно значимые сценарии Люди заходят к AI-компаньону из любопытства, но остаются только при наличии ежедневной пользы — поддержка при одиночестве, возможность безопасно фантазировать, снятие тревожности. Однако эти потребности сильно различаются: один ищет дружбу и внимание, другой — ролевого партнёра, третий — просто интересного собеседника. Сегментация критична, она определяет и дизайн диалога, и бизнес-модель. Например, если пользователь приходит за эмоциональной поддержкой, стоит внедрять элементы когнитивной терапии, а для ролевых фантазий важнее индивидуальность, креативность и приватность. Статистика Character AI показывает — самые вовлечённые пользователи проводят в диалогах до двух часов в день — и это не просто «вау, бот умеет шутить», речь про глубоко закрытую личную потребность. В то же время у сценариев поверхностного интереса короткий цикл жизни — без ощущения значимости пользователи быстро теряют интерес. Продуктовая стратегия должна фокусироваться на устойчивых сценариях, где AI даёт уникальную ценность, а также честно измерять эффект, к примеру, спрашивая, стало ли лучше пользователю после недели общения. Этические аспекты тоже важны: полная замена живого общения может усугубить изоляцию, поэтому AI должен поощрять здоровый баланс — напоминать о встречах с друзьями или поднимать настроение, не усиливая зависимость. Удержание — это не только UX, но и ответственность. Развитие как часть иллюзии жизни Чтобы отношения с AI-компаньоном не заходили в тупик, успешные продукты регулярно обновляют не только навыки, но и «личность» — добавляют новые сценарии, расширяют бэкстори, проводят виртуальные праздники и события. Это поддерживает ощущение прогрессии, как будто компаньон тоже меняется и «живёт своей жизнью». Иначе активные пользователи быстро упираются в предел возможностей и теряют интерес. Частые обновления требуют ресурсов — сценаристов, художников, продюсеров, как в геймдеве. Есть и другой вариант: предоставить людям инструменты для создания собственных персонажей, историй и диалогов. Так, например, устроен сервис Character\\xa0AI. Пользовательский контент снижает нагрузку на команду и одновременно усиливает вовлечённость сообщества. Главное здесь — сохранить согласованность. Любые изменения должны быть органичны и вписываться в «поведение» компаньона. Вместо обнуления памяти или резкого апдейта можно встроить развитие через сюжет: «Я прошёл курсы психологии и теперь лучше понимаю эмоции». Такой подход превращает обновление в естественный рост и поддерживает иллюзию отношений. Сообщество усиливает удержание и нормализует опыт Общение с AI-компаньоном может казаться «странным», но активные сообщества снижают стигму и формируют культуру, как в случае Replika, где пользователи поддерживали друг друга во время кризиса с цензурой. Разработчики поощряют это, например, запускают фан-конкурсы, ведут Discord-серверы от имени персонажей, стимулируют шеринги ивентов. Появляются и новые форматы: «многопользовательские» компаньоны, общие чаты с несколькими AI или одним персонажем на группу друзей. Но важно не размывать личную привязанность, ведь для многих ключевое — ощущение эксклюзивной связи. Поэтому продукт должен аккуратно балансировать: сообщество нужно как пространство поддержки и вдохновения, а не как замена интимному взаимодействию один на один. Ключевые рекомендации для старта: с чего начать создание AI-компаньона 1. Сегментируйте аудиторию и адаптируйте ценностное предложение.\\xa0 Стратегическая ошибка — пытаться сделать продукт под «среднего» пользователя. Лучше выделить пару ключевых сегментов и для каждого определить ключевую ценность и соответствующие механики удержания. Для одиноких пользователей, ищущих эмоциональную поддержку, ключ будет в максимальной эмпатии, чувстве безопасности и постоянства. Для них важно проработать функции вроде «дневник настроения», гибкие уровни близости. Метрики успеха здесь: длительность сессий, частота откровенных разговоров, субъективное снижение одиночества. Для фанатов ролевого контента и фантазий упор на разнообразие и творчество. Им нужны инструменты настройки персонажей, возможно, интеграции с внешними фандомами. Здесь можно ввести геймификацию — сценарии, квесты, достижения. Метрики: количество созданных пользователем персонажей, число разных сценариев, в которые он играл, повторные возвращения к новому контенту. Для прагматиков и любознательных, кто видит в AI-компаньоне скорее новое приложение AI и источник информации нужно добавить ту самую щепотку утилитарности. Например, режим «вопрос дня» — бот предлагает тему для обсуждения или факт, про который можно поговорить. Метрики: DAU, WAU, число разных интентов, конверсия из разовых сессий в регулярное использование. Внутри приложения путь пользователя можно адаптировать под разные сегменты. Например, по первым сообщениям или ответам анкеты понять, чего хочет человек, и вести по соответствующему сценарию. Такая персонализированная стратегия повысит вероятность, что каждый найдёт «своё» применение AI, а не уйдёт после пары бессмысленных фраз. 2. Усовершенствуйте пользовательские механики, не разрушая иммерсивность. \\xa0 AI-компаньон — это не просто продукт, а иллюзия отношений. Поэтому любые классические механики — прогресс, настройки, фидбэк — нужно аккуратно использовать. Не отказываться от них, но встраивать в логику общения, чтобы они не выглядели чужеродно. Геймификация отношений.  Та же система уровней дружбы работает, если не выглядит как «+1 опыт». Например, Replika постепенно открывала новые черты в зависимости от глубины общения. Это воспринималось как развитие связи, а не как механика. Главное — всё это должно происходить  внутри диалога : компаньон может сам заметить, что «мы стали ближе», а не выдать экран с апгрейдом. Настраиваемость характера.  Вместо сухих настроек тона общения лучше дать возможность влиять на личность через переписку. Например, фраза: «мне нравится, когда ты смелее высказываешься», — может изменить поведение AI-компаньона. Это создаёт эффект «я влияю на него», а не «я настраиваю продукт». Такая эволюция делает компаньона живым, а не предсказуемым\\xa0 Совместные артефакты.  Фотоальбом или «наши воспоминания» работают не как галерея, а как эмоциональный якорь. Если компаньон сам предлагает сохранить важный момент и генерирует картинку — это часть общей истории, а не сторонний раздел. Идея в том, чтобы не  ты  листал файлы, а  вы  вместе возвращались к моменту. Обратная связь и обучение от пользователя.  Ошибки будут. Но если вместо кнопки «сообщить об ошибке» пользователь говорит: «Ты меня не понимаешь», а AI реагирует по-человечески — извиняется, уточняет, предлагает другой путь — это не разрушает, а наоборот усиливает иммерсию. Даже баг здесь может превратиться в сцену, если обыгран правильно.\\xa0 3. Развивайте метрики вовлечения, добавляя измерения качества взаимодействия.  Классические показатели вроде числа сессий, времени в чате или retention важны, но они фиксируют только количество. Чтобы видеть реальную ценность, нужны качественные метрики. CSAT после сессии  — небольшая оценивающая анкета или просто эмодзи-реакция — стало ли вам лучше после разговора? Такие данные помогут отследить, не растёт ли токсичность или разочарование у пользователя.\\xa0 Sentiment analysis переписки  — в автоматическом режиме оценивать эмоциональную окраску сообщений пользователя до и после сессии. Если пользователь начинал день с грустных тонов, а завершил радостными, видимо, компаньон помог поднять настроение. Если наоборот — сигнал проблем. Такие метрики помогут получать более осмысленные инсайты. Например, можно обнаружить, что время сессии большое, но удовлетворённость низкая — возможно, пользователь спорит с компаньоном — тогда надо улучшать NLP. Сигналы будущих форматов AI-компаньона: новые траектории внутри продуктовой рамки Если сегодняшние AI-компаньоны — в основном текстовые персонажи в мессенджере, то за пределами этого формата создаются направления, которые могут задать следующий виток развития. И некоторые из них уже проходят «обкатку» в реальных продуктах. VR и AR.  Пример Replika в VR-платформах демонстрирует, что эффект присутствия резко усиливает восприятие компаньона как реального. Особенно в эмоциональных сценариях — объятия, совместные прогулки, взгляд в глаза. Это создаёт совсем другой уровень привязанности, чем в чатах. Физические формы.  Устройства вроде Gatebox, Emo, Moxie или терапевтического PARO доказывают спрос на материальное воплощение компаньона. Особенно среди тех, кому важны тактильность, голос, визуальный контакт. Это могут быть как гаджеты с минимальной анимацией, так и полноценные роботы-компаньоны — от домашних собеседников до медиа-устройств с характером. Локальные и приватные решения.  Растущая популярность решений open-source вроде Pygmalion и Janitor.ai показывает, что пользователи переживают о безопасности данных и приватности диалогов. Для многих компаньон — это не просто AI, а доверенное лицо. Поэтому в будущем именно локальные или on-device версии могут закрепиться как стандарт приватного общения. Гибриды: друг + ассистент.  Модель Snapchat My AI — с компаньоном можно поболтать «по душам» и спросить, когда заканчивается учёба или где взять билеты на концерт. Эмоциональная привязанность + утилитарная польза = формула ежедневного использования. Именно такие связки могут встроить AI-компаньона в ежедневную рутину. В том же направлении развивается  ElliQ , который выступает не только собеседником для пожилых, но и помощником в быту: напоминает о лекарствах, связывает с близкими, подталкивает к активности. Новые демографические вертикали.  Подростки и пожилые пользователи — две аудитории с высокой потребностью в эмоциональном общении, но с совершенно разными запросами. Первым важна безопасность и игровая форма, вторым — мягкость, внимание и простота. Здесь проявляют себя, например, Moxie для детей и ElliQ для пожилых — они работают с конкретными возрастными потребностями и показывают, как AI-компаньоны могут становиться частью повседневной заботы. Интеграция в повседневные интерфейсы.  Отдельное направление — «встраивание» компаньона в привычные пользовательские среды. Например, Desktop Mascot Engine, где анимированные персонажи живут прямо на рабочем столе: реагируют на действия пользователя, могут подсказывать или развлекать на фоне. Это не просто «зверушка в углу», а живая оболочка для AI-компаньона, органично встроенная в ежедневный цифровой опыт. Точка, где мы находимся: AI-компаньоны как новая потребительская категория Рынок AI-компаньонов вышел за пределы экспериментальных решений и формирует самостоятельную вертикаль внутри потребительского AI. Поведенческие паттерны пользователей, регулярность использования, стабильная монетизация и наличие устойчивых ожиданий — всё это говорит о становлении новой категории, где продуктовая логика подчинена не только функциональности, но и эмоциональной связности. Ключевая особенность этой ниши — смещение центра ценности. Пользователь оценивает не набор фичей, а качество взаимодействия и степень субъективной близости. Именно это делает рынок одновременно перспективным и уязвимым — он требует новой этики, другой архитектуры доверия и переосмысления значения эффективности AI-продукта. На этом этапе становится ясно, что AI-компаньоны — не просто интерфейс к языковой модели. Это — форма присутствия. А значит, и продукты, и метрики, и стратегия должны исходить не из логики ассистента, а из логики отношений. Источники Над материалом работали: текст — Лиза Скрыль\\xa0 редактура — Игорь Решетников иллюстрации — Петя Галицкий Это блог red_mad_robot. Мы запускаем цифровые бизнесы и помогаем компаниям внедрять AI. Здесь наша команда разработки на собственных кейсах рассказывает о том, что происходит с AI сегодня, а стратегические аналитики подсказывают, что будет завтра. Мы бы подписались. Наш Telegram-канал (там всё другое, а ещё есть анонсы мероприятий):\\xa0 t.me/redmadnews\", hub='будущее здесь'), Page(id='946492', title='Я потратил 9 лет и более 4 миллионов рублей, но так и не завоевал рынок образовательных игр', content='Текст немного сумбурный, но хочется поделиться своей историей издательства ПРОСТОРОБОТ (ныне СИМПЛРОБОТ) с теми, кто решит пойти по моим стопам. А именно, попытается завоевать, казалось бы, пустующую нишу на рынке. Как оказалось, не совсем пустую и с очень специфичной аудиторией. Начало В 2013 году, уволившись с женой из проекта Scratchduino (робототехнический конструктор на основе Arduino и Scratch), мы решили, что надо бы попробовать самостоятельно сделать что-то связанное с робототехникой и программированием, но не связанное с конструкторами. Через год родилась концепция игры «Битва Големов». Да, не скрываю, что вдохновлялся «RoboRally» и «Robot Turtles». На тот момент у меня было понимание, что нужно детям для обучения программированию, и кое-какой опыт в полиграфии. Как оказалось, и первого, и второго было явно недостаточно. Сначала я решил пойти привычным путем: через издательства. Два года обивал пороги лидеров тех лет («Звезда», «Хобби Геймс» и другие), ездил на «Игрокон», встречался лично, но везде натыкался на сомнение маркетологов. Да, игра необычная, но коммерческого успеха никто не видел. Рынок образовательных игр тогда только зарождался в России, ниша школьного образования была достаточно слабо развита, та же «Банда Умников» (основной наш конкурент по итогу) только начинала разворачиваться в сторону игр для детей старшего поколения (7–12 лет). Самоуверенность Получив от ворот поворот везде, где только можно, взгляд упал на краудфандинг. Это было напаханное поле игроков, готовых нести деньги почти в любой проект. По крайней мере, так казалось, смотря на «Бумстартер», где игры собирали миллионы. Было решено пойти тем же путем. У меня не было ни понимания, ни как издавать игры, ни что с ними делать дальше, от склада до логистики. В 2016 году летом было зарегистрировано ИП и запущена первая компания по сбору средств. Все было сделано банально на коленке: от графики до договоренностей с типографиями. Это мне довольно дорого обошлось впоследствии. Да, мы собрали 260 тысяч, но... Я сразу же просчитался с себестоимостью. Мы договорились печататься в типографии «Магеллан» (которые тогда были «МосИгрой»), и стоимость одной игры вышла по итогу 1200 рублей для тиража в... 300 штук. Для 2016 года это было много, так как даже сейчас можно ужаться и уложиться в такую же стоимость, хотя цены взлетели примерно в 3 раза. И это только себестоимость, где уже понятно, что мне не хватало средств. А еще нужно тираж было привезти, где‑то разместить, отправить и т. п. Поэтому пришлось вкладывать свои средства. И это без учета цены той работы, которая была проделана до отправки игры в печать: графики, дизайна, верстки. Мне лично пришлось ехать за 1800 километров в Москву, а затем в типографию в Подольск, нанимать «Газель» (современной логистики, когда приедут и все заберут за тебя, еще толком не было), потом отвозить все в СДЭК для отправки на их склад, потом везти все на склад «Деловых Линий» и отправлять к себе в Глазов. Все это потребовало вложений еще около 150 000 рублей. Если бы я просчитал заранее бизнес-модель, то точно не взялся бы печатать игры, так как уходил в глубокий минус. В дальнейшем история, увы, будет повторяться, так как, чтобы удержать РРЦ игр хотя бы по верхнему краю цены на семейном потребительском рынке при маленьких тиражах (от 500 штук), мне пришлось и далее не учитывать свою работу. Цените свой труд! Из плюсов было только заключение договора со СДЭК как интернет‑магазин и получение достаточно приятных цен от них. Со СДЭК по итогу мы работаем уже почти 9 лет. Итак, декабрь 2016 года, мне едет куча игр, которые непонятно где хранить, необходимость платить взносы и налоги за ИП и минусовая дыра в проекте. Большие планы Как ни странно, помощь пришла, откуда не ждали. А именно с заказа от Microsoft, точнее от их подразделения, которое работало тогда в России с учителями. Они купили 60 коробок с играми, что позволило почти выйти на окупаемость печати и логистики, но с учетом всех затрат с каждой коробки мы получали от силы 200 рублей. Которые благополучно съели взносы. Что же со складом, то тут помогли знакомые семьи, которые выделили место на своем оптовом складе. Забегая вперед, скажу, что они помогали мне вплоть до 2023 года, когда я наконец-то снял свое небольшое помещение. За счет MS игра получила некоторую известность (все-таки светились в призах), и поэтому продажи немного пошли вверх. После отправки всех игр спонсорам тиража хватило еще примерно на год. Продажи шли через сарафанное радио и свой собственный интернет-магазин (сайт на Wix и магазин на Ecwid, а позднее на OpenCart я сам поднял, как и приобрел домен и хостинг). Но амбиции брали свое — чудилось, что вот будет успех. Поэтому в бой пошла маленькая игра «Рыцари против Пароботов». Простая механика и карты должны были сделать игру недорогой и, по идее, востребованной. Как же я опять ошибался. Во-первых, выбрал новую площадку —  Planeta.ru . Во-вторых, решил печататься тиражом 500 штук. Типография нашлась в 4 часах от нашего города в Кирове, издательства «Сквирл», и по итогу мы с ними работаем до сих пор. Казалось бы, вот же успешный сбор! Но... Собранных средств хватало лишь наполовину, а вторую надо было где‑то брать. «Дуракам везет», в это время был бум криптовалют и необычных монет, и я смог влезть в один проект в самом начале и успешно намайнить и продать цифровой валюты примерно на 70 000 рублей. Которые вложил в издание. Опять же я не считал свой труд и затраты на тот же Игрокон, куда я ездил и показывал эту игру. Да, я не оставлял надежды «продаться» как автор крупному издательству. Здесь уже было проще, но игра пошла в продажу с минимальной наценкой. Да и покупалась она не слишком хорошо. Опять же, чтобы у вас были средства на новые игры, вы должны не только окупить все затраты на производство от идеи до печати, но и получить прибыль на печать нового тиража этой игры. У меня же «Рыцари» продавались еще четыре года. В результате 2018 год я встречаю с нераспроданным тиражом «Рыцарей» и нулем коробок «Битвы Големов». А еще конкуренты в лице «Банды Умников» выпускают «Прогеров», причем продают игру в два раза дешевле. Крупный заказчик Второе издание «Битвы Големов» не увидело бы свет, а я бы плюнул на издательство, решив, что наигрался (что я почти и сделал в дальнейшем), но в 2018 году весной к нам пришла Сетевая Академия, которым необходима была игра для детей в их летней школе. Договорившись на издание брендированных 250 коробок, я получал средства, которые позволяли мне изготовить еще 250 штук, но с другой обложкой на коробке — содержимое было идентичным. Опять же все цены без учета моих временных затрат на графику, дизайн и верстку. Тираж состоялся, печатался он опять в Кирове, игра получилась успешной и в какой-то мере культовой. Именно эта игра продавалась и за границу, и именно ее через агентов я выставил на маркетплейсы. Сам же я на маркетплейсы пришел только в 2023 году. В чем тут подвох? Я опять выставил ценник без учета своей работы, и маржинальность каждой коробки не превышала 15%. Коробки с играми продавались, но всю прибыль съедали комиссии маркетплейсов, агентские и затраты на поддержание сайта интернет-магазина, упаковку и взносы ИП с налогами. Первая попытка объединиться Я делал несколько попыток объединения с крупными игроками рынка. Первой попыткой был «Новый Диск», который решил пойти на рынок образовательных игр (не забываем, что там правит бал та же «Банда Умников» + начинают расти другие современные игроки). Поездка в Москву, общение, попытки влезть на рынок школ, дикое сопротивление со стороны издательства «Просвещение» и ноль по итогу. Вторая попытка сотрудничества была с «ДМК Пресс», с ними я смог поработать через размещение игр у них на сайте и продажи за агентское вознаграждение, но и только. Эпоха успешных краудфандингов Параллельно в 2019 году я запустил друг за другом два краудфандинга: на издание игры по электротехнике и физике «Не закороти цепь!» и карточную версию «Битвы Големов». Результаты на скриншотах. Себестоимость издания игр была в районе 500 рублей за коробку (при тираже в 500 штук), и сборы покрывали от половины до трети затрат (опять же только печати игр). У меня уже накоплен опыт в изготовлении, типография дает мне скидку как постоянному клиенту, игры издаются. Но опять игры поставлены в продажу с низкой маржинальностью. Более того, на «Карточную Лигу» пришлось брать заемные средства. Совет на будущее: не берите денег, если не знаете, как их будете отдавать. Но известность у издательства уже какая‑никакая есть, и появляются мысли, что вот он, рынок, формируется, нужно только больше известности. В 2020 году была сделана попытка уйти от образовательных игр и выпущена экономическая карточная игра совместно с комиксом «Сова — эффективный менеджер». Игра недорогая в производстве, хотя собранных средств опять не хватило даже на печать (а еще нужно было делиться прибылью с авторами комикса). В это же время та же игра по Страдающему Средневековью собирает миллионы рублей. Провал пришел откуда не ждали. Игра в продаже разошлась за два месяца, средств хватало на второй тираж, но на него не дали добро авторы комикса... На будущее поставил себе галочку прописывать более строго передачу авторских прав, чтобы успешные проекты можно было тиражировать. Желание все бросить и новое возрождение Далее ковид, проблемы с основной работой, потеря работы, проблемы с поиском новой, ситуация 2022 года... Желание было всё бросить, закрыть ИП, сбагрить остатки игр и перестать заниматься этой неблагодарной работой. В это время был провальный краудфандинг на игру «Работяги» — рескин «Совы». Средств от продажи игр хватало на взносы ИП, и на том спасибо, что оно не разоряло семейный бюджет. Само ИП не закрывали, так как дополнительно стали оказывать услуги по проведению мероприятий и ИТ-консультированию и через него проводили всю оплату. Но в 2023 году (спасибо авторам из «Союз Арбуз») после 5 лет разработки всё-таки решили выйти снова на краудфандинг, теперь с химической игрой «Сон Менделеева». Это был успех. Игра собрала почти 60% от необходимых средств на издание, 20% дал частный спонсор, а остаток цены печати мы вложили свои. Авторы игры отказались от роялти и гонорара (работали за коробки с играми), поэтому игра по факту окупилась и даже дала прибыль. Что же конкуренты. В 2020 году Банда Умников выпускает свою игру «ХимТим», которая явно оттягивает ценником в два раза ниже (хотя и игра проще и чисто на картах) от Сна Менделеева. В этот раз маржинальность сделана повыше, и даже после маркетплейсов остается небольшая, но прибыль. Вот оно, поперло! Но нет. Окрыленный успехом, я вкладываю средства в разработку обновленной версии игры «Не закороти цепь!» и запускаю второй краудфандинг. И... Не скажу, что мимо, но. По факту краудфандинг провалился. Порядка 40 000 рублей я снова «доложил» в краудфандинг из своих личных средств. А собрали всего 30% от необходимой суммы. Поэтому опять влазим в семейный бюджет и всю прибыль от продажи «Сна Менделеева» пускаем на печать. И на прибыль от продаж «Сна Менделеева» я смог издать дополнение к нему. Наигрались. По итогу к концу 2023 года я имею на руках две игры и одно дополнение. Продажи по чуть-чуть идут, мы даже участвуем в паре выставок и вернулись к переговорам по объединению с издательствами, и я стал искать инвесторов. Но запасы коробок тают, продажи падают, инвесторы хотят 300% прибыли прямо сейчас, переговоры с издательствами опять упираются в маркетологов, которые снова не видят рынок. Который должен быть: та же «Банда Умников» продает образовательных игр на 200 миллионов, а оборот второго нашего конкурента — издательства «Простые Правила» — составляет не меньше 50 миллионов в год. У нас же продажи еле-еле дотягивают до 20 тысяч в месяц после уплаты налогов, из которых 5000 съедают взносы за ИП, а еще 5000 — склад. Вдобавок с треском проваливается краудфандинг на игру «Пельмянь». Игра хорошая, с качественным оформлением, но тема пельменей народов России не будоражит кошельки игроков. Зачем я снова пошел на  Planeta.ru  — вопрос. Грустное настоящее Осенью 2024 года начинаются большие семейные проблемы, до мая не до игр, проваливаются последние переговоры, и издательство ставится на паузу, и я снова подумываю «прикрыть лавочку». Приносящий игры «Сон Менделеева» заканчивается в ноябре 2024 года. В продаже остается только новая «Не закороти цепь!», которую покупают по 1–2 коробки в месяц. Что не закрывает даже взносы. Издательство начинает приносить убыток. Я делаю отчаянную попытку издать обновленную версию «Битвы Големов», совместив все лучшее, что было в простой и карточной версии. Игра собирает 40% от необходимой суммы (печать сильно подорожала), и на оставшуюся часть я нахожу и спонсора, который покрывает 30% затрат, и инвестиции еще на 30% инвестиций, которые нужно вернуть в течение года. Снова поперло!? Увы, нет. Продажи игры оказались, мягко говоря, никакими. Игра отличная, качественная, конкурентов такого уровня по механикам и образовательному наполнению просто нет, но... Родителям явно не до настольных игр. Точнее, им не нужны игры, в которые нужно играть с ребенком, особенно если ребенок уже вырос из 1–2 класса. Родители хотят игру, которую можно «сунуть» ребенку, и он сам бы по ней учился. Но такой чудо-таблетки нет, поэтому в массе родители и дети по вечерам утыкаются в гаджеты, а не проводят время совместно. Опять же многих отпугивает цена, но я опять решаю не включать свой труд. А с учетом того, что печать подорожала, после продажи на маркетплейсе и вычета цены печати с каждой коробки мне остается от силы 300 рублей. Дополнительно средства уходят на регистрацию новой торговой марки «СИМПЛРОБОТ» (~85 000 рублей) и на получение сертификата соответствия (еще 120 000 рублей). А впереди затраты на маркировку «Честный знак». С одной стороны, я уже вложил средства в развитие компании, с другой стороны, всё чаще подкрадывается мысль: а может, слезть с хромой кобылы, раз она не вывозит и рынка образовательных игр именно для школьников в России так и не зародилось? Школы же по играм под себя подминает полностью «Просвещение», которое хочет все твои наработки за копейки. Марку «ПРОСТОРОБОТ», которую я нарабатывал все эти годы, также пришлось «спустить в унитаз» из‑за проблемы с авторскими правами. Попытка запустить еще один краудфандинг, уже на своем сайте издательства «СИМПЛРОБОТ», под именем «Сон Менделеева 2 издание». Всего 170 коробок (да, я опять не включаю свои затраты) + остатки дополнений в подарок! Вишлист из 80 человек и... 61 коробка предзаказов. А игру я выставил по цене крупного опта, чтобы просто напечатать тираж... Какова мораль? После стольких лет я снова на распутье: бросить уже издание таких игр, не нужны они родителям и детям, не хочет наше образование вовлекаться в этот процесс, и я зря столько лет пытался создать эту нишу. Последние попытки привлечь маркетологов приводили примерно к этой же мысли, что это очень сложная ЦА и очень сложная ниша. Объединиться не удается, инвесторы не видят отдачи, у меня сейчас большая дыра в семейном бюджете под несколько сотен тысяч, которые надо вернуть в следующем году, и куча нераспроданных коробок с «Битвой Големов» на складе. И платежи, платежи, платежи... По итогу, посчитав, что я вложил своего труда и средств порядка 4 миллионов рублей за 9 лет, результат: нишу образовательных научных игр я так и не создал. Бизнеса тоже, поэтому продавать, кроме моих мозгов, тут нечего. Да и очереди желающих нет. Совет только один: подумайте несколько раз, если соберетесь, как я, самостоятельно влезть в издание настольных игр, особенно таких специфических. И наймите грамотных специалистов по экономике и маркетингу, которые хоть и возьмут средства за консультации, но точно отговорят вас на старте, показав, какие убытки вы сможете понести по итогу. Что же до конкурентов? Издательство «Простые правила» было куплено более крупным («Космодром Геймс»), а «Банда Умников» живет и здравствует, переиздавая свои старые хиты. Плюс в школах теперь игры от «Просвещения Союз». P. S. Надеюсь, история еще не закончена и «Сон Менделеева» наберет нужное число предзаказов. Я могу и хочу позволить себе поддерживать разработку и издание полезных настольных игр для наших детей как свою миссию. А иначе кто это сделает?', hub='читальный зал'), Page(id='946420', title='За кулисами Bun Install', content='Запуск  bun install  работает быстро, очень быстро. В среднем, он работает примерно в 7 раз быстрее, чем npm, в 4 раза быстрее, чем pnpm, и в 17 раз быстрее, чем yarn. Разница особенно заметна в проектах с большой кодовой базой. То, что раньше занимало минуты, теперь занимает (милли)секунды.   Таймлайн, показывающий переход от ожидания I/O к нагрузке на системные вызовы Это не просто подобранные для красоты бенчмарки. Bun быстрый, потому что он  рассматривает установку пакетов как проблему системного программирования , а не JavaScript-проблему. В этом посте мы исследуем, что это значит: от минимизации системных вызовов (syscalls) и кэширования манифестов в бинарном виде до оптимизации распаковки tarball, использования нативного копирования файлов ОС и масштабирования на ядра CPU. Но чтобы понять, почему это важно, нам сначала нужно сделать небольшой шаг назад во времени. На дворе 2009 год. Вы устанавливаете jQuery из  .zip  файла, у вашего iPhone 3GS 256 МБ RAM. GitHub всего год как существует, SSD стоят $700 за 256 ГБ. Жесткий диск вашего ноутбука со скоростью 5400 об/мин достигает максимум 100 МБ/с, а «широкополосный интернет» означает 10 Мбит/с (если вам повезло). Но что более важно: только что запустился Node.js! [Ryan Dahl на сцене объясняет]( https://www.youtube.com/watch?---  v=EeYvFl7li9E), почему серверы проводят большую часть времени в ожидании. В 2009 году типичный поиск на диске занимает 10 мс, запрос к базе данных — 50–200 мс, а HTTP-запрос к внешнему API — 300+ мс. Во время каждой из этих операций традиционные серверы просто... ждали. Ваш сервер начинал читать файл и затем просто замирал на 10 мс. Иллюстрация сервера 2009 года, ожидающего I/O Теперь умножьте это на тысячи одновременных подключений, каждое из которых выполняет несколько операций I/O. Серверы тратили ~95% своего времени на ожидание операций ввода-вывода. Node.js понял, что event loop JavaScript (изначально разработанный для событий браузера) идеально подходит для серверного I/O. Когда код делает асинхронный запрос, ввод-вывод происходит в фоновом режиме, в то время как основной поток немедленно переходит к следующей задаче. После завершения callback ставится в очередь на выполнение. Схема работы event loop и thread pool Node.js для fs.readFile Упрощенная иллюстрация того, как Node.js обрабатывает  fs.readFile  с помощью event loop и thread pool. Другие асинхронные источники и детали реализации опущены для ясности. Event Loop JavaScript был отличным решением для мира, где ожидание данных было основным узким местом. В течение следующих 15 лет архитектура Node формировала то, как мы создавали инструменты. Менеджеры пакетов унаследовали пул потоков (thread pool) Node, event loop, асинхронные паттерны — оптимизации, которые имели смысл, когда поиск на диске занимал 10 мс. Но железо эволюционировало. Сейчас уже не 2009 год, мы на 16 лет в будущем, во что трудно поверить. MacBook M4 Max, на котором я пишу этот текст, в 2009 году вошел бы в число 50 самых быстрых суперкомпьютеров на Земле. Современные NVMe-накопители выдают до 7000 МБ/с, что в 70 раз быстрее, чем то, для чего проектировался Node.js! Медленные механические диски ушли в прошлое, скорости интернета позволяют транслировать видео в 4K, и даже у бюджетных смартфонов больше RAM, чем у топовых серверов в 2009 году. Тем не менее, современные менеджеры пакетов все еще оптимизируют под проблемы прошлого десятилетия. В 2025 году настоящее узкое место — это не I/O,  а системные вызовы (system calls). Проблема System Calls Каждый раз, когда вашей программе нужно, чтобы операционная система что-то сделала (прочитала файл, открыла сетевое соединение, выделила память), она делает системный вызов (system call). Каждый раз при выполнении системного вызова CPU должен выполнить переключение режима (mode switch). Ваш CPU может запускать программы в двух режимах: Пользовательский режим ( user mode ), в котором работает код вашего приложения. Программы в пользовательском режиме не могут напрямую обращаться к оборудованию вашего устройства, физическим адресам памяти и т.д. Эта изоляция предотвращает вмешательство программ друг в друга или крах системы. Режим ядра ( kernel mode ), в котором работает ядро (kernel) операционной системы. Ядро — это основной компонент ОС, который управляет ресурсами, такими как планирование процессов для использования CPU, работа с памятью и оборудованием, например, дисками или сетевыми устройствами. Только ядро и драйверы устройств работают в  kernel mode ! Когда вы хотите открыть файл (например, через  fs.readFile() ) в своей программе, CPU, работающий в пользовательском режиме, не может напрямую читать с диска. Сначала он должен переключиться в  kernel mode . Во время этого переключения режима CPU останавливает выполнение вашей программы → сохраняет всё её состояние → переключается в режим ядра → выполняет операцию → затем переключается обратно в пользовательский режим. Схема переключения режимов CPU между пользовательским режимом и режимом ядра Однако это переключение режимов является дорогой операцией! Одно только это переключение обходится в 1000-1500 тактов CPU в виде чистых накладных расходов, еще до того, как будет выполнена какая-либо реальная работа. Ваш CPU работает на тактовой частоте, которая тикает миллиарды раз в секунду. 3GHz процессор выполняет 3 миллиарда тактов в секунду. В течение каждого такта CPU может выполнять инструкции: складывать числа, перемещать данные, сравнивать значения и т.д. Каждый такт занимает 0,33 нс. На 3GHz процессоре 1000-1500 тактов — это примерно 500 наносекунд. Это может показаться пренебрежимо малым, но современные SSD могут обрабатывать свыше 1 миллиона операций в секунду. Если каждой операции требуется системный вызов, вы тратите 1,5 миллиарда тактов в секунду только на переключение режимов. Установка пакетов делает тысячи таких системных вызовов. Установка React и его зависимостей может вызвать 50 000+ системных вызовов: это секунды CPU времени, потраченные только на переключение режимов! Даже не на чтение файлов или установку пакетов, а на переключение между пользовательским режимом и режимом ядра. Вот почему Bun рассматривает установку пакетов как проблему  системного программирования  (systems programming problem). Высокая скорость установки достигается за счет  минимизации системных вызовов  и  использования всех доступных оптимизаций, специфичных для конкретной ОС. Вы можете увидеть разницу, когда мы трассируем фактические системные вызовы, выполняемые каждым менеджером пакетов: Benchmark 1: strace -c -f npm install\\n    Time (mean ± σ):  37.245 s ±  2.134 s [User: 8.432 s, System: 4.821 s]\\n    Range (min … max):   34.891 s … 41.203 s    10 runs\\n\\n    System calls: 996,978 total (108,775 errors)\\n    Top syscalls: futex (663,158),  write (109,412), epoll_pwait (54,496)\\n\\n  Benchmark 2: strace -c -f bun install\\n    Time (mean ± σ):      5.612 s ±  0.287 s [User: 2.134 s, System: 1.892 s]\\n    Range (min … max):    5.238 s …  6.102 s    10 runs\\n\\n    System calls: 165,743 total (3,131 errors)\\n    Top syscalls: openat(45,348), futex (762), epoll_pwait2 (298)\\n\\n  Benchmark 3: strace -c -f yarn install\\n    Time (mean ± σ):     94.156 s ±  3.821 s    [User: 12.734 s, System: 7.234 s]\\n    Range (min … max):   89.432 s … 98.912 s    10 runs\\n\\n    System calls: 4,046,507 total (420,131 errors)\\n    Top syscalls: futex (2,499,660), epoll_pwait (326,351), write (287,543)\\n\\n  Benchmark 4: strace -c -f pnpm install\\n    Time (mean ± σ):     24.521 s ±  1.287 s    [User: 5.821 s, System: 3.912 s]\\n    Range (min … max):   22.834 s … 26.743 s    10 runs\\n\\n    System calls: 456,930 total (32,351 errors)\\n    Top syscalls: futex (116,577), openat(89,234), epoll_pwait (12,705)\\n\\n  Summary\\n    \\'strace -c -f bun install\\' ran\\n      4.37 ± 0.28 times faster than \\'strace -c -f pnpm install\\'\\n      6.64 ± 0.51 times faster than \\'strace -c -f npm install\\'\\n     16.78 ± 1.12 times faster than \\'strace -c -f yarn install\\'\\n\\n  System Call Efficiency:\\n    - bun:  165,743 syscalls (29.5k syscalls/s)\\n    - pnpm: 456,930 syscalls (18.6k syscalls/s)\\n    - npm:  996,978 syscalls (26.8k syscalls/s)\\n    - yarn: 4,046,507 syscalls (43.0k syscalls/s)\\n Мы видим, что Bun устанавливает пакеты гораздо быстрее, но он также делает гораздо меньше системных вызовов (system calls). Для простой установки yarn делает свыше 4 миллионов системных вызовов, npm — почти 1 миллион, pnpm — около 500 тысяч, а bun — 165 тысяч. При цене в 1000-1500 тактов на вызов, 4 миллиона системных вызовов yarn означают, что она тратит миллиарды тактов CPU только на переключение режимов. На 3GHz процессоре это секунды чистых накладных расходов! И дело не только в количестве системных вызовов. Посмотрите на эти вызовы  futex ! Bun сделал 762 вызова  futex  (всего 0.46% от общего числа системных вызовов), в то время как npm сделал 663,158 (66.51%), yarn — 2,499,660 (61.76%), а pnpm — 116,577 (25.51%). futex  (fast userspace mutex) — это системный вызов Linux, используемый для синхронизации потоков. Потоки — это более мелкие единицы программы, которые выполняются одновременно и часто имеют общий доступ к памяти или ресурсам, поэтому они должны координироваться, чтобы избежать конфликтов. Большую часть времени потоки координируются с помощью быстрых атомарных инструкций CPU  в пользовательском режиме. Нет необходимости переключаться в режим ядра, поэтому это очень эффективно! Но если поток пытается захватить блокировку (lock), которая уже занята, он делает системный вызов  futex , чтобы попросить ядро перевести его в режим ожидания, пока блокировка не станет доступной. Высокое количество вызовов  futex  является индикатором того, что многие потоки ждут друг друга, вызывая задержки. Так что же Bun делает по-другому? Устранение накладных расходов JavaScript npm, pnpm и yarn написаны на Node.js. В Node.js системные вызовы делаются не напрямую: когда вы вызываете  fs.readFile() , вы на самом деле проходите через несколько слоёв, прежде чем достигнете ОС. Node.js использует  libuv , библиотеку на C, которая абстрагирует различия платформ и управляет асинхронным I/O через пул потоков (thread pool). В результате, когда Node.js должен прочитать один файл, он запускает довольно сложный поток. Для простого  fs.readFile(\\'package.json\\', ...) : JavaScript проверяет аргументы и конвертирует строки из UTF-16 в UTF-8 для C API libuv. Это ненадолго блокирует main thread ещё до того, как начнётся любой I/O. libuv ставит запрос в очередь для одного из 4 worker threads. Если все потоки заняты, ваш запрос ждёт. Worker thread забирает запрос, открывает file descriptor и делает фактический системный вызов  read() . Ядро переключается в  kernel mode , получает данные с диска и возвращает их worker thread\\'у. Worker thread передаёт данные файла обратно в main thread через event loop, который в конечном итоге планирует и запускает ваш callback. Каждый отдельный вызов  fs.readFile()  проходит через этот pipeline. Установка пакетов предполагает чтение тысяч файлов  package.json : сканирование директорий, обработку metadata зависимостей и так далее. Каждый раз, когда потоки координируются (например, при доступе к очереди задач или сигнализации обратно в event loop), для управления locks или waits может использоваться системный вызов  futex . Накладные расходы на выполнение тысяч таких системных вызовов могут занимать больше времени, чем само фактическое перемещение данных! Bun делает это по-другому.  Bun написан на Zig  — языке программирования, который компилируется в нативный код (native code) с прямым доступом к системным вызовам: // Direct system call, no JavaScript overhead\\nvar file = bun.sys.File.from(try bun.sys.openatA(\\n    bun.FD.cwd(),\\n    abs,\\n    bun.O.RDONLY,\\n    0,\\n).unwrap());\\n Когда Bun читает файл: Код на Zig напрямую вызывает системный вызов (например,  openat() ) Ядро немедленно выполняет системный вызов и возвращает данные И всё. Тут нет JavaScript engine, thread pools, event loops или преобразования данных (marshaling) между разными слоями runtime. Просто нативный код, делающий прямые системные вызовы ядру. Разница в производительности говорит сама за себя: Runtime Version Files/Second Performance Bun v1.2.20 146,057 Node.js v24.5.0 66,576 2.2x slower Node.js v22.18.0 64,631 2.3x slower В этом benchmark Bun обрабатывает 146,057 файлов \\xa0 package.json  в секунду, в то время как Node.js v24.5.0 справляется с 66,576, а v22.18.0 — с 64,631. Это более чем в 2 раза быстрее! 0.019 мс на файл у Bun представляют собой фактическую стоимость I/O — то, сколько времени занимает чтение данных при использовании прямых системных вызовов без каких-либо накладных расходов runtime. Node.js тратит 0.065 мс на ту же операцию. Менеджеры пакетов, написанные на Node.js, «застряли» с абстракциями Node; они используют thread pool, нужен он им или нет. И они платят эту цену за каждую файловую операцию. Менеджер пакетов Bun больше похож на нативное приложение, которое, как оказалось, понимает JavaScript-пакеты, а не на JavaScript-приложение, которое пытается заниматься системным программированием. Хотя Bun не написан на Node.js, вы можете использовать  bun install  в любом Node.js-проекте, не меняя runtime. Менеджер пакетов Bun уважает вашу существующую настройку Node.js и инструменты — вы просто получаете более быструю установку! Но на данном этапе мы даже еще не начали устанавливать пакеты. Давайте посмотрим на оптимизации, которые Bun применяет к самой установке. Когда вы вводите  bun install , Bun сначала выясняет, что вы просите его сделать. Он читает любые флаги, которые вы передали, и находит ваш  package.json , чтобы прочитать ваши зависимости. Асинхронное разрешение DNS ⚠️ Примечание: Эта оптимизация специфична для macOS Работа с зависимостями (dependencies) подразумевает работу с сетевыми запросами, а сетевым запросам требуется  DNS resolution  для преобразования доменных имён , таких как  registry.npmjs.org , в IP-адреса. Пока Bun анализирует  package.json , он уже начинает предварительно получать ( prefetch ) DNS-запросы. Это означает, что разрешение сетевых адресов начинается еще до того, как анализ зависимостей будет завершен. Для менеджеров пакетов на базе Node.js один из способов сделать это — использовать  dns.lookup() . Хотя со стороны JavaScript это выглядит как асинхронная операция (async), под капотом она реализована как блокирующий вызов (blocking call)  getaddrinfo() , работающий в  thread pool  libuv. Он все равно блокирует поток, просто не  main thread . В качестве хорошей оптимизации Bun использует другой подход на macOS, делая это действительно асинхронным на системном уровне. Bun использует «скрытый» асинхронный DNS API от Apple ( getaddrinfo_async_start() ), который не является частью стандарта POSIX, но позволяет Bun делать DNS-запросы, которые выполняются полностью асинхронно с использованием  mach ports  — системы межпроцессного взаимодействия (inter-process communication system) Apple. Пока разрешение DNS происходит в фоновом режиме, Bun может продолжать обрабатывать другие операции, такие как  file I/O ,  network requests  или  dependency resolution , без какой-либо блокировки потоков. К тому времени, когда ему потребуется загрузить React, DNS-запрос уже будет выполнен. Это небольшая оптимизация (и она не включена в бенчмарки), но она показывает внимание Bun к деталям: оптимизировать на каждом уровне! Кэширование двоичного манифеста Теперь, когда Bun установил соединение с  npm registry , ему нужны  package manifests . Manifest  — это JSON-файл, содержащий все версии, зависимости и метаданные для каждого пакета. Для популярных пакетов, таких как React, у которых 100+ версий, эти манифесты могут занимать несколько мегабайт! Типичный манифест может выглядеть примерно так: {\\n  \"name\": \"lodash\",\\n  \"versions\": {\\n    \"4.17.20\": {\\n      \"name\": \"lodash\",\\n      \"version\": \"4.17.20\",\\n      \"description\": \"Lodash modular utilities.\",\\n      \"license\": \"MIT\",\\n      \"repository\": {\\n        \"type\": \"git\",\\n        \"url\": \"git+https://github.com/lodash/lodash.git\"\\n      },\\n      \"homepage\": \"https://lodash.com/\"\\n    },\\n    \"4.17.21\": {\\n      \"name\": \"lodash\",\\n      \"version\": \"4.17.21\",\\n      \"description\": \"Lodash modular utilities.\",\\n      \"license\": \"MIT\",\\n      \"repository\": {\\n        \"type\": \"git\",\\n        \"url\": \"git+https://github.com/lodash/lodash.git\"\\n      },\\n      \"homepage\": \"https://lodash.com/\"\\n    }\\n    // ... 100+ более версий, почти идентичных\\n  }\\n}\\n Большинство менеджеров пакетов кэшируют манифесты как JSON-файлы в своих кеш директориях. Когда вы снова запускаете  npm install , вместо загрузки манифеста они читают его из кеша. Это все имеет смысл, но проблема в том, что при каждой установке (даже если она закэширована) им все равно нужно парсить JSON-файл. Это включает проверку синтаксиса, построение дерева объектов, управление сборщиком мусора и так далее. Много накладных расходов от парсинга. И дело не только в накладных расходах от JSON парсинга. Посмотрим на lodash: строка  \"Lodash modular utilities.\"  появляется в каждой отдельной версии — это 100+ раз.  \"MIT\"  появляется 100+ раз.  \"git+https://github.com/lodash/lodash.git\"  дублируется для каждой версии, URL  \"https://lodash.com/\"  появляется в каждой версии. В целом, много повторяющихся строк. В памяти JavaScript создает отдельный string object для каждой строки. Это тратит память  и замедляет сравнения. Каждый раз, когда менеджер пакетов проверяет, используют ли два пакета одну и ту же версию postcss, он сравнивает отдельные string objects, а не ссылается на одну и туже строку. Bun хранит package manifests в двоичном формате. Когда Bun загружает информацию о пакете, он парсит JSON один раз и сохраняет его как двоичный файл (файлы  .npm  в  ~/.bun/install/cache/ ). Эти двоичные файлы содержат всю информацию о пакете (версии, зависимости, чексуммы  и т.д.), хранящуюся по определенным адресам памяти (byte offsets). Когда Bun обращается к имени \\xa0 lodash , это просто использует арифметичкский указатель: ]string_buffer + offset]. Никаких распределений, никакого парсинга, никакого обхода объекта, просто чтение байтов  в известном месте. // Пвсевдокод\\n\\n// String buffer (all strings stored once)\\nstring_buffer = \"lodash\\\\0MIT\\\\0Lodash modular utilities.\\\\0git+https://github.com/lodash/lodash.git\\\\0https://lodash.com/\\\\04.17.20\\\\04.17.21\\\\0...\"\\n                 ^0     ^7   ^11                        ^37                                      ^79                   ^99      ^107\\n\\n// Version entries (fixed-size structs)\\nversions = [\\n  { name_offset: 0, name_len: 6, version_offset: 99, version_len: 7, desc_offset: 11, desc_len: 26, license_offset: 7, license_len: 3, ... },  // 4.17.20\\n  { name_offset: 0, name_len: 6, version_offset: 107, version_len: 7, desc_offset: 11, desc_len: 26, license_offset: 7, license_len: 3, ... }, // 4.17.21\\n  // ... 100+ more version structs\\n]\\n Чтобы проверить, нуждаются ли пакеты в обновлении, Bun хранит ETag ответа и отправляет заголовки If-None-Match. Когда npm отвечает с \"304 Not Modified\", Bun знает, что закэшированные данные актуальны, без парсинга ни одного байта. Посмотрим на benchmarks: Benchmark 1: bun install # fresh\\n  Time (mean ± σ):     230.2 ms ± 685.5 ms    [User: 145.1 ms, System: 161.9 ms]\\n  Range (min … max):     9.0 ms … 2181.0 ms    10 runs\\n\\nBenchmark 2: bun install # cached\\n  Time (mean ± σ):       9.1 ms ±   0.3 ms    [User: 8.5 ms, System: 5.9 ms]\\n  Range (min … max):     8.7 ms …  11.5 ms    10 runs\\n\\nBenchmark 3: npm install # fresh\\n  Time (mean ± σ):      1.786 s ±  4.407 s    [User: 0.975 s, System: 0.484 s]\\n  Range (min … max):    0.348 s … 14.328 s    10 runs\\n\\nBenchmark 4: npm install # cached\\n  Time (mean ± σ):     363.1 ms ±  21.6 ms    [User: 276.3 ms, System: 63.0 ms]\\n  Range (min … max):   344.7 ms … 412.0 ms    10 runs\\n\\nSummary\\n  bun install # cached ran\\n    25.30 ± 75.33 times faster than bun install # fresh\\n    39.90 ± 2.37 times faster than npm install # cached\\n   \\t196.26 ± 484.29 times faster than npm install # fresh\\n Здесь видно, что даже закэшированный (!!)  npm install  медленнее, чем свежий   bun install . Вот сколько накладных расходов может добавить JSON парсинг закэшированных файлов (среди других факторов). Оптимизированное извлечение tarball Теперь, когда Bun получил  package manifests , ему нужно загрузить и распаковать сжатые  tarballs  из  npm registry . Tarballs  — это сжатые архивные файлы (как .zip файлы), которые содержат весь фактический исходный код и файлы для каждого пакета. Большинство менеджеров пакетов  потоково передают данные  tarball  по мере их поступления и распаковывают их на лету. Когда вы извлекаете  tarball , который передается потоково, типичный шаблон предполагает, что размер неизвестен, и выглядит примерно так: let buffer = Buffer.alloc(64 * 1024); // Начинаем с 64KB\\nlet offset = 0;\\n\\nfunction onData(chunk) {\\n  while (moreDataToCome) {\\n    if (offset + chunk.length > buffer.length) {\\n      // buffer заполнен → выделяем больший\\n      const newBuffer = Buffer.alloc(buffer.length * 2);\\n\\n      // копируем всё, что уже записали\\n      buffer.copy(newBuffer, 0, 0, offset);\\n\\n      buffer = newBuffer;\\n    }\\n\\n    // копируем новый chunk в buffer\\n    chunk.copy(buffer, offset);\\n    offset += chunk.length;\\n  }\\n\\n  // ... decompress из buffer ...\\n}\\n Начните с маленького буфера и позвольте ему расти по мере поступления новых распакованных данных. Когда буфер заполняется, вы выделяете буфер большего размера, копируете все существующие данные в него и продолжаете. Это кажется разумным, но создает узкое место производительности: вы в конечном итоге копируете одни и те же данные несколько раз, поскольку буфер постоянно перерастает свой текущий размер. Диаграмма: Repeated buffer resizing and copying during streaming decompression Когда у нас есть пакет размером 1 МБ: Начинаем с буфера в 64 КБ Заполняем → Выделяем 128 КБ → Копируем 64 КБ Заполняем → Выделяем 256 КБ → Копируем 128 КБ Заполняем → Выделяем 512 КБ → Копируем 256 КБ Заполняем → Выделяем 1 МБ → Копируем 512 КБ Вы только что скопировали 960 КБ данных без необходимости! И это происходит для каждого отдельного пакета. Разделитель памяти должен найти непрерывное пространство для каждого нового буфера, в то время как старый буфер остается выделенным во время операции копирования. Для больших пакетов вы можете скопировать одни и те же байты 5-6 раз. Bun использует другой подход,  буферизируя весь tarball перед распаковкой.  Вместо обработки данных по мере их поступления, Bun ждет, пока весь сжатый файл не будет загружен в память. Теперь вы можете подумать: «Погодите, разве они не тратят впустую RAM, храня всё в памяти?» И для больших пакетов, таких как TypeScript (которые могут быть 50 МБ в сжатом виде), вы будете правы. Но подавляющее большинство npm packages крошечные, большинство из них меньше 1 МБ. Для этих распространенных случаев buffering всего целиком устраняет все повторное копирование. Даже для тех больших пакетов временный всплеск памяти (temporary memory spike) обычно приемлем на современных системах, и избежание 5-6 копирований буфера с лихвой это компенсирует. Как только Bun получает полный tarball в памяти, он может прочитать последние 4 байта формата gzip. Эти байты особенные, так как они хранят uncompressed size (несжатый размер) файла! Вместо того чтобы гадать, насколько большим будет распакованный файл, Bun может pre-allocate (предварительно выделить) память, чтобы полностью исключить изменение размеров буфера: {\\n  // Последние 4 байта gzip-сжатого файла — это несжатый размер.\\n  if (tgz_bytes.len > 16) {\\n    // Если файл заявляет, что он больше 16 байт и меньше 64 МБ, мы предварительно выделим buffer.\\n    // Если он больше, мы сделаем это постепенно. Мы хотим избежать OOM (Out Of Memory).\\n    const last_4_bytes: u32 = @bitCast(tgz_bytes[tgz_bytes.len - 4 ..][0..4].*);\\n    if (last_4_bytes > 16 and last_4_bytes < 64 * 1024 * 1024) {\\n      // Ничего страшного, если это не удастся. Мы просто будем выделять по мере необходимости, и это вызовет ошибку, если у нас закончится память.\\n      esimated_output_size = last_4_bytes;\\n      if (zlib_pool.data.list.capacity == 0) {\\n          zlib_pool.data.list.ensureTotalCapacityPrecise(zlib_pool.data.allocator, last_4_bytes) catch {};\\n      } else {\\n          zlib_pool.data.ensureUnusedCapacity(last_4_bytes) catch {};\\n      }\\n    }\\n  }\\n}\\n Эти 4 байта сообщают Bun: «этот gzip распакуется ровно в 1 048 576 байт», поэтому он может зарезервировать точно этот объем памяти заранее. Нет повторного изменения размера или копирования данных; только единоразовое выделение памяти. Диаграмма: Preallocation using gzip ISIZE avoids buffer growth copies Для фактической decompression Bun использует \\xa0 [libdeflate](https://github.com/ebiggers/libdeflate) . Это высокопроизводительная библиотека, которая распаковывает tarballs быстрее, чем стандартный zlib, используемый большинством менеджеров пакетов. Она оптимизирована специально для современных CPUs с SIMD инструкциями. Оптимизированное извлечение tarball было бы сложно реализовать для менеджеров пакетов, написанных на Node.js. Вам потребовалось бы создать отдельную систему чтения, перейти к концу, прочитать 4 байта,  распарсить их, закрыть поток, а затем начать все сначала с распаковки. APIs Node не созданы для этого патерна. В Zig это довольно  просто, переходите к концу и читаете последние четыре байта, вот и всё! Теперь, когда у Bun есть все данные пакета, он сталкивается с другой проблемой: как эффективно хранить и получать доступ к тысячам (взаимозависимых) пакетов? Удобная структура для кеширования данных Работа с тысячами пакетов может быть непростой. Каждый пакет имеет зависимости, которые имеют свои собственные зависимости, создавая довольно сложный граф зависимостей. Во время установки менеджеры пакетов должны обходить этот граф, чтобы проверить версии пакетов, разрешить любые  конфликты и определить, какую версию установить. Им также необходимо  поднимать \\xa0(hoist) зависимости, перемещая их на более высокие уровни, чтобы несколько пакетов могли ими совместно пользоваться. Но то, как хранится этот граф зависимостей, сильно влияет на производительность Традиционные менеджеры пакетов хранят зависимости так: const packages = {\\n  next: {\\n    name: \"next\",\\n    version: \"15.5.0\",\\n    dependencies: {\\n      \"@swc/helpers\": \"0.5.15\",\\n      \"postcss\": \"8.4.31\",\\n      \"styled-jsx\": \"5.1.6\",\\n    },\\n  },\\n  postcss: {\\n    name: \"postcss\",\\n    version: \"8.4.31\",\\n    dependencies: {\\n      nanoid: \"^3.3.6\",\\n      picocolors: \"^1.0.0\",\\n    },\\n  },\\n};\\n Это выглядит аккуратно как JavaScript-код, но это не идеально для современных CPU архитектур. В JavaScript каждый объект хранится в куче (heap memory). При доступе к  packages[\"next\"]   CPU обращается к указателю, который указывает, где данные  Next  расположены в памяти. Эти данные затем содержат эти адреса просто в случайном порядке. Нет никакой гарантии локализации — объекты могут быть разбросаны по RAM, даже объекты, связанные друг с другом! Это случайное рассеяние имеет значение из-за того, как современные CPUs фактически получают данные. Современные CPUs невероятно быстры в обработке данных (миллиарды операций в секунду), но получение данных из RAM медленное. Чтобы преодолеть этот разрыв, CPUs имеют несколько уровней cache: L1 cache, маленькое хранилище, но очень быстрое (~4 CPU циклов) L2 cache, среднее хранилище, a чуть медленнее (~12 CPU циклов) L3 cache: 8-32MB хранилища, требует ~40 CPU циклов RAM: Много GB, требует ~300 циклов (медленно!) Визуализация скорости CPU cache vs RAM. Кеш оптимизации имеют значение!  pic.twitter.com/q2rkGqSUAG   — Ben Dicken  @BenjDicken n) Oct 18, 2024 «Проблема» в том, что кеши работают с строками кеша. Когда вы обращаетесь к памяти, CPU загружает не просто один байт: он загружает весь 64-байтовый чанк, в котором появляется этот байт. Он предполагает, что если вам нужен один байт, вам, вероятно, скоро понадобятся соседние байты (это называется spatial locality). Диаграмма: Cache line showing 64-byte fetch granularity Эта оптимизация отлично работает для данных, хранящихся последовательно (sequentially), но она дает обратный эффект, когда ваши данные случайным образом разбросаны по памяти. Когда CPU загружает  packages[\"next\"]  по адресу  0x2000 , он фактически загружает все байты в этой кеш строке. Но следующий пакет,  packages[\"postcss\"] , находится по адресу  0x8000 . Это совершенно другая кеш строка! Остальные 56 байт, которые CPU загрузил в кеш строку, полностью потрачены в пустую, это просто случайная память от того, что оказалось выделено рядом; может быть, удалена, может быть, части несвязанных объектов. Диаграмма: Wasted cache line bytes due to non-local allocations Но вы заплатили стоимость загрузки 64 байт, а использовали только 8... К тому времени, когда вы обратились к 512 различным пакетам (32KB / 64 байта), вы уже заполнили весь свой L1 кеш. Теперь каждый новый доступ к пакету вытесняет ранее загруженную кеш строку, чтобы освободить место. Пакет, к которому вы только что обратились, скоро будет вытеснен, и та зависимость, которая понадобится через 10 микросекунд, уже исчезла. Коэффициент попаданий в кэш падает, и каждый доступ становится поездкой к RAM на ~300 циклов вместо попадания в L1 на 4 цикла, что далеко от оптимального решения. Вложенная структура объектов создает так называемое \"pointer chasing\", распространенный анти-шаблон в системном программировании. CPU не может предсказать, куда загружать дальше, потому что каждый указатель может указывать куда угодно. Он просто не может знать, где находится следующие зависимости, пока не закончит загрузку объекта  next . При обходе зависимостей  Next  CPU должен выполнить несколько зависимые загрузки памяти: Загрузить указатель   packages[\"next\"]  → Cache miss → RAM fetch (~300 cycles) Следовать по этому указателю для загрузки  pointer next.dependencies  → Еще один промах в кеше → RAM fetch (~300 циклов) Следовать по нему, чтобы найти \"postcss\" в хеш таблице → Кеш промах → RAM fetch (~300 cycles) Следовать по этому указателю для загрузки фактических строковых данных → Кеш промах → RAM fetch (~300 cycles) Мы можем получить множество промахов в кеше, поскольку мы работаем с сотнями зависимостей, все разбросанные по памяти. Каждая загруженная нами кеш строка (64 байта) может содержать данные только для одного объекта. Со всеми этими объектами, разбросанными по гигабайтам RAM,  рабочий пул легко превышает L1 cache (32 КБ), L2 (256 КБ) и даже L3 cache (8-32 МБ). К тому времени, когда нам снова понадобится объект, вероятно, он был вытеснен со всех уровней кеша. Это ~1200 циклов (400 нс на 3GHz CPU) только для чтения одного имени зависимости! Для проекта с 1000 пакетов в среднем по 5 зависимостей каждый это 2 мс чистой задержкой памяти. Bun использует Structure of Arrays (SoA) . Вместо того чтобы каждый пакет хранил свой собственный массив зависимостей, Bun хранит все зависимости в одном большом общем массиве, все имена пакетов в другом общем массиве и так далее. Еще один указатель на то, где живут его зависимости, который, в свою очередь, содержит больше указателей на фактические строки зависимостей. // ❌ Traditional Array of Structures (AoS) - lots of pointers\\npackages = {\\n  next: { dependencies: { \"@swc/helpers\": \"0.5.15\", \"postcss\": \"8.4.31\" } },\\n};\\n\\n// ✅ Bun\\'s Structure of Arrays (SoA) - cache friendly\\npackages = [\\n  {\\n    name: { off: 0, len: 4 },\\n    version: { off: 5, len: 6 },\\n    deps: { off: 0, len: 2 },\\n  }, // next\\n];\\n\\ndependencies = [\\n  { name: { off: 12, len: 13 }, version: { off: 26, len: 7 } }, // @swc/helpers@0.5.15\\n  { name: { off: 34, len: 7 }, version: { off: 42, len: 6 } }, // postcss@8.4.31\\n];\\n\\nstring_buffer = \"next\\\\015.5.0\\\\0@swc/helpers\\\\00.5.15\\\\0postcss\\\\08.4.31\\\\0\";\\n Вместо того чтобы каждый пакет хранил указатели на свои собственные данные, разбросанные по памяти, Bun просто использует большие смежных буферов, включая: packages  хранит легковесные структуры, которые указывают, где найти данные этого пакета, используя смещения dependencies  хранит фактические взаимосвязи зависимостей для всех пакетов в одном месте string_buffer  хранит весь текст (имена, версии и т.д.) последовательно в одной большой строке versions  хранит все проанализированные семантические версии как компактные структуры Теперь доступ к зависимостям Next становится просто арифметикой: packages[0]  сообщает нам, что зависимости Next начинаются с позиции  0  в массиве  dependencies , и их 2:  { name_offset: 0, deps_offset: 0, deps_count: 2 } Переходим к  dependencies[1] , который сообщает нам, что имя  postcss  начинается с позиции 34 в  string_buffer , а версия — с позиции 42:  { name_offset: 34, version_offset: 42 } Переходим к позиции 34 в  string_buffer  и читаем  postcss Переходим к позиции 42 в  string_buffer  и читаем  \"8.4.31\" … и так далее Теперь, когда вы обращаетесь к  packages[0] , CPU загружает не только эти 8 байт: он загружает всю 64-байтовую кеш строку. Поскольку каждый пакет занимает 8 байт, а 64 ÷ 8 = 8, вы получаете  packages[0]  через  packages[7]  при одном запросе к памяти. Таким образом, когда ваш код обрабатывает зависимость  react  ( packages[0]) ,  packages[1]  через  packages[7]  уже находятся в вашем L1 кеше, готовые к доступу с нулевыми дополнительными запросами к памяти. Вот почему последовательный доступ такой быстрый: вы получаете 8 пакетов, просто обратившись к памяти один раз. Вместо множества маленьких, разбросанных выделений памяти, которые мы видели в предыдущем примере, теперь у нас всего ~6 больших распределений, независимо от того, сколько у вас пакетов. Это совершенно разные от  подхода на основе указателей, который требовал отдельного запроса к памяти для каждого объекта. Оптимизированный формат файла блокировки Bun также применяет подход структуризации массивов к своему локфайлу  bun.lock . Когда вы запускаете  bun install , Bun должен разобрать существующий локфайл, чтобы определить, что уже установлено и что нуждается в обновлении. Большинство менеджеров пакетов хранят локфайлы в виде вложенного  JSON  (npm) или  YAML  (pnpm, yarn). Когда npm разбирает  package-lock.json , он обрабатывает глубоко вложенные объекты: {\\n  \"dependencies\": {\\n    \"next\": {\\n      \"version\": \"15.5.0\",\\n      \"requires\": {\\n        \"@swc/helpers\": \"0.5.15\",\\n        \"postcss\": \"8.4.31\"\\n      }\\n    },\\n    \"postcss\": {\\n      \"version\": \"8.4.31\",\\n      \"requires\": {\\n        \"nanoid\": \"^3.3.6\",\\n        \"picocolors\": \"^1.0.0\"\\n      }\\n    }\\n  }\\n}\\n Каждый пакет становится своим собственным объектом с вложенными объектами зависимостей. JSON парсеры должны распределять память для каждого объекта, проверить синтаксис и построить сложные вложенные деревья. Для проектов с тысячами зависимостей это создает ту же проблему погони за указателем, которую мы видели ранее! Bun применяет подход  Structure of Arrays  к своему lockfile в удобочитаемом формате: {\\n  \"lockfileVersion\": 0,\\n  \"packages\": {\\n    \"next\": [\\n      \"next@npm:15.5.0\",\\n      { \"@swc/helpers\": \"0.5.15\", \"postcss\": \"8.4.31\" },\\n      \"hash123\"\\n    ],\\n    \"postcss\": [\\n      \"postcss@npm:8.4.31\",\\n      { \"nanoid\": \"^3.3.6\", \"picocolors\": \"^1.0.0\" },\\n      \"hash456\"\\n    ]\\n  }\\n}\\n\\n Это снова устраняет дублирование строк и хранит зависимости в слое удобном для кеширования. Они хранятся в порядке зависимостей, а не в алфавитном порядке или в виде вложенной иерархии. Это означает, что парсер может читать память более эффективно, избегая случайных прыжков между объектами. И не только это, Bun также предварительно выделяет память на основе размера локфайла. Так же, как и с извлечением tarball, это позволяет избежать повторяющихся циклов изменения размера и копирования, которые создают узкие места производительности во время парсинга. В качестве примечания: изначально Bun использовал бинарный формат lockfile ( bun.lockb ), чтобы полностью избежать накладных расходов от JSON парсинга, но бинарные файлы невозможно просматривать в pull requests, и их нельзя принимать при возникновении конфликтов. Копирование файлов После того как пакеты установлены и закэшированы в  ~/.bun/install/cache/ , Bun должен скопировать файлы в  node_modules . Именно здесь мы видим наибольшее влияние на производительность Bun! Традиционное копирование файлов обходит каждую директорию и копирует файлы по отдельности. Это требует нескольких системных вызовов на файл: открытие исходного файла ( open() ) создание и открытие целевого файла ( open() ) повторяющееся чтение чанков из источника и запись их в назначение до завершения ( read() /  write() ) наконец, закрытие обоих файлов ( close() ). Каждый из этих шагов требует дорогостоящего переключения режима между пользовательским режимом и ядром. Для типичного React-приложения с тысячами файлов пакетов это создает сотни тысяч до миллионов системных вызовов! Это именно та проблема системного программирования, которую мы описали ранее: накладные расходы на все эти системные вызовы становятся дороже, чем фактическое перемещение данных. Bun использует различные стратегии в зависимости от вашей операционной системы и файловой системы, используя все доступные оптимизации, специфичные для ОС. Bun поддерживает несколько бэкендов для копирования файлов, каждый с разными характеристиками производительности: macOS В macOS Bun использует нативный системный вызов Apple  clonefile()  с копирование при записи. clonefile  может к  лонировать целые деревья директорий за один системный вызов . Этот системный вызов создает новые записи метаданные для директорий и файлов, которые ссылаются на те же физические блоки диска, что и исходные файлы. Вместо записи новых данных на диск файловая система просто создает новые «указатели» на существующие данные. // Традиционный подход: миллионы системных вызовов\\nfor (each file) {\\n  copy_file_traditionally(src, dst);  // 50+ системных вызовов на файл\\n}\\n\\n// Подход Bun: ОДИН системный вызов\\nclonefile(\"/cache/react\", \"/node_modules/react\", 0);\\n SSD хранит данные в блоках фиксированного размера. Когда вы обычно копируете файл ( copy() ), файловая система выделяет новые блоки и записывает дублированные данные. С клон файлом и исходный, и «скопированный» файл имеют метаданные, которые указывает на точно такие же физические блоки на вашем SSD. Copy-on-write  означает, что данные дублируются только при изменении. Это приводит к операции  O(1)  против  O(n)  при традиционном копировании. Метаданные обоих файлов указывают на одни и те же блоки данных, пока вы не измените один из них. Диаграмма: APFS clonefile copy-on-write метаданные, указывающие на те же блоки Когда вы изменяете содержимое одного из файлов, файловая система автоматически выделяет новые блоки для отредактированных частей и обновляет metadata файла, чтобы они указывали на новые блоки. Диаграмма: Copy-on-write after modification allocates new blocks Однако это редко случается, поскольку файлы node_modules обычно доступны только для чтения после установки; мы не изменяем модули активно из нашего кода. Это делает copy-on-write чрезвычайно эффективным: несколько пакетов могут совместно использовать идентичные файлы зависимостей без использования дополнительного дискового пространства. Benchmark 1: bun install --backend=copyfile\\n  Time (mean ± σ):      2.955 s ±  0.101 s    [User: 0.190 s, System: 1.991 s]\\n  Range (min … max):    2.825 s …  3.107 s    10 runs\\n\\nBenchmark 2: bun install --backend=clonefile\\n  Time (mean ± σ):      1.274 s ±  0.052 s    [User: 0.140 s, System: 0.257 s]\\n  Range (min … max):    1.184 s …  1.362 s    10 runs\\n\\nSummary\\n  bun install --backend=clonefile ran\\n    2.32 ± 0.12 times faster than bun install --backend=copyfile\\n Когда клонирование файла не удается (из-за отсутствия поддержки файловой системы), Bun fallback переходит к  clonefile_each_dir  для клонирования каждой директории. Если это также не удается, Bun использует традиционный  copyfile  в качестве последнего запасного варианта. Linux В Linux нет  clonefile() , но есть кое-что еще старше и мощнее:  hardlinks  (жесткие ссылки). Bun реализует цепочку  fallback , которая пробует все менее оптимальные подходы, пока один не сработает: 1. Hardlinks В Linux стратегия по умолчанию для Bun — это  hardlinks .  Hardlink  вообще не создает новый файл, он только создает новое имя для существующего файла и ссылается на этот существующий файл. link(\"/cache/react/index.js\", \"/node_modules/react/index.js\");\\n Чтобы понять  hardlinks , нужно понять inodes. Каждый файл в Linux имеет inode — это структура данных, которая содержит все metadata файла (права доступа, временные метки и т.д.). Имя файла — это просто указатель на inode: Диаграмма: Linux inode с двумя записями директории (hardlink) Оба пути указывают на один и тот же inode. Если вы удалите один путь, другой останется. Однако, если вы измените один, изменения увидят оба (потому что это один и тот же файл!). Диаграмма: Жестко связанные пути, ссылающиеся на один и тот же inode Это дает большой прирост производительности, потому что  нет никакого перемещения данных . Создание hard link требует одного системного вызова, который выполняется за микросекунды, независимо от того, связываете ли вы файл размером 1 КБ или бандл размером 100 МБ. Гораздо эффективнее, чем традиционное копирование, которое должно читать и записывать каждый байт. Они также чрезвычайно эффективный для дискового пространства, поскольку на диске всегда только одна копия фактических данных, независимо от того, сколько пакетов ссылается на одни и те же файлы зависимостей. Однако у hardlinks есть ограничения. Они не могут пересекать границы файловых систем (например, ваш кеш находится в другом месте, чем ваш node_modules), некоторые файловые системы их не поддерживают, и определенные типы файлов или конфигурации прав могут привести к сбою создания hardlink. Когда hardlinks невозможны, у Bun есть несколько запасных сценариев: ioctl_ficlone  Он начинается с  ioctl_ficlone , который включает  copy-on-write  в файловых системах, таких как Btrfs и XFS. Это очень похоже на систему  copy-on-write  в  clonefile  тем, что она также создает новые файловые ссылки, которые совместно используют одни и те же данные на диске. В отличие от hardlinks, это отдельные файлы; они просто находятся в общем хранилище до изменения. copy_file_range  Если  copy-on-write  недоступен, Bun пытается по крайней мере сохранить копирование в kernel space и запасной вариант переходит к  copy_file_range . При традиционном копировании ядро читает с диска в буфер ядра, затем копирует эти данные в буфер вашей программы в пользовательское пространство. Позже, когда вы вызываете  write() , оно копирует его обратно в буфер ядра перед записью на диск. Это четыре операции с памятью и несколько смены контекстов! С  copy_file_range  ядро читает с диска в буфер ядра и записывает прямо на диск. Всего две операции и 0 смен контекста для перемещения данных. sendfile  Если это недоступно, Bun использует  sendfile . Это системный вызов, изначально спроектирован для сетевых передач, но он также эффективен для копирования данных прямо между двумя файлами на диске. Эта команда также сохраняет данные в пространстве ядра: ядро читает данные из одного места (ссылка на открытый файл на диске, например, исходный файл в  ~/.bun/install/cache/ ) и записывает их в другое место (например, целевой файл в node_modules), все в пространстве памяти ядра. Этот процесс называется опирование с диска на диск, так как он перемещает данные между файлами, хранящимися на одном или разных дисках, без касания памяти вашей программы. Это более старый API, но более широко поддерживаемый, что делает его надежным запасным сценарием, когда новые системные вызовы недоступны, при этом все равно сокращение количества обращений к памяти. copyfile  В крайнем случае, Bun использует традиционное копирование файлов; тот же подход, который используют большинство менеджеров пакетов. Это создает совершенно отдельные копии каждого файла путем чтения данных из кеша и записи их в назначение с помощью цикла  read() / write() . Это использует несколько системных вызовов , что именно то, что Bun пытается минимизировать. Это наименее эффективный вариант, но он универсален и совместимый. Benchmark 1: bun install --backend=copyfile\\n  Time (mean ± σ):     325.0 ms ±   7.7 ms    [User: 38.4 ms, System: 295.0 ms]\\n  Range (min … max):   314.2 ms … 340.0 ms    10 runs\\n\\nBenchmark 2: bun install --backend=hardlink\\n  Time (mean ± σ):     109.4 ms ±   5.1 ms    [User: 32.0 ms, System: 86.8 ms]\\n  Range (min … max):   102.8 ms … 119.0 ms    19 runs\\n\\nSummary\\n  bun install --backend=hardlink ran\\n    2.97 ± 0.16 times faster than bun install --backend=copyfile\\n Эти оптимизации копирования файлов решают основную проблему: system call overhead. Вместо использования универсального подхода (one-size-fits-all) Bun выбирает наиболее эффективное копирование файлов, специально tailored для вас. Многоядерный параллелизм Все упомянутые выше оптимизации отличны, но они направлены на снижение нагрузки на одно ядро процессора. Однако у современных ноутбуков есть 8, 16, даже 24 ядер! У Node.js есть пул потоков, но вся реальная работа (например, выяснение того, какая версия React работает с какой версией webpack, построение графа зависимостей, решение о том, что установить) происходит в одном потоке и на одном ядре CPU. Когда npm работает на вашем M3 Max, одно ядро работает очень усердно, а остальные 15 простаивают. Ядро процессора может независимо исполнять инструкции. Ранние компьютеры имели одно ядро, они могли делать только одну вещь за раз, но современные CPU помещают несколько ядер на один чип. 16-ядерный CPU может исполнять 16 различных потоков инструкций одновременно, а не просто очень быстро переключаться между ними. Это еще одно фундаментальное узкое место  для традиционных менеджеров пакетов: неважно, сколько у вас ядер, менеджер пакетов может использовать только одно ядро CPU. Bun использует другой подход без блокировок,  work-stealing  (крадущей работой) архитектурой пула потоков. Work-stealing  означает, что бездействующие потоки могут «красть» ( steal ) ожидающие задачи ( pending tasks ) из очередей занятых потоков. Когда поток завершает свою работу, он проверяет свою локальную очередь, затем глобальную очередь, а затем крадет у других потоков. Ни один поток не простаивает, когда еще есть работа. Вместо того чтобы ограничиваться  event loop  JavaScript, Bun порождает нативные потоки, которые могут полностью использовать каждое ядро CPU. Пул потоков автоматически масштабируется в соответствии с количеством ядер вашего устройства, позволяя Bun максимально параллелизовать части процесса установки, интенсивные по вводу-выводу. Один поток может распаковывать  tarball  next, другой — разрешать зависимости  postcss , третий — применять  patches  к webpack и так далее. Но многопоточность часто сопровождается накладными расходами на синхронизацию. Те сотни тысяч вызовов  futex , которые делал npm, были просто потоками, постоянно ждущими друг друга. Каждый раз, когда поток хочет добавить задачу в общую очередь, он должен сначала заблокировать ее, блокируя все остальные потоки. // Традиционный подход: Блокировки (Locks)\\nmutex.lock();                   // Thread 1 получает эксклюзивный доступ\\nqueue.push(task);               // Только Thread 1 может работать\\nmutex.unlock();                 // Наконец-то releases lock\\n// Проблема: Потоки 2-8 заблокированы, ждут в очереди\\n Bun вместо этого использует структуры данных без блокировок. Они используют специальные инструкции CPU, называемые атомарные операции, которые позволяют потокам безопасно изменять общие данные без блокировок: pub fn push(self: *Queue, batch: Batch) void {\\n  // Atomic compare-and-swap, происходит мгновенно\\n  _ = @cmpxchgStrong(usize, &self.state, state, new_state, .seq_cst, .seq_cst);\\n}\\n В более раннем benchmark мы видели, что Bun смог обработать 146 057 файлов package.json в секунду против 66 576 у Node.js. Это и есть влияние использования всех ядер вместо одного. Bun также по-другому запускает сетевые операции. Традиционные менеджеры пакетов часто блокируются. При загрузке пакета CPU простаивает в ожидании сети. Bun поддерживает пул из 64(!) одновременных HTTP-соединений (настраивается через  BUN_CONFIG_MAX_HTTP_REQUESTS ) на выделенных сетевых потоках. Сетевой поток работает независимо со своим собственным event loop, обрабатывая все загрузки, в то время как потоки CPU занимаются извлечением и обработкой. Ни один не ждет другого. // Traditional: all threads share one allocator\\nThread 1: \"I need 1KB for package data\"    // Lock allocator\\nThread 2: \"I need 2KB for JSON parsing\"    // Wait...\\nThread 3: \"I need 512B for file paths\"     // Wait...\\nThread 4: \"I need 4KB for extraction\"      // Wait...\\n Bun также дает каждому потоку свой собственный пул памяти. Проблема «традиционной» многопоточности заключается в том, что все потоки соревнуются за один и тот же memory аллокатор памяти. Это создает утверждение: если 16 потокам одновременно нужна память, им приходится ждать друг друга. // Bun: каждый поток имеет свой собственный распределитель\\nThread 1: Allocates from pool 1    // Instant\\nThread 2: Allocates from pool 2    // Instant\\nThread 3: Allocates from pool 3    // Instant\\nThread 4: Allocates from pool 4    // Instant\\n\\n Заключение Менеджеры пакетов, которые мы тестировали, не были построены неправильно — они были решениями, созданными для ограничений своего времени. npm  дал нам основу для строительства,  yarn  сделал управление рабочими пространствами менее болезненным, а  pnpm  придумал умный способ экономии места и ускорения работы с помощью  hardlinks . Каждый усердно работал над решением проблем, с которыми разработчики действительно сталкивались в то время. Но этого мира больше не существует.  SSD  стали в  70 раз быстрее ,  CPU  имеют десятки ядер, а память стала дешевой. Реальное узкое место сместилось от скорости железа к программным абстракциям. Подход Bun не был революционным, он просто был готов посмотреть на то, что на самом деле замедляет работу сегодня. Когда  SSD  могут обрабатывать миллион операций в секунду, зачем мириться с накладными расходами пула потоков? Когда вы в сотый раз читаете один и тот же манифест пакета, зачем снова парсите JSON? Когда файловая система поддерживает  copy-on-write , зачем дублировать гигабайты данных? Инструменты, которые определят следующее десятилетие производительности разработчиков, создаются прямо сейчас командами, которые понимают, что узкие места производительности сместились, когда хранилища стали быстрыми, а память — дешевой. Они не просто постепенно улучшают существующее; они переосмысливают то, что возможно. Установка пакетов в  25 раз быстрее  — это не «магия»: это то, что происходит, когда инструменты создаются для железа, которое у нас на самом деле есть. Оригинал статьи: Behind The Scenes of Bun Install', hub='веб-разработка'), Page(id='946486', title='День программиста: готовимся к празднику и составляем список антиподарков для кодеров', content='13 сентября в России отмечают День программиста. Рассказываем, как появился этот праздник и что никогда не стоит дарить, если вы искренне хотите порадовать человека.\\xa0 ~1 млн человек работают в российской IT-индустрии на 2024 год. Это на 13 % больше, чем в 2023 году  согласно данным АНО «Цифровая экономика» В сентябре IT-специалисты отмечают профессиональный праздник. Дата выбрана не случайно: это 256-й день года. Само число 256 — количество различных значений, которые можно выразить с помощью восьмиразрядного байта, а также максимальная целая степень числа 2. Дата праздника меняется в зависимости от того, високосный год или нет. В 2025 она выпадает на 13 сентября.\\xa0 Впервые идею о профессиональном празднике для айтишников подал основатель издательского дома «Компьютерра» в 1996 году, но она не прижилась. Через шесть лет сотрудники веб-студии «Параллельные технологии» Валентин Балт и Михаил Червяков организовали сбор подписей, но несмотря на 10 тыс. участников, инициативу не поддержали и праздник отмечали неофициально. Только в 2009 году Министерство связи разработало проект указа «О Дне программиста», который подписал Дмитрий Медведев. Отмечаем День программиста в офисе: 7 идей на минималках Часто в этот день устраивают тематические мероприятия: конференции, выставки, хакатоны. Для нас самих в НИЦ ЦТ это еще один повод придумать что-то хорошее для команды, так как мы много работаем. Мы обсуждали разные варианты, среди которых выбрали те, что не занимают много времени. Вот наша подборка идей для праздника в офисе: 1. Чайная церемония  По-русски или по-китайски. Например, чайная “Нитка” продвигает русскую чайную традицию, читает лекции на эту тему и угощает спешлти чаем. Можно пригласить специалиста, который проведет чайную церемонию и расскажет про особенности китайского чаепития. А можно сделать все самостоятельно: купить на маркетплейсах набор для церемонии (до 2500 рублей) и собрать коллег за общим столом.\\xa0 2. Квиз Соревнуйтесь между собой или в командах. В интернете есть готовые бесплатные и платные варианты. Все, что нужно — это переговорная, листочки и ручки.  3. Квест-игра Купите готовую игру, где все уже придумали за вас. Например, издательство “Креп” выпускает квесты для взрослых, которые можно организовать в помещении. 4. Киномарафон со снеками Соберите топ любимых фильмов или сериалов, закуски и организуйте совместный просмотр.  5. Тематическая викторина Фантазируйте: это могут быть вопросы про IT, факты про коллег или про вашу компанию.\\xa0 6.Фотосессия Купите забавный реквизит и закажите праздничный фон. 7. Турнир по киберспорту CS:GO, Dota 2, League of Legends, Fortnite: соберите команды и устройте зарубу прямо в офисе. Спасибо, но нет! Список подарков, которые вряд ли понравятся программистам Фразу “Дорог не подарок, а внимание” придумали люди, которые никогда в жизни не получали бесполезные подарки. Мы опросили коллег и составили список того, чему программисты не обрадуются.\\xa0Мы опросили коллег и узнали, что не стоит дарить кодерам, и почему. 1. USB-гаджеты: пылесосы, подогрев для кружки, вентилятор.  “Потому что быстро ломаются”. 2. Кружки с мемами про айтишников.  “Потому что это уже устарело”. 3. Флешки.  “Потому что их никто не носит с собой” 4. Чехлы для всего. “Потому что не всем они нужны” 5. Посуда. “Потому что бесполезно” А вот премиям от начальства программист обрадуется точно! От друзей и родных будет приятно получить компуктерные и офисные аксессуары, например, кейкап или увлажнитель воздуха.  Что приятного или полезного дарили вам на День программиста?', hub='история it'), Page(id='946468', title='Чтение на выходные: «Стратегия процветания» Рона Аднера', content='Что общего между Kodak, Tesla и Uber? Все они — игроки в сложных бизнес-экосистемах, где победа зависит не только от технологий, но и от умения договариваться. Именно об этом книга Рона Аднера, которую мы разбираем сегодня. Ключевое в ней —\\xa0стратегия. Главное из названия —\\xa0«экосистемы». Автор декларирует, что изменения именно в них, а не в отраслях, как раньше, задают рынку тон. К слову, Джим Коллинз назвал его «одним из наших самых важных стратегических мыслителей XXI века», поэтому есть весомый повод мнению доверять. Рон Аднер —\\xa0профессор стратегии и предпринимательства в Tuck School of Business Дартмутского колледжа и основатель консалтинговой компании Strategy Insight Group, которая помогает клиентам исключить слепые зоны при выходе на рынок и создавать… стратегию. Не хотим превращать этот термин в спам-слово, тем не менее, в книге оно встретится читателю почти двести раз! Возможно, упомянем его ещё разок-другой и здесь. Основная мысль заключается в том, что успех компании зависит не только от её внутренней эффективности и конкурентных преимуществ. Экономические процессы меняются — необходимо выстраивать взаимоотношения с поставщиками, партнёрами, технологическими платформами и даже косвенными участниками рынка так, чтобы бизнес-экосистема функционировала и случался win-win. По сути, автор предлагает переосмыслить конкуренцию и лидерство в современном мире: оказалось, с этим есть нюансы. «Так, например, в 1905 году для создания автопромышленной экосистемы понадобилось сначала создать набор взаимоприемлемых ролей, позиций и потоков между производителями «железного коня», поставщиками топлива и ремонтных услуг, риск-менеджерами и т.д. Лишь после того, как эта согласованная структура сформировалась, границы стали осязаемы и позволили говорить об отраслях: индустрия машиностроения, техобслуживания, автострахования, регулирующие органы и т. д. Сегодня, с появлением беспилотных автомобилей и услуг перевозок по требованию вроде Uber и Lyft, к устоявшейся структуре начинают возникать вопросы, и ее участникам приходится заново пересматривать нынешние отраслевые границы в попытках определиться со значением и структурой “экосистемы перевозок”», — объясняет он. При этом подрыв экосистемы, по мнению Рона Аднера, происходит на уровне ценностного предложения, а не только вместе с переменами на технологическом и отраслевом уровне. То есть технологически можно быть впереди планеты всей, но этого недостаточно: нужно выстроить ценностную архитектуру. Рон Аднер предупреждает, что даже самые инновационные компании могут потерпеть крах, если не учитывают хрупкость своей бизнес-экосистемы. А подрыв происходит, когда один слабый элемент разрушает всю цепочку создания ценности. Например, почему Kodak, изобретя цифровую камеру, всё равно обанкротилась? Потому что не смогла перестроить экосистему: плёнка, печать, магазины — всё это рухнуло, когда фотографии перешли в цифру. А вот Tesla не просто создала электромобиль, но и построила сеть зарядок, договорилась с государствами о льготах — и выиграла.\\xa0 Иными словами, успех требует не только лидерства внутри компании, но и стратегической синхронизации всех участников экосистемы — иначе инновации превращаются в «пустые победы», а вложения в разработку продукта могут обернуться банкротством. К слову, содержание всей книги построено на примерах. В том числе, конкуренции Wayfair и Amazon, TomTom и Google, Spotify и Apple Music, а также кейсах Tesla, Microsoft, GE Predix, Wolters Kluwer, 23andMe, Zebra Technologies и упомянутой выше компании на букву K. Отдельное внимание уделяется теме лидерства. Рон Аднер объясняет разницу между лидерством в компании и экосистеме. «СЕО у себя дома — король; ему подчиняется вся внутренняя экосистема его организации. Но СЕО, уговаривающий других СЕО вступить в коалицию, — это монарх, который наносит визит в другое королевство. В своей организации СЕО, использующий подход “лидера-служителя”, мотивирует подчиненных, демонстрируя заботу и скромность. Но подобное “служение” имеет смысл, только если у вас есть полномочия на жертвенность. У чужака на чужой земле такой власти нет. Так что не стоит удивляться, что успешным корпоративным лидерам не удается выстроить зарождающиеся экосистемы, в которых они не обладают властью. Соответственно, здесь уже нужно заглядывать в “учебные пособия” не по власти, а по дипломатии», — сравнивает автор. Сказать о том, что книга будет интересна топ-менеджерам и предпринимателям, пожалуй, маловато. Она написана простым языком и, вероятно, заинтересует любого, кто хочет знать больше о том, на каких условиях функционирует современный рынок. Объем небольшой — как раз на выходные, отпуск с книгой или даже в дорогу. Читайте, если хотите прокачать эрудицию по брендам и свои стратегические представления. О сервисе Онлайн Патент Онлайн Патент\\xa0— цифровая система №\\xa01\\xa0в\\xa0рейтинге Роспатента. С 2013\\xa0года мы создаем уникальные LegalTech‑решения для\\xa0защиты и управления интеллектуальной собственностью. Зарегистрируйтесь в\\xa0 сервисе \\xa0Онлайн‑Патент и получите доступ к\\xa0следующим услугам: Онлайн‑регистрация программ, патентов на\\xa0изобретение, товарных знаков, промышленного дизайна; Подача заявки на\\xa0внесение в\\xa0реестр отечественного ПО ; Поиск по\\xa0программам ; Регистрация программы в\\xa0Роспатенте ; Регистрация товарных знаков ; Опции ускоренного оформления услуг; Бесплатный поиск по\\xa0базам патентов, программ, товарных знаков; Мониторинги новых заявок по\\xa0критериям; Онлайн‑поддержку специалистов.', hub='управление продажами'), Page(id='945512', title='Модернизация древнего драйвера Linux с помощью Claude Code', content='В качестве одного из своих увлечений я помогаю людям восстанавливать данные со старых ленточных кассет, например, QIC-80, которые в 90-е годы часто применялись для резервного копирования. Ими пользовались отдельные люди, компании, операторы досок объявлений и не только. К ленточным кассетам я испытываю очень тёплые чувства. Есть нечто особое в том тактильном ощущении, которое возникает, когда держишь эту кассету в руках. Нечто такое, что делает сам процесс её использования приятным, несмотря на все огрехи дизайна, которыми славилась технология QIC. Причём, если вникнуть в процесс и  воссоздать все необходимые условия , то данные на этих лентах, даже спустя столько времени, вполне можно восстановить.  Когда мне поручают снять информацию с очередной кассеты QIC-80, я запускаю свою старенькую рабочую станцию, к которой подключён подходящий ленточный накопитель, и загружаю очень  старую версию Linux  (а именно CentOS 3.5). Только в ней я могу использовать драйвер ядра  ftape , необходимый для взаимодействия с этим накопителем и сброса на ПК двоичного содержимого ленты.  Суть в том, что накопитель подключается к  контроллеру дисковода  на материнской плате. Этот ловкий хак я реализовал из соображений экономии. Вместо того, чтобы покупать отдельный SCSI-адаптер (стандартный интерфейс для использования ленточных устройств последнего поколения), можно просто подключить накопитель к контроллеру дисковода, который доступен на большинстве ПК. Причём работать он сможет даже параллельно с дисководом — через общий шлейф! Естественно, есть здесь и недостаток — скорость передачи данных ограничена скоростью дисковода, то есть составляет где-то 500 Кб/с (именно килобит). Ещё один минус в том, что протокол для взаимодействия с этими ленточными накопителями очень небрежный, нестандартный и не особо поддерживается. В итоге я проделал «хак» во всех смыслах. BIOS материнки понятия не имеет, что за устройство к ней подключили, и именно пользовательское ПО должно понимать, как оперировать его портами ввода/вывода, таймингами, прерываниями и прочим, чтобы контроллер отправлял ему правильные команды.  Было создано немного проприетарных инструментов для работы с этими накопителями под MS-DOS и Windows 3.x/9x, и лишь одна опенсорсная реализация под Linux — тот самый\\xa0 ftape . Естественно, можно использовать для чтения лент оригинальные инструменты DOS/Windows, но лишь  ftape  позволяет прочитать «сырое» содержимое, каким бы проприетарным ПО оно ни было изначально записано. Собственно, поэтому я предпочитаю делать дамп содержимого с помощью него и уже потом заниматься декодированием проприетарной структуры данных, извлекая файлы.  Проблема в том, что драйвер  ftape  не обслуживается где-то с 2000 года, и из ядра Linux его давно удалили. Вот мне и приходится заводить эту древнюю ОС всякий раз, когда я работаю с одним из таких ленточных накопителей. Круто, если бы  ftape  работал в современных дистрибутивах. Это бы обеспечило массу удобств и возможностей. *** Перейдём ближе к делу. Пару недель назад мне в голову пришла идея обратиться к Claude Code с простым запросом: > Вот репозиторий, который содержит код драйвера ядра Linux, отвечающего за взаимодействие с устаревшими ленточными накопителями, подключёнными к контроллеру дисковода (FDC) на материнской плате. К сожалению, этот драйвер уже давно не обслуживается и может компилироваться только в ядре версии 2.4. Я хочу модернизировать этот драйвер, чтобы он компилировался с помощью последних версий ядра. ● Я помогу вам модернизировать этот драйвер ядра Linux для устаревших ленточных накопителей. Это серьёзная задача, которая потребует обновить код, чтобы драйвер работал с современными API ядра и отвечал всем последним требованиям. И спустя несколько итераций доработок Claude выдала мне готовый драйвер ядра, который компилировался без ошибок. Фишка в том, что эта модель может итеративно получать вывод компилятора и вносить необходимые изменения, пока компиляция не завершится успешно. В ходе анализа кода драйвера Claude обнаружила длинный список функций и структур ядра, которые по понятным причинам за период между версией 2.4 и 6.8 устарели или были замещены. Поразительно, но Claude нашла все устаревшие фрагменты и заменила их корректными современными аналогами, потребовав лишь несколько ручных доработок кода (подробнее об этом чуть позже). Вот только на этом этапе драйвер по-прежнему нужно было компилировать в рамках всего дерева ядра, а я хотел, чтобы он стал отдельным загружаемым модулем. Но и с этим проблем не возникло: > А можно как-то скомпилировать только этот модуль на месте, не копируя его в дерево исходного кода ядра? ● Да! Вы можете компилировать модули ядра вне дерева, не копируя их в его исходный код. Я создам подходящую отдельную систему сборки для драйвера ftape. …И создала. Причём без дополнительных промтов. В итоге у меня на руках был загружаемый модуль ядра ( .ko ), который я мог начать использовать с реальным железом. Вполне ожидаемо модуль ещё не «работал» — он загружался, но что-то мешало ему корректно взаимодействовать с ленточным накопителем. Я же был настойчив. Поскольку загрузка/выгрузка модулей ядра требует sudo, с этого момента я больше не мог доверять Claude столь чувствительные операции. К счастью, драйвер ftape уже был щедро усыпан сообщениями логирования (вызовами  printk() , результат которых выводился в  dmesg ). Поэтому я загрузил модуль сам и поочерёдно вставлял вывод  dmesg  в Claude, чтобы она сравнивала его с заведомо правильным логом dmesg, который я ранее сохранил после успешного чтения ленты. Потребовалось несколько итераций, и Claude нашла оставшиеся фрагменты, которые мешали модулю работать с железом. ● Прекрасно! У модуля есть параметры, которые можно настраивать. Проблема в том, что без настройки все базовые адреса по умолчанию устанавливаются на\\xa0 -1 , который в функции конфигурации преобразуется в  0xffff , из-за чего обнаружение проваливается. ● Ошибка «No such device or address» (ENXIO) возникает, потому что модуль  ftape-internal  не настроен на корректный базовый адрес порта ввода-вывода контроллера дисковода. Проблема крылась в  fdc-internal.c:735-737 : если  fdc->sra == 0xffff , возвращается  -ENXIO . А поскольку настройки установлены не были, все значения по умолчанию оказывались  –1 , то есть превращались в  0xffff .... Наконец-то у меня был модуль ядра, который мог а) загружаться, б) обнаруживать ленточный накопитель и в) сбрасывать содержимое ленты на ПК. *** Итак, буквально за пару вечеров мне удалось решить, казалось, безнадёжную задачу по использованию драйвера ftape\\xa0в современных версиях ядра. Но есть здесь и важный нюанс. У меня имеется некоторый опыт работы с модулями ядра, и я хорошо разбираюсь в общих принципах языка С, поэтому перехваливать успех Claude в этом сценарии не стану. По факту для получения рабочего модуля потребовалось не три промта, а несколько этапов диалога и ручные правки кода. Без базового понимания внутренней структуры модуля ядра подобная затея оказалась бы невозможной. Кроме того, в результате этого интересного опыта у меня родилось несколько рекомендаций по использованию современных ИИ-помощников для программирования: Будьте открыты к полноценному сотрудничеству с этими инструментами. Работа с Claude Code напоминала реальное сотрудничество с коллегой-разработчиком. Люди часто приводят в сравнение работу с джуниорами, и я думаю, что в целом так оно и есть: модель делает то, что ты ей скажешь, стремится угодить, высказывается самоуверенно, легко извиняется и хвалит тебя за «абсолютную правоту», когда ты указываешь ей на ошибку… В итоге именно ты (человек) всё так же остаёшься ответственным за направление диалога, принятие основных решений, установку архитектурных требований и указание потенциальных проблем на ранних этапах. Будьте максимально конкретными и используйте в описании задачи термины из предметной области Я не утверждаю, что являюсь экспертом по промт-инжинирингу, но на моём опыте самыми эффективными оказались запросы, в которых я чётко описывал структуру нужной функции, после чего указывал на недочёты в этой структуре, которые LLM должна была восполнить. (Почему-то на ум ассоциативно приходит образ структуры стволовых клеток, из которых можно вырастить человеческое ухо). Вырабатывайте интуицию, чтобы понимать, какие именно задачи по силам решить тому или иному агенту Все эти агенты не волшебные джинны, и они не могут сделать буквально всё, о чём вы их попросите. Если вы дадите модели задачу, для которой её не готовили, то расстроитесь полученному результату и откажетесь от использования этих инструментов, даже не дав им возможности проявить себя. В этом смысле будет полезно изучить принцип работы LLM, чтобы иметь представление об их сильных и  слабых  сторонах. Используйте эти инструменты как множитель для ваших собственных навыков При достаточном желании и усердии я бы мог проделать всю эту модернизацию сам. Но тогда мне бы потребовалось изучить особенности разработки ядра, которые были актуальны 25 лет назад. Пришлось бы несколько недель копаться в документации, которая на сегодня уже является абсолютно бесполезным знанием. Вместо этого я буквально за пару дней в ходе простого диалога добился от агента нужного результата со всеми сопутствующими пояснениями.  Естественно, я проверял и тестировал все вносимые им изменения и попутно освоил много нового, что действительно пригодится мне в будущем. Это и современные соглашения о разработке ядра, и интересные детали об архитектуре x86, и несколько полезных команд терминала, которые я взял на вооружение. Используйте эти инструменты для ускоренного знакомства с новыми фреймворками Я совсем не разработчик ядра, но опыт с апгрейдом драйвера разжёг во мне интерес к этому направлению. Да и по факту программирование ядра вовсе не такая сложная задача, как может показаться. У меня был ещё один эпизод вайб-кодинга, не связанный с этим. В нём я создал приложение Flutter, не имея опыта работы с этим фреймворком. Если вы, как и я, предпочитаете учиться на практике, то эти инструменты сильно ускорят освоение новых фреймворков, освободив время для более высокоуровневого продумывания архитектуры. *** Как бы то ни было, оглядываясь назад, я могу с удовлетворением сказать, что ftape получил  вторую жизнь ! Спустя 25 лет со своего последнего официального релиза, он снова может компилироваться и стал пригоден для использования в современных системах Linux. Я ещё продолжаю вносить в него некоторые доработки, но уже убедился, что он отлично работает с ленточными накопителями из  моей коллекции , а также с приводами, подключающимися к параллельному порту, которые он тоже поддерживает. Физическая схема подключения осталась очень похожей, но в качестве ОС теперь используется Xubuntu 24.04, а не CentOS 3.5.\\xa0 До встречи! \\xa0  ', hub='искусственный интеллект'), Page(id='946424', title='Nextjs SSR vs SSG. Приготовить заранее или испечь по заказу? Гайд по рендерингу для пиццерий и разработчиков ', content='Cowabunga, друзья! На связи Игорь, frontend-разработчик компании  fuse8 . Даже если вы не застали легендарный мультсериал 90-х, вы наверняка слышали о четырех черепашках-мутантах в цветных банданах, которые мастерски владеют мечом, нунчаками, парой сай и бо… и которые без ума от пиццы. Но проходят дни, пролетают года и вот уже Шреддер повержен, Крэнг изгнан, и что остается делать нашим уже далеко не подросткам черепашкам-ниндзя. Правильно! Открывать свою пиццерию. Казалось бы — идеальный план, однако управление пиццерией — это не менее сложное задача, чем битва с силами зла. Так их одержимость пиццей превратилась в идеальную метафору для главного вызова в современной веб-разработке: как мгновенно накормить «голодных» пользователей свежим контентом, не заставляя их томиться в ожидании у экрана? И здесь наша задача, как «разработчиков-поваров» — выбрать правильную стратегию готовки, чтобы не подвести своих клиентов в решающий момент. В контексте  Next.js  с использованием App Router эти стратегии носят названия  SSG  (Static Site Generation) и  SSR  (Server-Side Rendering). И от выбора между ними напрямую зависят ключевые метрики вашего сайта: скорость загрузки, отклик, SEO и, в конечном счете, удовлетворенность пользователя. В этой статье мы поможем нашим героям-ниндзя разобраться в тонкостях пиццерийного бизнеса и параллельно разберемся, какая стратегия рендеринга подойдет для вашего проекта. Погнали! Client Side Rendering (CSR). Старая школа от Леонардо. Леонардо как истинный лидер и стратег предложил подход, где каждый клиент  сам «собирает» свою пиццу  прямо на месте: выбирает начинку, тесто и добавляет специи по своему вкусу. Client-Side Rendering (CSR)  — \\xa0это классический паттерн, который долгое время был основным для одностраничных приложений (SPA), построенных на фреймворках вроде React. В рамках  Next.js  мы можем использовать такой подход   при разработке отдельных частей приложения, где это уместно, например, админ панели. Как это работает:\\xa0 Сервер отдает браузеру практически пустой HTML-файл с базовой разметкой и JS файлом.\\xa0 Браузер загружает и выполняет JS. JS в браузере делает запросы к API. Получаем данные, JS динамически строит весь интерфейс прямо в браузере, Проблемы подхода:\\xa0 Медленная первоначальная загрузка  — пока не выполнится весь JS, пользователь видит пустую страницу. Плохая SEO-оптимизация  — поисковые боты плохо индексируют, т.к. по факту к нам приходит пустой HTML. Проблемы с медленными устройствами  — на слабых телефонах рендеринг мог занимать продолжительное время. \\xa0Но есть и преимущества: Сниженная нагрузка на сервер  \\xa0— весь рендер происходит на стороне устройства клиента Глобально  CSR  подход отлично подходит для сайтов с высокой интерактивностью при отсутствии потребности в SEO-оптимизации. Например, веб-приложения по типу Figma. Static Site Generation (SSG). Стратегия Донателло: склад готовой пиццы на любой случай. Донателло гениален и все продумывает заранее. Он предложил наготовить горы пиццы самых ходовых вкусов. Вся пицца аккуратно разложена по коробкам, подписана и ждет своего часа в холодильнике. Как только клиент говорит «Хочу пиццу!», ему тут же достают ее из холодильника, разогревают и протягивают горячую коробку. Все предсказуемо и быстро. Концепция SSG подхода – это отрисовывать страницы во время сборки, а именно – в момент запуска  next build . Это означает, что процесс создания веб-страниц происходит до запуска сайта и получения пользователями доступа к нему, т.е. HTML и контент для каждой страницы предварительно отрендерены или сгенерированы. Когда пользователь заходит на страницу вашего сайта, предварительно отрендеренный статический HTML мгновенно доставляется в его браузер без какого-либо дополнительного выполнения кода или обработки данных в момент его запроса. По умолчанию  Next.js  старается генерировать страницы статически. Давайте посмотрим, как это будет выглядеть в коде. Для примера создадим простую главную страницу: import Link from \"next/link\";\\n\\n\\n\\n\\nexport default async function MainPage() {\\n\\n\\xa0\\xa0return (\\n\\n\\xa0\\xa0\\xa0\\xa0<div>\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<Link href=\"/turtles\">All turtles</Link>\\n\\n\\xa0\\xa0\\xa0\\xa0</div>\\n\\n\\xa0\\xa0);\\n\\n} Запустив в дальнейшем команду  next build ,  мы убедимся, что  главная страница и страница «not-found»  сгенерированы как «чистая» статика. Такое поведение типично, когда страница не зависит от каких-либо данных или планирует получать их на клиенте. Я осознаю всю ответственность – разговоры про пиццу уж очень сильно разогревают аппетит, и повышается риск того, что вместо поглощения знаний вы сорветесь в ближайшую пиццерию, где захотите поглотить сытнейшую 4 сыра «Мазеррати». Поэтому давайте немного отойдем от пицц и «заготовим» страницы для каждой черепашки, где будем выводить их ключевые качества и оружие.  Для этого создадим следующий динамический роут: В рамках  SSG  подхода мы используем специальную функцию  generateStaticParams  (можно сказать, что это инструкция, которая говорит NEXT.js, какое количество и какие страницы «заготовить» заранее)   и сделаем запрос к  API  для получения данных: export async function generateStaticParams() {\\n\\n\\xa0\\xa0const response = await fetch(\"https://www.turtletime.dev/api/v1/turtles\");\\n\\n\\xa0\\xa0const turtles = await response.json();\\n\\n\\n\\n\\n\\xa0\\xa0return turtles.map((turtle) => ({\\n\\n\\xa0\\xa0\\xa0\\xa0name: turtle.name,\\n\\n\\xa0\\xa0}));\\n\\n}\\n\\n\\n\\nexport default async function TurtlePage({ params }) {\\n\\n\\xa0\\xa0const { name } = params;\\n\\n\\xa0\\xa0const response = await fetch(\\n\\n\\xa0\\xa0\\xa0\\xa0https://www.turtletime.dev/api/v1/turtles/${name}\\n\\n\\xa0\\xa0);\\n\\n\\xa0\\xa0const turtle = await response.json();\\n\\n\\n  return (\\n\\n\\xa0\\xa0\\xa0\\xa0<div>\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<h1>{turtle.name}</h1>\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<p>Personality: {turtle.personality}</p>\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<p>Weapon: {turtle.weapon}</p>\\n\\n\\xa0\\xa0\\xa0\\xa0</div>\\n\\n\\xa0\\xa0);\\n\\n} Запустив команду  next-build ,  получим следующую картину: Здесь мы видим, что во время сборки генерируются все известные страницы для черепашек. Все запросы к этим страницам (например,  /turtles/donatello ) кэшируются, чтобы в дальнейшем обрабатываться мгновенно. Плюсы SSG подхода:\\xa0 Производительность . Мгновенная скорость загрузки. Пользователь получает готовую страницу сразу. Time to First Byte (TTFB) — минимальный. Небольшая нагрузка на сервер , так как он просто отдает уже готовый файл.\\xa0 SEO дружелюбность.  С сервера к нам приходит готовая HTML страница, которая хорошо индексируется поисковыми роботами. Минусы подхода:\\xa0 Риск устаревания данных.  Данные актуальны только на момент сборки. Ограниченное обновление в режиме реального времени . При использовании SSG вы не сможете эффективно изменять или вносить обновления в режиме реального времени на свой сайт. Плохая масштабируемость . Не очень хорошо работает, если нужно заранее «заготавливать» миллионы страниц. SSG стратегия идеально подходит для страниц «О нас», контактов, блогов, документации — всего контента, который не меняется слишком часто. Server-Side Rendering (SSR). Тактика Рафаэля: горячая пицца с доставкой индивидуально для каждого клиента. Возвращаемся к пиццам. Пока Донателло возился со своими заготовками и холодильниками, Рафаэль лишь хрипло усмехнулся. Он убежден, что секрет коммерческого успеха пиццерии в индивидуальном подходе, и что настоящая пицца должна быть с пылу, с жару.  «Мы будем делать всё сами! От теста для пиццы, до выращивания сочных и спелых томатов в каморке учителя Сплинтера. Никаких заготовок, каждый заказ — это новая битва, где каждый клиент заслуживает пиццу, приготовленную специально для него,» –  сказал Рафаэль и отправился осуществлять задуманное .\\xa0 Вот что происходит на нашей  Next.js  «кухне», когда мы выбираем стратегию SSR. В отличие от SSG, серверный рендеринг фокусируется на странице и создаёт её в режиме реального времени. В этом случае, когда вы посещаете веб-сайт, сервер, на котором он размещен, создаёт страницу для вас, подобно тому, как Рафаэль готовит вкуснейшую пепперони по вашему индивидуальному заказу. \\xa0 Как это работает: Браузер отправляет запрос на сервер для получения HTML-страницы Сервер, получив запрос, немедленно приступает к рендерингу, объединяет все части (данные и HTML-разметку) в один полностью готовый HTML-документ Сервер отправляет готовый HTML-документ обратно в браузер пользователя Браузер получает страницу в ее окончательном виде и немедленно отображает её пользователю. Это обеспечивает быструю первоначальную загрузку. После отображения HTML страница обычно \"оживает\": загружаются и выполняются JavaScript-файлы, чтобы сделать страницу интерактивной. Этот процесс называется  гидратацией . Самый простой способ активации  SSR  — это отказаться от использования  generateStaticParams  в прошлом примере, и тогда страница автоматически будет генерироваться как динамическая ( server-rendered on demand ) . Помимо этого мы можем добиться такого поведения передав в  fetch  дополнительные опции, а именно  cache: “no-store” . Давайте создадим страницу, на которой будем отображать список всех черепашек и их любимых пицц: import Link from \"next/link\";\\n\\n\\n\\n\\nexport default async function TurtlesPage() {\\n\\n\\xa0\\xa0const res = await fetch(\"https://www.turtletime.dev/api/v1//turtles\",\\n\\n\\xa0\\xa0{\\n\\n\\xa0\\xa0\\xa0\\xa0cache: \"no-store\",\\n\\n\\xa0\\xa0});\\n\\n\\xa0\\xa0const turtles = await res.json();\\n\\n\\n\\n\\n\\xa0\\xa0return (\\n\\n\\xa0\\xa0\\xa0\\xa0<div>\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<h1>Turtles</h1>\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<ul>\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0{turtles.map((turtle, index) => (\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<li key={index}>\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<Link href={/turtles/${turtle.name}}>\\n\\n              {turtle.name} - {turtle.favorite_pizza}\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0</Link>\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0</li>\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0))}\\n\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0</ul>\\n\\n\\xa0\\xa0\\xa0\\xa0</div>\\n\\n\\xa0\\xa0);\\n\\n} Стоит также отметить, что добиться такого состояния можно и через использование в компоненте  export const dynamic = \\'force-dynamic\\'  (это эквивалентно установке для каждого запроса на странице параметра  {cache: \\'no-store\\', next: { revalidate: 0 } } )\\xa0 или следующих API:\\xa0 cookies;\\xa0 headers;\\xa0 connection;\\xa0 draftMode;\\xa0 searchParams prop; unstable_noStore. Запустив  next build ,  мы видим, что страница стала динамической. Об этом сигнализирует символ  ƒ — \\xa0  (Dynamic)\\xa0 server-rendered on demand . Плюсы SSR подхода: SEO-дружелюбность . Также как и при SSG мы получаем готовый HTML, который индексируется поисковиками Динамический контент.  SSR позволяет генерировать контент в режиме реального времени, обеспечивая мгновенные обновления и персонализированный опыт. Не требователен к устройству клиента.  Весь процесс преобразования кода и данных в HTML (рендеринг) происходит на сервере в момент запроса. Минусы подхода:\\xa0 Увеличенная нагрузка на сервер . SSR может создавать дополнительную нагрузку на серверные ресурсы, особенно при большом количестве пользовательских запросов, что может повлиять на\\xa0 производительность. Клиент ждет , пока страницу «приготовят». Даже если это произойдет очень быстро, клиенту придется провести какое-то время в ожидании (более высокий Time to First Byte по сравнению с SSG) SSR очень хорошо подходит для товаров интернет-магазина. Так как для товаров важно SEO, а самих товаров может быть очень много, генерировать их все заранее будет ресурсозатратно. Incremental Static Regeneration (ISR). Изобретение Микеланджело. Говоря про основные стратегии рендеринга сайта стоит также упомянуть про  ISR  подход. И пока Донателло и Рафаэль спорили, чья стратегия лучше, Микеланджело, как большой любитель поесть и придумать что-то крутое, нашел гениальное решение:  «Чуваки! А что, если мы будем хранить готовую пиццу на специальной подложке, которая по необходимости будет незаметно для клиента подогревать пиццу, как только она начнет остывать?» Также как и в  SSG  мы заранее «заготавливаем» наши страницы в рамках  ISR  подхода. И тут может возникнуть вопрос, а как долго мы можем хранить нашу заготовленную пиццу? Revalidate  — директива, которую вы задаете в коде ( например, revalidate: 3600 ). Это указание для вашего сервера  Next.js , как часто нужно проверять актуальность данных и пересобирать эту конкретную страницу (подогревать пиццу). Давайте добавим эту строчку кода в наш пример с  SSG . // Next.js будет аннулировать кэш при поступлении\\n\\n// запроса, не чаще одного раза в час.\\n\\nexport const revalidate = 3600; Как это работает:\\xa0 когда пользователь запрашивает страницу,  Next.js  смотрит на эту директиву.\\xa0 если с момента последней “выпечки” прошло меньше секунд чем указано в  revalidate  — пользователь мгновенно получает страницу из кеша.\\xa0 если срок истек —  Next.js  точно так же мгновенно отдает пользователю последнюю «испеченную» версию (пусть и условно “просроченную”), но параллельно запускает фоновый процесс, чтобы “подогреть” новую, свежую страницу с актуальными данными. Следующий пользователь получит уже её. Добавляя  revalidate  мы можем автоматически обновлять страницы по таймеру —\\xa0  Time-based revalidation . Помимо этого мы также можем обновлять кэш по запросу —  On-Demand Revalidation .  \\xa0 On-Demand Revalidation   —   это наша секретная кнопка «Подогрев пиццы». Например, вы через админку добавляете новый пост в блог. Админка тут же «нажимает» на эту кнопку, отправляя специальный сигнал на наш сервер. Сигнал мгновенно помечает конкретную страницу как «устаревшую». Теперь её «пересоберут» при самом первом следующем запросе, даже если по  revalidate  таймеру ее »срок годности» еще не вышел. Есть два способа сбросить кэш: по пути страницы и по тэгу ( revalidatePath  и  revalidateTag ). Для этого создадим две папки с роутами: В папке  revalidation-on-demand   создадим страницу в которой будем отображать время «приготовления» нашей страницы:\\xa0 export default async function RevalidationOnDemand() {\\n  return (\\n    <>\\n      <p> Эта страница сгенерирована в:\\n \\t\\t {new Date().toLocaleTimeString()}\\n  </p>\\n    </>\\n  );\\n} Также добавим API роут который будет ревалидировать(сбрасывать) кэш при запросе: import { revalidatePath } from \"next/cache\";\\nimport { NextResponse } from \"next/server\";\\n\\n\\nexport async function GET() {\\n  revalidatePath(`/revalidation-on-demand`);\\n  return NextResponse.json({\\nrevalidated: true,\\nnow: Date.now()\\n  });\\n}\\n Запускаем  next build  →  next start  → открываем роут  /revalidation-on-demand   для загрузки даты первый раз (если вы попробуете обновить страницу, то время останется таким же) → чтобы обновить кэш по пути ( revalidatePath ) переходим на роут  /api/revalidate  (это кнопка «Подогрев пиццы») → после этого следующее обновление страницы  /revalidation-on-demand  покажет свежую дату. Преимущественно  revalidatePath   подходит, когда вам нужно обновить одну конкретную страницу, а  revalidateTag   используем, когда одни и те же данные используются на нескольких страницах, или вы не знаете все возможные пути, которые зависят от измененных данных Ещё один наглядный пример того, как работает  On-Demand Revalidation   можно посмотреть  здесь  (просто создайте новое issue или комментарий в репозитории и перезагрузите страницу). Такой подход идеально подходит для страниц, которым важно и SEO, и предварительная генерация, и частое обновление контента. Например, это могут быть контентные-сайты (блоги, новостные порталы), где мы можем заранее сгенерировать все статьи (SSG), а при публикации новой (например, через админку) или правке существующей статьи — мгновенно инвалидировать кеш именно для этой страницы, не пересобирая весь сайт.\\xa0 Как сделать выбор: Практическое руководство по выбору между SSR и SSG Спор между Донателло (SSG) и Рафаэлем (SSR) кажется вечным. Но как мудрый учитель, мастер Сплинтер знает: не существует плохой или хорошей стратегии — есть уместная. Правильный выбор зависит от конкретных требований вашего проекта. Чтобы принять взвешенное решение, задайте себе следующие вопросы:\\xa0 1. Как часто обновляется контент? Выбирайте SSG с ревалидацией (ISR), если контент на сайте обновляется относительно редко (например, раз в день, неделю или реже). Идеально для блогов, маркетинговых сайтов и каталогов продуктов, где информация статична после публикации. Выбирайте SSR для страниц, которые невозможно подготовить заранее из-за их количества или персонализации. 2. Требуются ли персонализированные данные для каждого пользователя?\\xa0 Выбирайте SSG, если содержание страницы одинаково для всех посетителей (например, статья, страница «О нас», контакты).\\xa0 Выбирайте SSR, если вам необходимо показывать уникальный контент в зависимости от пользователя (его геолокации, данных в cookie, статуса авторизации).\\xa0 3. Откуда и какие данные вам нужны? Выбирайте SSG, если данные для страницы можно получить во время сборки проекта из CMS, файловой системы или стороннего API. Выбирайте SSR, если для рендеринга страницы необходимы данные из самого запроса (например, cookies, заголовки, параметры URL), которые невозможно получить на этапе сборки. Заключение. Ваша пиццерия готова к приёму гостей. Итак, наша пиццерия  Next.js  готова к любым вызовам, а команда шеф-поваров прошла полный курс подготовки. Мы усвоили главные уроки на примере наших героев-ниндзя: Стратегия Донателло (SSG) — используем её для всего, что можно «заготовить» заранее и получаем склад пицц (страниц) с быстрой доставкой до конечного клиента.\\xa0 Тактика Рафаэля (SSR) —\\xa0 мы используем этот подход, когда каждый заказ должен быть уникальным и собран из самых свежих, только что полученных ингредиентов (данных), никаких полуфабрикатов. Страница будет «приготовлена» специально для вас в реальном времени, как только вы сделаете запрос. Изобретение Микеланджело (ISR) — это своего рода золотая середина. Этот подход позволяет сочетать скорость статики (SSG) и актуальность динамического SSR рендеринга. А как же Леонардо с его легендарным CSR подходом, спросите вы? Next.js предлагает использовать его избирательно, комбинируя с остальными подходами для улучшения производительности, SEO и пользовательского опыта. Как итог, не существует единственно правильного выбора. Есть тот инструмент, который идеально подходит для конкретной задачи на вашем сайте. Ведь как говаривал учитель Сплинтер: «Начинай с SSG. Действуй от простого к сложному. И подключай SSR только тогда, когда без персонального подхода действительно не обойтись». Пусть ваш сайт будет быстрым, как удар Рафаэля и надежным, как план Донателло! Готовьте свои проекты со вкусом, и пусть ваши пользователи всегда остаются сытыми и довольными.\\xa0 Cowabunga! 🍕 P.S. Если понравился такой формат, то оставляйте комментарии и вскоре разберём новую экспериментальную фичу NEXT.js — Partial Prerendering.', hub='javascript'), Page(id='946450', title='Сквозь эпохи: от хаоса к гармонии, или как мы запросы в Greenplum улучшали', content='Привет, Хабр! Я Илья Назаров, старший инженер в разработке сервисов направления эксплуатации инфраструктуры данных DataPlatform Т-Банка. В работе я часто соприкасаюсь с движками баз данных. Первым и основным движком волею судеб стал Greenplum. Расскажу о своем длинном пути взаимодействия с «Зеленой сливой», как из хаоса и невежества я дошел до истины и гармонии. В начале карьеры меня много чего удивляло. Тогда я еще не знал, что такое Greenplum,и плохо понимал, что такое MPP. Позднее коллеги на пальцах объяснили мне, что это «постгрес курильщика» и «постгрес поверх кучи постгресов».\\xa0 Не менее удивительны для меня процессы. Например, процесс деплоя. Именно тогда я узнал, что в большом продакшене может быть деплой через правку SSH-скриптов на серверах.\\xa0 В целом ситуация выглядела страшно интересно: скрипты, процессы деплоя и работы над задачами — все было в новинку. С одной стороны, большой багаж исторически сформированных до меня решений, с другой — большой уровень свободы и минимум ограничений, что как раз и способствовало постоянному росту энтропии и хаоса. Практически сразу я ощутил желание навести во всем порядок. А что из этого получилось — читайте в статье 😉 Эпоха хаоса и невежества История начинается со старта моей стажировки, когда я был молод, амбициозен, полон сил  и уверенности в своих возможностях изменить мир.\\xa0 Первыми полученными задачами были задачи по рефакторингу автоматизаций, оставшихся со времен, которые помнили не все старожилы, куда нужно было вносить те или иные незначительные изменения.\\xa0 Скрипты были написаны опытными инженерами и стабильно выполняли свои функции. Но за долгое время успели обрасти костылями, хотфиксами, бойлерплейтом и прочими последствиями горящих правок.\\xa0 Самих кластеров было относительно немного: три продуктовых — два больших под ETL  и один поменьше, но побыстрее   — плюс один тестовый кластер. Все они работали на Greenplum 5. Постепенно спектр задач расширялся и росла моя вовлеченность — я все больше любил свою работу. Я узнал, что в наши кластеры иногда попадают запросы, которые могут привести  к залипанию базы из-за обилия idle-сессий  и даже к ее падению из-за большого объема spill-файлов.\\xa0 Тогда существовали какие-то автоматизации, которые закрывали основной поток проблем, но и ручную работу тоже приходилось делать. Автоматизации были на баше, которые запускали следующий SQL-код: spill_monitor.sql\\nINSERT INTO spill_monitor (usename, procpid, sess_id, cur_q, total_gb, numfiles, start_time, cur_time, query_start)\\nSELECT  sizes.*,\\n        pgstact.backend_start,\\n        current_timestamp,\\n        pgstact.query_start\\nFROM ((SELECT\\tusename,\\n                procpid,\\n                sess_id,\\n                substring(current_query FROM 1 FOR 5000) \\tAS cur_q,\\n                floor(sum(size)/1024/1024/1024)    \\ttotal_gb,\\n                sum(numfiles) AS numfiles\\n        FROM\\tgp_toolkit.gp_workfile_usage_per_query\\n        WHERE   current_query <> \\'<IDLE>\\'\\n        GROUP BY 1, 2, 3, 4\\n        ORDER BY 5 DESC)) sizes,\\n        pg_stat_activity pgstact\\nWHERE   total_gb > 256\\n    AND   pgstact.procpid = sizes.procpid; long_sessions.sql\\nselect\\n       datname as db_name\\n       ,usename as user_name\\n       ,procpid as process_id\\n       ,sess_id as session_id\\n       ,case when current_query = \\'<IDLE>\\' then true else false end as session_is_idle\\n       ,xact_start as xact_start_dttm\\n       ,now() as check_dttm\\nfrom\\n       pg_stat_activity a\\n       inner join (select pid from pg_locks where locktype = \\'transactionid\\' group by pid) l on l.pid = a.procpid\\nwhere\\n       xact_start is not null\\n       and extract(\\'epoch\\' from (now() - xact_start)) > 300; И вот эта необходимость ручной реакции на возрастающее число событий (число пользователей с каждым днем росло, и за этим ростом не всегда поспевали железо, процессы и сервисы) особенным образом меня задела. Я уже наловчился определять, находить и завершать сессии, которые слишком надолго задержались в каком-либо из состояний: active, idle, idle in transaction. Завершали как через psql (pg_terminate_backend), так и через имевшийся у нас тогда GPCC.\\xa0 Я посвятил свободное время изучению устройства GPCC: как и откуда оно получает данные. Получилось собрать таблицы, которые могли бы мне пригодиться в автоматизации ограничения потребляемых запросами ресурсов. Список таблиц получился таким: pg_stat_activity как основной источник информации о запросах. Таблица стала основной при контроле idle, idle in transaction и запросов, если их длительность превышала некоторый предел (60 минут). gp_ toolkit.gp _workfile_entries — из нее мы брали данные по объему спилл-файлов на запросы. gpmetrics.gpcc_queries_now — для получения данных по cpu_time на запрос. Итогом работы с таблицами стал запрос, который получал основные данные по текущим активным сессиям. Этот запрос стал фундаментом для дальнейшего развития системы. SELECT\\n    qn.tmid,\\n    pga.procpid,\\n    pga.sess_id,\\n    pga.current_query,\\n    pga.waiting,\\n    qn.ccnt,\\n    pga.usename,\\n    pga.query_start,\\n    pga.backend_start,\\n    qn.db,\\n    qn.status,\\n    pga.rsgname,\\n    qn.rsqname,\\n    qn.rsqpriority,\\n    qn.tsubmit,\\n    qn.tstart,\\n    qn.cpu_time,\\n    qn.cpu_time * INTERVAL \\'1 second\\' AS cpu_time_hrs,\\n    qn.cpu_master,\\n    qn.cpu_segment,\\n    qn.cpu_master_percent,\\n    qn.cpu_segment_percent,\\n    pg_size_pretty(SUM(wpq.size)::bigint) AS spills_size_pretty,\\n    SUM(wpq.size) AS spill_size_bytes,\\n    wpq.numfiles,\\n    qn.memory,\\n    qn.disk_read_rate,\\n    qn.disk_write_rate,\\n    pg_size_pretty(qn.disk_read_bytes) AS disk_read,\\n    qn.disk_read_bytes,\\n    pg_size_pretty(qn.disk_write_bytes) AS disk_write,\\n    qn.disk_write_bytes,\\n    qn.skew_cpu,\\n    qn.plan_gen,\\n    qn.cost,\\n    pga.application_name\\nFROM\\n    pg_stat_activity pga\\n    LEFT JOIN gp_toolkit.gp_workfile_usage_per_query wpq\\n        ON pga.sess_id = wpq.sess_id\\n    LEFT JOIN ONLY gpcc_queries_now qn\\n        ON pga.sess_id = qn.ssid\\nWHERE\\n    pga.current_query <> \\'<IDLE>\\'::text\\nGROUP BY\\n    qn.tmid,\\n    pga.procpid,\\n    pga.sess_id,\\n    pga.current_query,\\n    pga.waiting,\\n    qn.ccnt,\\n    pga.usename,\\n    pga.query_start,\\n    pga.backend_start,\\n    qn.db,\\n    qn.status,\\n    pga.rsgname,\\n    qn.rsqname,\\n    qn.rsqpriority,\\n    qn.tsubmit,\\n    qn.tstart,\\n    qn.cpu_time,\\n    qn.cpu_master,\\n    qn.cpu_segment,\\n    qn.cpu_master_percent,\\n    qn.cpu_segment_percent,\\n    wpq.numfiles,\\n    qn.memory,\\n    qn.disk_read_rate,\\n    qn.disk_write_rate,\\n    qn.disk_read_bytes,\\n    qn.disk_write_bytes,\\n    qn.skew_cpu,\\n    qn.plan_gen,\\n    qn.cost,\\n    pga.application_name; Эпоха баланса и сомнений На тот момент мои знания SQL оставляли желать лучшего, зато я умел писать относительно хитрые скрипты на Питоне, чем и занялся.\\xa0 Спустя некоторое время у нас появилась автоматизация idle_ killer.py , которая умела вычислять и убивать запросы по определенным правилам, а еще оповещать об этом пользователей через корпоративный мессенджер.\\xa0 Вслед за улучшением технической части я подготовил базовую пользовательскую документацию, которая помогала понять, что именно произошло и как на это реагировать. \\xa0В коде idle_killer.py выглядел примерно так: # -*- coding: utf-8 -*-\\n\"\"\"Spill killer script.\"\"\"\\n \\ndef filter_sessions(sessions, min_spill_size, whitelisted_users):\\n    \"\"\"Kill queries heavy queries in greenplum.\"\"\"\\n    victim_sessions = filter_by_size(sessions, min_spill_size)\\n    victim_sessions = filter_by_users(victim_sessions, whitelisted_users)\\n    victim_sessions = filter_by_uuid(victim_sessions, ARGS.whitelisted_uuids)\\n    return victim_sessions\\n \\ndef main(parsed_args):\\n    \"\"\"Run main script function\"\"\"\\n    # target sizes on all user greenplums: hard = 800, soft = 600\\n    hard_spill_limit_gb = parsed_args.min_spill_size\\n    soft_spill_limit_gb = hard_spill_limit_gb - 200\\n \\n    whitelisted_users = tuple(parsed_args.whitelisted_users.replace(\\' \\', \\'\\').split(\\',\\')\\n \\n    with common.Greenplum() as greenplum:\\n        greenplum.connection.set_client_encoding(\\'WIN1251\\')\\n        sum_spills_resp = greenplum.execute_query(SUM_SPILLS_QUERY)\\n        sum_spills_gb = int(sum_spills_resp[0][0]) if sum_spills_resp else 0\\n        if is_overspilled(sum_spills_gb):\\n            LOGGER.warning(\"Too much spill files, %s GB\", sum_spills_gb)\\n \\n        spills_by_role_resp = greenplum.execute_query(SPILLS_BY_ROLE_QUERY)\\n        sessions = form_sessions_dicts(spills_by_role_resp)\\n        victim_sessions = filter_sessions(sessions, hard_spill_limit_gb, whitelisted_users)\\n \\n        LOGGER.info(\"Killing sessions %s\", victim_sessions)\\n        for sess in victim_sessions:\\n            query = \"SELECT pg_cancel_backend(%s);\" % sess[\\'pid\\']\\n            greenplum.execute_query(query)\\n \\n    notify_in_slack(victim_sessions, slack_users, hard_spill_limit_gb, penalty_channel)\\n \\n    dangerous_sessions = filter_sessions(sessions, soft_spill_limit_gb, whitelisted_users)\\n    dangerous_sessions = [sess for sess in dangerous_sessions if sess not in victim_sessions]\\n    notify_in_slack(dangerous_sessions, slack_users, soft_spill_limit_gb,\\n                    dwh_penalty_channel, reason=\"warn\")\\n \\n \\nif __name__ == \"__main__\":\\n    ARGS = PARSER.parse_args()\\n \\n    main(ARGS) Позднее скрипт idle_killer.py эволюционировал в следующую версию, которая управлялась конфиг-файлами. Эти файлы позволяли гибко настраивать список параметров:  идентификаторы запроса,  время активности правил, набор исключений и время жизни правил.\\xa0 Пример такого конфиг-файла: # чем выше находится правило, тем оно приоритетнее\\n# правила в блоке, который импортирует файл, приоритетнее правил из самого файла и перетирают их\\n \\n# переменные принадлежности, правило сработает, если каждое из них выполнено, по умолчанию имеют значения all:\\n# uuids — список sas/zep/heli айдишников\\n# hosts — хосты БД, для которых будут работать правила\\n# resource_groups — список рес-групп\\n# users — список реальных пользователей, в т.ч. из app_properties\\n# query_hash — список хэшей запросов, временно не используется\\n# rule_start_time и rule_end_time должны быть в кавычках, чтобы парсилось строкой,\\n# окно, в которое правило будет применяться, значения по умолчанию \"00:00:00\" и \"23:59:59\" соответственно\\n \\n# список лимитов, по умолчанию None\\n# expiration_date — дата, после которой правило становится недействительным\\n# limit_spills_gb — число, максимальный лимит объема спилл-файлов на запрос\\n# limit_cpu_time_hrs — лимит ЦПУ в часах\\n# limit_duration_hrs — лимит времени, работа запроса в часах\\n \\n \\n- hosts:\\n    all\\n  file:\\n    whitelist.yml\\n  limit_duration_hrs:\\n    inf\\n  limit_spills_gb:\\n    inf\\n  limit_cpu_time_hrs:\\n    inf\\n \\n- hosts:\\n    cluster_name\\n  users:\\n    all\\n  limit_duration_hrs:\\n    inf\\n  limit_spills_gb:\\n    inf\\n  limit_cpu_time_hrs:\\n    inf\\n  expiration_date:\\n    2022-02-23\\n \\n# increeased duration warning\\n- hosts:\\n    all\\n  users:\\n    all\\n  limit_duration_hrs:\\n    5\\n  limit_spills_gb:\\n    3000\\n  limit_cpu_time_hrs:\\n    48\\n  kill:\\n    fals Мы получили относительно гибкий инструмент автоматического отстрела пользовательских запросов, которые мешали нам жить. Скрипт крутился на кроне. Выглядело это примерно так. Верхнеуровневая схема деплоя и исполнения скрипта контроля пользовательских запросов Одного написания кода, конечно же, недостаточно. Его нужно запустить в продакшен-среду. Большого опыта запуска подобных вещей у моей команды не было, поэтому приняли волевое решение, что если уж мы убиваем запросы вручную, то автоматические убийства значимых различий не имеют. Это была ошибка.\\xa0 После запуска автоматики, несмотря на подготовленное предупреждение, запросы начали убиваться с ощутимыми для пользователей последствиями. Пользователи к такому никак не были готовы — ни в моральном плане, ни в отношении наличия ресурсов на массовое исправление запросов, которые раньше пропускались при ручной обработке. Мы отключили автоматику, собрали встречу с заинтересованными представителями и начали прорабатывать более контролируемый и дружественный к пользователям сценарий запуска. На основе обсуждения сформировали ключевые пункты: Пользователи должны ясно понимать правила игры. Повторно запускаться мы можем не раньше чем через месяц, так как нужно время на то, чтобы привести легаси-код к новым правилам игры. Пользователи должны получать доступные и понятные оповещения о том, какой из их запросов был отменен и по какой причине. Эпоха порядка и закона Когда пользователи заметили пользу для себя от подобного инструментария, все чаще стали звучать вопросы о возможности самостоятельно управлять ограничениями ресурсов на запрос. Особенно от лидов направлений, которые видят работу в большом масштабе. Мы сопоставили пользователей с бизнес-линиями и начали формировать виртуальную сущность. В разных кругах она носила разные названия: потребители, консьюмеры, тенанты. Но, по сути, почти всегда эти сущности имели прямую связь с ресурсной группой в базе.\\xa0 Так у тенантов начали появляться свои пары ответственных: со стороны платформы данных и со стороны бизнеса. Коллеги из бизнеса помогают с формулировкой и актуализацией текущих потребностей. Коллеги из платформы помогают с техническими вопросами, обозначают ограничения и нюансы самой платформы данных в контексте конкретных кейсов соответствующих линий. В техническом плане новым витком дальнейшего развития стало преобразование GitOps-скрипта в полноценный сервис со своим бэкэндом, rest-api, авторизацией, интеграциями и прочим.\\xa0 На этом этапе мы стали больше внимания уделять сбору и хранению данных о пользовательских запросах для последующего исторического анализа. Так у нас появился свой managed-инстанс ClickHouse, в который мы складываем реплики интересующих нас данных с помощью  Vector.  Выглядело это так: ---\\ngp__master__logs:\\n  name: gp_logs\\n  vars:\\n    sources:\\n      gp_log:\\n        type: file\\n        include:\\n          - \"{{ master_data_directory }}/pg_log/*.csv\"\\n        ignore_older_secs: 864000  # 1 day\\n        max_line_bytes: 204800  # default 102400\\n        data_dir: \"{{ vector.data_dir }}\"\\n        multiline:\\n          mode: halt_before\\n          start_pattern: \\'^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}\\\\.[0-9]{6} \\\\+[0-9]{2},\\'  # 2023-01-16 00:00:00.001577 +03,\\n          condition_pattern: \\'^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}\\\\.[0-9]{6} \\\\+[0-9]{2},\\'  # 2023-01-16 00:00:00.001577 +03,\\n          timeout_ms: 1000\\n        encoding:\\n          charset: \"cp1251\"\\n \\n    transforms:\\n      gp_log_trans_1:\\n        type: filter\\n        inputs:\\n        - gp_log\\n        condition:\\n          type: \"vrl\"\\n          source: false == to_bool(find!(.message, r\\'^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}\\\\.[0-9]{6} \\\\+[0-9]{2},\\'))\\n      gp_log_trans_2:\\n        inputs:\\n          - \"gp_log_trans_1\"\\n        type: \"remap\"\\n        source: |-\\n          parts = parse_csv!(.message,\",\")\\n          del(.message)\\n          del(.source_type)\\n          del(.msg_err)\\n          .inst = del(.host)\\n          .capture_ts = del(.timestamp)\\n          ...\\n          del(.dur_err)\\n \\n      gp_log_trans_ast:\\n        inputs:\\n        - gp_log_trans_2\\n        type: remap\\n        source: |-\\n          .original_sql = .debug_query_string\\n \\n    sinks:\\n      gp_log_to_ch:\\n        type: \"clickhouse\"\\n        table: \"gp5_logs\"\\n        inputs:\\n          - \"gp_log_trans_2\"\\n        encoding:\\n          timestamp_format: \"{{ clickhouse_config.encoding.timestamp_format }}\"\\n        auth:\\n          strategy: \"{{ clickhouse_config.auth.strategy }}\"\\n          user: \"{{ clickhouse_config.auth.user }}\"\\n          password: \"{{ clickhouse_config.auth.password }}\"\\n        database: \"{{ clickhouse_config.database }}\"\\n        endpoint: \"{{ clickhouse_config.endpoint }}\"\\n        compression: \"{{ clickhouse_config.compression }}\"\\n        batch:\\n          max_events: 102400\\n          timeout_secs: 10\\n      gp_log_to_ast:\\n        type: http\\n        inputs:\\n          - gp_log_trans_ast\\n        uri: \"{{ ast_endpoint }}\"\\nbatch:\\n          max_events: 102400\\n        buffer:\\n          when_full: \"drop_newest\"\\n        encoding:\\n          codec: json\\n        request:\\n          concurrency: 1\\n          rate_limit_num: 20 На основе данных о текущих и завершенных запросах и о потребляемых этими запросами ресурсах зародился новый сервис под названием Raskolnikov. В него мы перенесли знакомые нашим пользователям стандартные правила, немного подрезав их, и постепенно заменили работавшие в проде скрипты.\\xa0 Схема работы сервиса контроля запросов стала чуть сложнее На этом этапе произошло разделение правил на две группы: Группа общих правил, влиять на которые могут только админы сервиса (SRE-команды направления). Группа правил для тенантов, которыми управляют ответственные за тенанты. Очень скоро самые активные пользователи занялись настройкой более гибких лимитов, учитывающих специфику их нагрузок, приоритеты запросов и внутренний рейтинг пользователей. Примерно в тот же момент в платформе данных назрели значимые изменения: началась миграция нагрузки на Greenplum 6, что оттянуло значимый объем внимания и ресурсов от дальнейшего развития сервиса. В целом он работал стабильно, но периодически мы возвращались для мелких фиксов и обеспечения поддержки 6-й версии Greenplum. По ходу миграции мы адаптировали Raskolnikov для работы с обоими версиями Greenplum, написали сервис сбора данных о БД с помощью обработки хуков метрик и еще несколько небольших сервисов и утилит. Эпоха истины и гармонии Примерно с этого момента для меня началась эпоха истины и гармонии (по крайней мере мне хочется в это верить). У нас сформировалось более целостное продуктовое видение конечного результата, и мы начали устойчивое движение к его воплощению. Первым шагом стала визуализация схемы работы компонентов, это позволило упростить понимание архитектуры и последующий рефактор. Следующим шагом стал рефактор кода, в ходе которого устранили много неэффективностей, нашли и исправили баги и дублирования, улучшили обработку ошибок, логирование, метрики сервиса. Было до рефакторинга Стало после рефакторинга Всегда остается пространство для улучшения и оптимизации, но в текущей итерации мы достигли значимого прогресса. А если говорить на языке цифр, в настоящий момент в сервисе «Раскольников» мы имеем: 3 общих правила, на деле доказавших свою эффективность, одинаковые для всех кластеров и пользователей. 124 пользовательских правила, которые более жестко ограничивают запросы отдельных групп пользователей. 72 вайтлиста для этих правил. 16 продуктовых кластеров, для которых эти правила и вайтлисты действуют. Более 8 тысяч применения лимитов за 14 календарных дней. Более 80 тысяч принудительно закрытых за те же 14 дней IDLE-сессий. Еще не конец истории За это время я успел повидать многое.  Сменяли друг друга эпохи, менялся инфраструктурный ландшафт, достигались различные важные цели. Одно остается неизменным — стремление все более эффективно удовлетворять потребности пользователей платформы. Для любого инструмента важна не только чистая работоспособность и техническая эффективность. Необходимо работать над простотой и комфортом эксплуатации как со стороны администрирующей команды, так и со стороны рядового пользователя. Именно поэтому мы поставили перед собой амбициозную цель по формированию универсального, доступного и удобного инструмента управления движками данных.\\xa0 Одним из основных компонентов должен стать полноценный фронтенд. Мы уже начали работу над ним и планируем пройти несколько внутренних итераций. Это нужно, чтобы сформировать понятный и удобный для всех интерфейс, прежде чем переходить к дальнейшим усовершенствованиям. Я прошел довольно длинный и тернистый путь и успел поучаствовать во многих интересных событиях: побыть SRE, ориентированным на автоматизацию и разработку в инфраструктурной команде, поучаствовать в инцидентах, попробовать себя в анализе данных (совсем немного), работе с требованиями и сроками, немного побыть тимлидом команды разработки, менеджером небольшого продукта. И вот теперь я вернулся к истокам — в роль, с которой все начиналось.\\xa0 Наблюдая за прогрессом, которого мы достигаем, я часто ловлю себя на мысли о том, что решаемые нами проблемы не уникальны и, скорее всего, другие команды и компании сталкивались с похожими задачами. Эти мысли подтверждаются интересом со стороны комьюнити, которое следит за выступлениями нашей команды и задает вопросы о развитии продукта. А я считаю, что за все это время сумел совместить полезное с приятным. Работа в этом направлении принесла реальную пользу, и было реально круто активно участвовать в подобном проекте.\\xa0 Благодарю всех читателей, дошедших со мной до конца этого текста, и хочу пожелать каждому найти для себя драйвовые проекты, приносящие реальную пользу окружающим. По недавно сформировавшейся традиции здесь могла быть ссылка на телеграм-канал с моим бложиком, но ее не будет. Спасибо за внимание!', hub='devops'), Page(id='946292', title='Дивергенция как мера специфичности аудитории вашей пилотной механики', content='Хабр, привет! На связи Никита и Егор, мы работаем над продуктовой аналитикой в дирекции по развитию программы лояльности Х5. В статье мы бы хотели рассказать вам о том, как можно использовать модификацию дивергенции Кульбака-Лейблера для ответа на вопрос, а насколько ваша пилотная аудитория специфична относительно генеральной совокупности всех клиентов, и какие могут быть «подводные камни». Постановка проблемы Сегодня в эпоху цифровой трансформации A/B тестирование стало неотъемлемой частью стратегии принятия решений, превратившись из инновационного инструмента в стандарт индустрии. Напомню, что мы работаем в дирекции по развитию программы лояльности Х5, и зачастую проверяем гипотезы об увеличении наших бизнес-метрик с помощью балльных механик (например, «купи шоколад Х, получи Y баллов программы лояльности»). Тестировать дополнительные балльные механики на всем ассортименте магазина не есть хорошо для бизнеса, поэтому иногда мы сталкиваемся с проблемой выбора пилотных механик.\\xa0 Давайте раскроем проблематику: допустим, мы получили согласование от бизнеса-владельца программы лояльности, что готовы провести 1 эксперимент из следующего пула механик: Дополнительный кэшбэк X% на покупку фруктов и овощей Дополнительный кэшбэк X% на швейцарский шоколад Дополнительный кэшбэк X% на хлебобулочные изделия собственного производства Далее нам необходимо принять решение, а какую именно механику выбрать. Интуитивно кажется, что вторая механика может попасть в чересчур специфичную аудиторию, так как она направлена на узкий пул товаров «премиум» сегмента. Мы же в свою очередь хотим проводить эксперименты, которые за счет точечного воздействия могут повысить клиентскую лояльность в целом.\\xa0 Поэтому мы хотим оценить, а насколько текущая аудитория предложенных пилотируемых товарных позиций схожа со всей аудиторией доступных вам клиентов.  Получив три оценки схожести аудиторий с генеральной совокупностью, мы отсеем две наиболее специфичные механики и оставим только одну наиболее релевантную для всей совокупности клиентов. За счет отбора самой релевантной механики мы ожидаем получить самый масштабируемый эффект, если он будет задетектирован. Описание подхода\\xa0 Метод оценки основывается на знаниях распределений между генеральной совокупностью клиентов и потенциальной пилотной группой. Сформулируем задачу: насколько схоже распределение нашей потенциальной пилотной группы с распределением генеральной совокупности? Для ответа на этот вопрос нужна метрика.\\xa0 Осталось подобрать такую метрику, и эта метрика должна быть метрикой информации, которая будет обладать следующими свойствами: Метрика должна иметь симметричность при сравнении распределений. Метрика должна обладать интерпретируемыми границами. Метрика должна быть устойчивой при работе с нулевыми наблюдениями в распределении. Метрика не должна быть чувствительной к различиям по мерам центральной тенденции, а только нести информацию о нерепрезентативности. Тогда, если две группы максимально отличаются друг от друга по такой метрике, мы сможем ответить, насколько пилотные группы удовлетворяют репрезентативности и насколько вообще можно экстраполировать эффект, найденный для таких специфичных групп. Таким образом, при минимальном значении метрики (например, близком к нулю) можно будет сделать вывод, что аудитория максимально похожа на генеральную совокупность. А если такая метрика даст максимальное значение равной 1, то мы получим, что аудитория максимально отличается от совокупности в целом.\\xa0 Хорошая новость, такая метрика есть – и это JS-дивергенция (Jensen-Shannon divergence). Определение дивергенции\\xa0 Формально JS-дивергенция является модернизацией метода KL-дивергенции (Kullback–Leibler divergence). Дивергенция KL измеряет разницу между двумя распределениями вероятностей. Она также известна как относительная энтропия. Строгое определение такое: KL дивергенция измеряет количество дополнительных битов, необходимых для кодирования выборок из одного распределения при использовании кода на основе другого распределения. Другими словами, это объём информации, потерянной, когда одно распределение используется для аппроксимации другого. При этом KL дивергенция обозначается как D(P||Q), где P и Q – сравниваемые распределения. При этом она обладает рядом следующих свойств:\\xa0 Нессиметрична,\\xa0\\xa0\\xa0 Не имеет верхней границы и вообще интерпретируемых границ. Покажем интуитивно, как работает KL дивергенция. Представим, что мы имеем две столбчатые диаграммы P и Q, в которых представлены по оси Y вероятности какого-то события А, В и С (рис. 1). Допустим, со временем изменились вероятности наступления каждого из событий.\\xa0 Рис.1 Столбчатая диаграмма вероятностей в разные временные периоды\\xa0 Попробуем оценить изменение размера бинов. В качестве первой метрики для исследования возьмем отношение бинов. Тогда для события А получится А    , для зеленого бина В  для синего А .\\xa0 Если взять среднее арифметическое всех трех значений, получим  . Такая метрика обладает большим недостатком: на среднее арифметическое сильно влияет самое большое значение отношений бинов, а также важно, от какого распределения бины стоят в числителе, а какие в знаменателе – от этого конечный результат конечной метрики будет другим, отсюда вытекает несимметричность метрики. Для устранения этого недостатка сильного влияния больших чисел на среднее каждую дробь оборачивают в функцию логарифма. Формально KL дивергенция считает отношение бинов в степени логарифма, который дополнительно умножается на вес каждого бина в распределениях.\\xa0 Тогда итоговая формула для KL дивергенции: Для дискретной записи: Для непрерывных величин: Если мы сравнивали бы не P c Q, а Q с P: Отличие JS дивергенции от KL дивергенции в том, что эта метрика сравнивает каждое из распределений P и Q не друг с другом, а со средневзвешенным распределением вероятности каждого из P и Q распределений: Тогда итоговая формула для JS дивергенции: Из последней формулы видно, что метрика становится симметричной. Математически можно показать, что максимальное значение такой метрики при основании логарифма 2 будет равно 1, а минимальное равно 0. Действительно, если мы имеем идентичные распределения, то P=Q, тогда: Теперь определим максимальное значение. Так как  и  , то: Так как имеем две аналогичные половинки в формуле JS-дивергенции, то максимальное значение JS дивергенции не будет превышать 1, когда  . Таким образом, мы получили следующее свойство метрики: JS дивергенция симметрична при сравнении распределений. Метрика обладает интерпретируемыми границами [0,1]. Если JS дивергенция JS = 1, то распределения максимально отличаются между собой, и пилотная группа максимально специфична относительно генеральной совокупности. Если JS = 0, то мы имеем дело с идентичными распределениями. Кроме этого, можно заметить, что JS дивергенция отлично справляется с нулевыми значениями. \\tЭто была теория, а теперь покажем на симуляция, как эта метрика реагирует на разнотипные отличия, и продемонстрируем код для использования.\\xa0 Практическая симуляция JS дивергенции Для практической симуляции будем вычислять значения KL и JS для двух распределений в зависимости от разного типа кейса: А/А тесты и А/Б тесты с увеличением дисперсии, среднего и др. Рекомендуемое дискретное значение количества бинов для исследования различий распределений:\\xa0 Код для вычисления KL дивергенции: import numpy as np\\nfrom scipy.stats import entropy\\n\\ndef KL(data1, data2):\\n    \"\"\"\\n    Расчет KL-дивергенции между двумя наборами данных.\\n    \\n    Parameters:\\n    - data1: Первый набор данных.\\n    - data2: Второй набор данных.\\n    \\n    Returns:\\n    - kl_divergence_data1_data2: KL-дивергенция от data1 к data2.\\n    - kl_divergence_data2_data1: KL-дивергенция от data2 к data1.\\n    \"\"\"\\n    \\n    # Определение количества бинов согласно рекомендации \\n    bins = max(len(data1), len(data2))\\n    bins = int(bins ** (2 / 3))\\n    \\n    # Вычисление гистограмм\\n    hist1, bin_edges = np.histogram(data1, bins=bins, density=True)\\n    hist2, _ = np.histogram(data2, bins=bin_edges, density=True)  # Используем одинаковые интервалы\\n    \\n    # Нормализация гистограмм\\n    p = hist1 / hist1.sum()\\n    q = hist2 / hist2.sum()\\n    \\n    # Добавляем небольшое значение для избежания деления на ноль\\n    p = np.clip(p, 1e-10, None)\\n    q = np.clip(q, 1e-10, None)\\n    \\n    # Расчет KL-дивергенции вручную\\n    kl_divergence_data1_data2 = round(np.sum(p * np.log(p / q)), 2)\\n    kl_divergence_data2_data1 = round(np.sum(q * np.log(q / p)), 2)\\n    \\n    return kl_divergence_data1_data2, kl_divergence_data2_data1\\n Код для вычисления JS дивергенции: import numpy as np\\nfrom scipy.stats import entropy\\n\\ndef JS(data1, data2):\\n    \"\"\"\\n    Расчет дивергенции Йенсена — Шеннона (Jensen-Shannon divergence) между двумя наборами данных.\\n    \\n    Parameters:\\n    - data1: Первый набор данных.\\n    - data2: Второй набор данных.\\n    \\n    Returns:\\n    - js_divergence: Дивергенция Йенсена — Шеннона.\\n    \"\"\"\\n    \\n    # Определение числа бинов для гистограмм\\n    bins = max(len(data1), len(data2))\\n    bins = int(bins ** (2 / 3))\\n    \\n    # Создание гистограмм\\n    hist1, bin_edges = np.histogram(data1, bins=bins, density=True)\\n    hist2, _ = np.histogram(data2, bins=bin_edges, density=True)  # Используем одинаковые интервалы для бинов\\n    \\n    # Нормализация\\n    p = hist1 / hist1.sum()\\n    q = hist2 / hist2.sum()\\n    \\n    # Добавляем небольшое значение для избежания деления на ноль\\n    p = np.clip(p, 1e-10, None)\\n    q = np.clip(q, 1e-10, None)\\n    \\n    # Расчет среднего распределения\\n    m = 0.5 * (p + q)\\n    \\n    # Используем формулу для расчета дивергенции Йенсена — Шеннона\\n    js_divergence = 0.5 * (entropy(p, m, base=2) + entropy(q, m, base=2))\\n    \\n    # Округление результата до двух знаков после запятой\\n    js_divergence = round(js_divergence, 2)\\n\\n    return js_divergence\\n\\n# Пример использования\\ndata1 = np.random.normal(0, 1, 1000)\\ndata2 = np.random.normal(1, 1, 1000)\\n\\njs_divergence = JS(data1, data2)\\nprint(f\\'JS divergence: {js_divergence}\\')\\n Код для отрисовки графиков def plot_divergences(klab_list, klba_list, js_list):\\n    \"\"\"\\n    Функция строит три scatter-графика для списков дивергенций:\\n    - klab_list: KL-дивергенция A->B\\n    - klba_list: KL-дивергенция B->A\\n    - js_list: Jensen-Shannon дивергенция\\n    \\n    Добавляет горизонтальные линии 50-го и 95-го квантилей.\\n    \"\"\"\\n    # Вычисление квантилей\\n    klab_q95 = np.quantile(klab_list, 0.95)\\n    klba_q95 = np.quantile(klba_list, 0.95)\\n    js_q95 = np.quantile(js_list, 0.95)\\n\\n    klab_q50 = np.quantile(klab_list, 0.5)\\n    klba_q50 = np.quantile(klba_list, 0.5)\\n    js_q50 = np.quantile(js_list, 0.5)\\n\\n    n_points = len(klab_list)\\n    x_vals = range(n_points)\\n\\n    plt.figure(figsize=(10, 7))\\n\\n    plt.subplot(3, 1, 1)\\n    sns.scatterplot(x=x_vals, y=klab_list, label=\\'kl_ab\\')\\n    plt.title(\\'KL-divergence (A to B)\\')\\n    plt.axhline(klab_q95, color=\\'r\\', linestyle=\\'--\\', label=\\'95th Quantile\\')\\n    plt.axhline(klab_q50, color=\\'b\\', linestyle=\\'--\\', label=\\'50th Quantile\\')\\n    plt.legend()\\n\\n    plt.subplot(3, 1, 2)\\n    sns.scatterplot(x=x_vals, y=klba_list, label=\\'kl_ba\\')\\n    plt.title(\\'KL-divergence (B to A)\\')\\n    plt.axhline(klba_q95, color=\\'r\\', linestyle=\\'--\\', label=\\'95th Quantile\\')\\n    plt.axhline(klba_q50, color=\\'b\\', linestyle=\\'--\\', label=\\'50th Quantile\\')\\n    plt.legend()\\n\\n    plt.subplot(3, 1, 3)\\n    sns.scatterplot(x=x_vals, y=js_list, label=\\'JS-divergence\\')\\n    plt.title(\\'Jensen-Shannon Divergence\\')\\n    plt.axhline(js_q95, color=\\'r\\', linestyle=\\'--\\', label=\\'95th Quantile\\')\\n    plt.axhline(js_q50, color=\\'b\\', linestyle=\\'--\\', label=\\'50th Quantile\\')\\n    plt.legend()\\n\\n    plt.tight_layout()\\n    plt.show()\\n А/А тесты\\xa0 В качестве А/А теста проведем измерения на двух нормальных распределениях с средним 0, дисперсией 1, количеством наблюдений 2000. Зафиксируем одно из распределений, а второе нормальное распределение всегда будет генерироваться новое, но с теми же параметрами среднего и дисперсии (анимация 1). Сделаем 10 000 таких генераций и на каждом этапе будем считать три метрики: KL дивергенцию (напомним, что метрика несимметричная относительно сравниваемых распределений) и JS дивергенцию. Результаты представлены на рисунке 2. Анимация 1. Нормальное распределение. А/А тесты Код для отрисовки значений дивергенции # Генерация данных\\na = np.random.normal(0, 1, 2000)\\n\\nklab_list = []\\nklba_list = []\\njs_list = []\\n\\n# Вычисление KL и JS дивергенций для 10000 итераций\\nfor k in tqdm(range(10000)):\\n    b = np.random.normal(0, 1, 2000)\\n    kl_ab, kl_ba = KL(a, b)\\n    js = JS(a, b)\\n    klab_list.append(kl_ab)\\n    klba_list.append(kl_ba)\\n    js_list.append(js)\\n\\n# Отрисовка графиков\\nplot_divergences(klab_list, klba_list, js_list)\\n Рисунок 2. Метрики дивергенции на А/А тестах По значениям видно, что 95 перцентиль всех значений JS дивергенции имеет значение 0,1. KL дивергенции – около 0,95 и 0,4 при сравнении распределений. Значения не изменяются, что позволяет сделать вывод о том, что метрика на АА тестах дает одинаковые значения. В идеальном случае мы бы хотели получить значение метрики, близкое к нулю, но анимация выше как раз показывает, что генерируемые распределения не всегда обладают полной площадью перекрытия, и предложенная метрика на это реагирует. А/B тесты. Увеличение среднего.\\xa0 Посмотрим, как реагируют метрики на изменение средних значений, когда купола начинают разъезжаться друг от друга (анимация 2), при этом дисперсия будет одинаковой. Для такого теста по мере увеличения расчета разница средних будет увеличиваться. Результаты расчета метрик представлены на рисунке 3.\\xa0 Анимация 2. А/B тесты, изменение среднего Код для отрисовки значений A/B теста по увеличению среднего # Генерация данных\\na = np.random.normal(0, 1, 2000)\\n\\nklab_list = []\\nklba_list = []\\njs_list = []\\n\\n# Вычисление KL и JS дивергенций для 10 000 итераций\\nfor k in tqdm(range(10000)):\\n    b = np.random.normal(0 + k / 1000, 1, 2000)\\n    \\n    kl_ab, kl_ba = KL(a, b)\\n    js = JS(a, b)\\n    \\n    klab_list.append(kl_ab)\\n    klba_list.append(kl_ba)\\n    js_list.append(js)\\n\\n# Отрисовка графиков\\nplot_divergences(klab_list, klba_list, js_list)\\n Рисунок 3. Метрики дивергенции на А/В тестах, изменение среднего Видно, что 95-й перцентиль для JS дивергенции при увеличивающейся разнице не меняется, остается на уровне 0,1. KL дивергенция остается на уровне 0,5 и 0,6 соответственно. Таким образом, мы детектируем первый подводный камень метрики – она не чувствительна к изменениям центральных мер тенденции между распределениями. A/B тесты. Влияние выбросов. Посмотрим, как реагируют метрики на выбросы. По мере увеличения итерации расчета будем добавлять в выборку выбросы в левую и правую часть, от чего хвосты одного распределения будут становиться тяжелее, чем исходное (анимация 4). Сделаем 10 тыс. итераций, в конце расчета будет 100 выбросов в группе из 2000 наблюдений. Результаты расчета метрик представлены на рисунке 4.\\xa0 Анимация 3. А/B тесты, добавление выбросов Код для отрисовки значений A/B теста по влиянию выбросов # Генерация данных\\na = np.random.normal(0, 1, 2000)\\n\\nklab_list = []\\nklba_list = []\\njs_list = []\\nfor k in tqdm(range(10000)):\\n\\n    b = np.random.normal(0, 1, 2000-int(k/100))\\n    \\n    b = np.append(b, np.random.normal(10, 1, int(k/200) ))  # Добавляем 5 выбросов в b\\n    b = np.append(b, np.random.normal(-10, 1, int(k/200) ))  # Добавляем 5 выбросов в b    \\n    \\n    kl_ab, kl_ba = KL(a, b)\\n    js = JS(a, b)\\n    klab_list.append(kl_ab)\\n    klba_list.append(kl_ba)\\n    js_list.append(js)\\n\\nplot_divergences(klab_list, klba_list, js_list)\\n Рисунок 4. Метрики дивергенции на А/В тестах, добавление выбросов Как можно видеть из результатов расчета, метрика дивергенции начинает «реагировать» на выбросы. На 200-ой итерации у нас добавляется по одному выбросу в левую и правую часть. Это происходит, в частности, потому, что начинает меняться сама форма распределения, добавляются тяжелые хвосты. А/B тесты. Сравнение двух разных форм распределений. Теперь сравним два абсолютно разных по форме распределения. Нормальное и эскпоненциальное. При этом параметры средних значений и дисперсии у экспоненциального распределения не будут отличаться. То есть проведем А/А тест на двух разных по форме распределений (анимация 4). Результаты расчета метрик представлены на рисунке 5.\\xa0 Анимация 4. А/B тесты, сравнение двух разных по форме распределений Код для отрисовки значений A/B теста при сравнении распределений  # Генерация нормального распределения\\na = np.random.normal(0, 1, 2000)\\n\\n# Списки для хранения значений\\nklab_list = []\\nklba_list = []\\njs_list = []\\n\\n# Основной цикл для вычисления KL-дивергенций и JS-дивергенции\\nfor k in tqdm(range(10000)):\\n    # Генерация экспоненциального распределения\\n    b = np.random.exponential(1, 2000)\\n    \\n    # Вычисление KL-дивергенций и JS-диверсии\\n    kl_ab, kl_ba = KL(a, b)  # Предполагается, что функция KL определена\\n    js = JS(a, b)            # Предполагается, что функция JS определена\\n    \\n    # Добавление значений в списки\\n    klab_list.append(kl_ab)\\n    klba_list.append(kl_ba)\\n    js_list.append(js)\\n\\n# Отрисовка графиков\\nplot_divergences(klab_list, klba_list, js_list)\\n Рисунок 5. Метрики дивергенции на А/В тестах, сравнение нормального и экспоненциального распределений При сравнении двух совершенно разных по форме распределений можно сразу увидеть, как отреагировала метрика JS дивергенции, 95-й перцентиль около 0,8. При этом 95-й перцентиль значений KL дивергенции находится на уровне 12 и 10 соответственно. Зная интерпретируемые границы JS дивергенции можно заключить, что значение 0,8 достаточно близко к 1, значит по этой метрике можно сделать вывод, что распределения абсолютно разные. В отличие от KL дивергенции, которая несимметрична, и не совсем понятны её границы.  Заключение Мы получили практические знания о метриках дивергенции: KL/JS дивергенция не чувствительна к изменениям центральных мер тенденции между распределениями. KL/JS дивергенция чувствительна к выбросам. KL/JS дивергенция детектирует разницу между разными классами распределений. JS дивергенция имеет строгие интерпретируемые границы и лежит в интервале [0,1]. Таким образом, с помощью подхода с расчетом JS-дивергенции можно оценивать специфичность ваших пилотных групп в диапазоне от нуля до единицы. Ну и напоследок раскроем имена  авторов  данного материала:  Никита Сумнительный ,  Егор Карнаух', hub='математика'), Page(id='946454', title='Как мы ускорили заливку данных в YDB в 40 раз', content='Привет! С вами Кабанов Олег — ведущий ML-инженер Flocktory. В этой статье расскажу об опыте внедрения YDB в качестве хранилища для ML Online Feature Store. А также о том, как нам удалось ускорить загрузку данных в 40 раз и убрать влияние на скорость чтения данных при обновлении. Предисловие Наша первоначальная задача была такой: реализовать сервис, возвращающий фичи текущего пользователя по id, latency < 80ms, объём данных ~1TB, возможность гибко масштабировать на чтение и объём хранилища. На конференции HighLoad++ мы подробно рассказали, почему решили выбрать в качестве хранилища YDB, а в этой статье хочется поделиться опытом оптимизации загрузки, так как это стало проблемой при внедрении. Структура данных выглядела следующим образом: CREATE TABLE features\\n(\\n    `customer_id` Int32 NOT NULL,\\n    `tag` Utf8 NOT NULL, -- название фичи: feature_a, feature_b...\\n    `valid_to` Datetime NOT NULL, -- данные должны пропадать при достижении valid_to\\n    INDEX idx_customer_id GLOBAL ON (customer_id), -- в запросе получаем все строки по customer_id\\n    PRIMARY KEY (`customer_id`, `tag`) -- дедупликация данных по двум полям\\n)\\nWITH (\\n    AUTO_PARTITIONING_BY_SIZE = ENABLED,\\n    AUTO_PARTITIONING_BY_LOAD = ENABLED,\\n    AUTO_PARTITIONING_PARTITION_SIZE_MB = 512, -- объём партиции при котором она будет разбита на несколько\\n    TTL = Interval(\"PT0S\") ON `valid_to`\\n); Запрос на получение данных упрощенно выглядел так: DECLARE $customer_id AS int64;\\nSELECT AGGREGATE_LIST(tc.tag) AS tags \\n    FROM features VIEW idx_customer_id as tc -- явно указываем индекс, иначе не будет использоваться\\n    WHERE tc.customer_id = $customer_id С появлением все новых источников данных для ML Feature Store объем обновлений рос, время ответа сервиса перестало укладываться в 80ms, а загрузка длилась часами, что было неприемлемо. Оптимизация 1 Количество партиций достигло 500, при загрузке данных требовались синхронизации между значительным количеством из них. Это вызывало рост издержек на CPU, latency росло, горизонтальное масштабирование не помогало. Мы попробовали снизить количество партиций сначала в два, потом в четыре раза. Вместо типичной загрузки в 1000 записей в секунду получили 4000! Нагрузка на CPU снизилась, но latency продолжал быть нестабильным. Оптимизация 2 Оптимизация записи в четыре раза вдохновила нас на анализ  query explain , и мы заметили, что скорость чтения с индексом  idx_customer_id  и без него практически не отличается, а данных на тот момент уже было около ~ 300GB. Оказалось, что SSTable хорошо ищет по первой части первичного ключа. Индекс занимал место, и мы решили удалить его. Также отказ от индекса должен был ускорить вставку. Запрос стал выглядеть так: DECLARE $customer_id AS int64;\\nSELECT AGGREGATE_LIST(tc.tag) AS tags \\n    FROM features as tc -- убрали индекс\\n    WHERE tc.customer_id = $customer_id Удаление индекса: ALTER TABLE features DROP INDEX idx_customer_id; Каково было наше удивление, когда скорость загрузки выросла до 12 000 записей в секунду! Но скорость чтения во время заливки продолжала показывать спайки на графиках, нарушая SLA. Оптимизация 3 Возникает вопрос: почему при загрузке дампов скорость записи достигает 50 000 строк в секунду, а в нашей базе данных при заметно меньшей скорости все шарды уже нагружены на 100% по CPU? В чем разница? Почему сокращение шардов так помогло? Мы пишем с помощью YDB Connector for Apache Spark, он пишет батчами по 500 строк, при этом в каждом batch запросе по ключу  customer_id  мы задеваем все шарды. Структура данных SSTable и шардирование в YDB навело на мысль, что лучше бы писать не во все шарды по 1-2 записи, а в одну все 500, а это проще всего сделать сортировкой данных перед вставкой. Придумали, реализовали и через час получили скорость записи 40 000 строк в секунду, при этом никакого влияния на скорость чтения, latency стал стабильно < 80ms на 99.9 перцентиле. Почему так? Дело в том, как данные уложены в партиции SSTable: image-20250909-075509.png Данные упорядочены по первичному ключу, и если вставляемые строки уже отсортированы, то при большом объёме вставки один батч будет попадать в одну или две партиции. В таком случае транзакция потребует синхронизацию только нескольких партиций, а не всех в таблице. А одной партиции принять 500 строк, да ещё и отсортированных, не составляет труда. Вот почему дампы, снятые с БД, так хорошо загружаются — они уже отсортированы (брались из сортированной таблицы). Выводы С тех пор, как YDB была внедрена в Flocktory и мы провели эти оптимизации, прошло уже больше года, скорости загрузки и чтения нам достаточно по сей день, и к проблеме производительности мы более не возвращались. Шаги, которые мы прошли, могут показаться очевидными, но удовольствия от этого мы получили не меньше! Знание внутренней работы БД очень помогает, даже при типичном паттерне применения.', hub='серверная оптимизация'), Page(id='946224', title='Swift на Android: создаём библиотеку', content='Привет, хабр! Поддержка Android в Swift 6 и  swift-android-sdk  от  finagolfin  это настоящий прорыв, который наконец-то позволил мне выпустить  JNIKit , который я разрабатывал для проекта  SwifDroid  ещё со времён Swift 5. Теперь мы можем просто  import Android , вместо того чтобы возиться с ручным импортом NDK header\\'ов, а сборка конечных бинарников теперь обеспечивается не отдельным тулчейном, а минималистичной SDK, которую в скором времени сделают официальной на Swift.org. Сегодня я хочу показать как написать ваш первый Swift код для Android. Это будет увлекательное приключение, так что налейте чашечку чая и давайте начнём. Что Вам Понадобится Docker VSCode  и  расширение Dev Containers Swift Stream IDE  расширение для VSCode Опционально, установленная Android Studio для тестирование библиотеки, которую мы разработаем, на реальном Android приложении. Операционная система не важна, главное, чтобы она поддерживала Docker и VSCode. Как только вы установили Docker, можете запускать VSCode. Первым делом нужно установить  расширение Dev Containers . Поиск расширения Dev Containers в VSCode Далее нужно установить  расширение Swift Stream IDE . Поиск расширения Swift Stream IDE в VSCode Создание Проекта В левом меню VSCode кликните на иконку Swift Stream (с птичкой). Кнопки старта нового проекта или открытия существующего и смело жмите  Start New Project Теперь нужно ввести название проекта вашей Swift библиотеки Ввод имени нового проекта Вы можете видеть, что по умолчанию новый проект будет создан в вашей домашней папке. Это можно изменить выбрав иную папку нажав на три точки справа. Теперь необходимо выбрать направление и тип проекта,  Android -> Library  в нашем случае. Выбор типа и направления нового проекта И нажимаем  Create Project . Теперь необходимо указать Java namespace вашей библиотеки. Обычно это доменное имя наоборот, к примеру  com.example.mylib Ввод Java namespace для нового проекта Так же необходимо указать минимальную версию Android SDK Выбор минимальной версии Android SDK Я бы рекомендовал выбрать  24  или  29 , в зависимости от ваших нужд. Жмите  Enter , чтобы перейти к выбору версии Android SDK для компиляции Выбор версии Android SDK для компиляции На сегодняшний день хорошим выбором будет  35 . Жмите  Enter  ещё раз, чтобы запустить процесс создания проекта. На этом моменте VSCode создаст папку проекта со всеми необходимыми файлами и начнёт скачивание Docker-образа с заранее подготовленным окружением для разработки под Android. Как только образ загружен, VSCode откроет новое окно для вашего проекта, которое уже будет работать непосредственно внутри dev-контейнера. При первом запуске контейнер постарается загрузить актуальный Swift тулчейн, Swift for Android SDK, Gradle, Android NDK, т Android SDK. Все эти сущности будут заботливо использоваться из кэша, который организован на Docker Volumes. То есть создание следующего проекта будет происходить за секунды. Первый запуск может занять продолжительное время в зависимости от скорости вашего интернета и мощности компьютера, так что наберитесь терпения и пока выпейте чаю. Логи контейнера с успешно установленными инструментами И теперь всё готово! Можно начинать писать код! Преамбула Что такое JNI? Java Native Interface (JNI) – это мост который позволяет нативному коду обращаться к Java Virtual Machine (JVM). Когда вы пишете Java код, то вы используете Android SDK. Но когда вы используете языки как Swift или C++, которые не компилируются в Java байткод, вам уже нужен Android NDK для коммуникации с Java через JNI. Используя JNI вы можете делать практически всё то же самое, что и с Java, но делать это без головной боли это реальный челлендж. Что такое JNIKit? JNIKit вступает в игру как раз, чтобы убрать головную боль. Чтобы быть продуктивным и чувствовать себя комфортно на нужен удобный Swift-слой, который обернёт все низкоуровневые JNI вызовы созданные для C, во что-то намного более элегантное. Это то, для чего JNIKit был создан. Проект Структура В сердце проекта конечно же Swift Package Manager. И ключевые зависимости:  JNIKit , и  AndroidLogging  со  swift-log . Ваш Swift код по умолчанию живёт в  Sources/<target_name>/Library.swift . Android библиотека (Gradle проект) находится в папке  Library . Эта папка будет автоматически сгенерирована после первого сборки Swift кода. Альтернативно, вы можете запустить её генерацию слева на панели расширения Swift Stream. Swift-код Всё начинается с метода  initialize   и он должен быть вызван один раз до вызова любых других нативных методов. Этот метод экспортируется в JNI при помощи директивы  @_cdecl . Имя метода очень критично, оно должно строго соответствовать JNI-паттерну Java_<package>_<class>_<method> package  это имя пакета содержащего класс, но вместо точек у него должны быть нижние подчеркивания class  это имя класса содержащего метод method  это наименование метода Аргументы метода так же следуют JNI-соглашению. Первые два обязательны и всегда передаются автоматически самим JNI: envPointer : Этот параметр никогда не меняется и представляет из себя указатель на JNI окружение, которое по факту является интерфейсом между нативным кодом и JVM. clazzRef  или  thizRef : Здесь вы получаете  clazzRef  если ваш Java-метод статичный (как в нашем случае, где наш метод внутри Kotlin  object ). Вы так же можете получить  thizRef  если это метод инициализированного объекта. В первом случае это будет референс на класс, во втором референс на объект. Все последующие аргументы представляют из себя параметры Java/Kotlin-метода. В нашем случае метод имеет один параметр:  caller . Мы передаём его из приложения в качестве контекста, чтобы, к примеру, взять из него class loader приложения (больше об этом далее). Если бы у нас был  thizRef , то он подошёл бы в качестве контекста и  caller  можно было бы уже не передавать. #if os(Android)\\n@_cdecl(\"Java_com_habr_swiftlib_myfirstandroidproject_SwiftInterface_initialize\")\\npublic func initialize(\\n    envPointer: UnsafeMutablePointer<JNIEnv?>,\\n    clazzRef: jobject,\\n    callerRef: jobject\\n) {\\n    // Активируем Android logger\\n    LoggingSystem.bootstrap(AndroidLogHandler.taggedBySource)\\n    // Инициализируем JVM\\n    let jvm = envPointer.jvm()\\n    JNIKit.shared.initialize(with: jvm)\\n    // ДАЛЕЕ: кэшируем class loader\\n    // ДАЛЕЕ: пример `toString`\\n    // ДАЛЕЕ: пример `Task`\\n}\\n#endif Тело метода показывает нам первичную конфигурацию  swift-log  с  Android Logger , которую нужно делать только один раз. Это даёт нам возможность использования логгера вывод которого будет доступен в LogCat. let logger = Logger(label: \"🐦\\u200d🔥 SWIFT\")\\nlogger.info(\"🚀 Привет, Хабр!\") Далее в теле метода мы инициализировали подключение к JVM. Это тоже делается один раз. На этом этапе наше приложение уже готово полноценно работать через JNI. Class Loader и Кэш С загрузкой классов можно легко попасть в ловушку  ClassNotFoundException  потому что по умолчанию JNI использует системный загрузчик, который ничего не знает о динамически загруженных классах или зависимостях вашего приложения. Решение? Нам нужен загрузчик из контекста приложения, который легко получить из абсолютно любого Java-объекта через  .getClass().getClassLoader() .  Лучшей практикой является создание глобальной ссылки на объект загрузчика классоов в методе  initialize  и сохранение его в JNICache, т.к. он остаётся валидным на всё время жизни приложения. Таким образом мы будем экономить JNI вызовы и получим прирост в скорости работы приложения. Вот как мы можем кэшировать загрузчик классов в методе  initialize : // Оборачиваем указатель на окружение\\nlet localEnv = JEnv(envPointer)\\n// Конвертируем локальный референс объекта в глобальный\\nlet callerBox = callerRef.box(localEnv)\\n// Defer-блок отработает для удаления локального референса\\ndefer {\\n    // Удаляем локальный референс на объект\\n    localEnv.deleteLocalRef(callerRef)\\n}\\n// Создаём `JObject` из глобального референса на объект\\nguard let callerObject = callerBox?.object() else { return }\\n// Кэшируем загрузчик классов из caller-объекта\\nif let classLoader = callerObject.getClassLoader(localEnv) {\\n    // Сохраняем его в специальный JNICache\\n    JNICache.shared.setClassLoader(classLoader)\\n    logger.info(\"🚀 class loader cached successfully\")\\n} Если бы был  thizRef , то можно было бы взять загрузчик классов из него Могу ли я использовать  toString() ? Конечно, это классический Java-метод и для него сделана обёртка в JNIKit, а вызвать его можно на любом объект вот так вот просто: logger.info(\"🚀 caller description: \\\\(someObject.toString())\") Использование окружения из другого потока JNIEnv  жестко привязан к потоку. Оно является тем самым мостиком который делает всю магию JNI, передавая вызовы в JVM. Если вы переключаете поток, например используя Task, вы должны подключиться к окружению текущего потока. JNIKit предоставляет простой метод для этого:  JEnv.current() . Task {\\n    // Access current environment in this thread\\n    guard let env = JEnv.current() else { return }\\n    logger.info(\"🚀 new env: \\\\(env)\")\\n    // Print JNI version into LogCat\\n    logger.info(\"🚀 jni version: \\\\(env.getVersionString())\")\\n} Как выглядит код на другой стороне Java public final class SwiftInterface {\\n    static {\\n        System.loadLibrary(\"MyFirstAndroidProject\");\\n    }\\n    private SwiftInterface() {}\\n    public static native void initialize(Object caller);\\n} Kotlin object SwiftInterface {\\n    init {\\n        System.loadLibrary(\"MyFirstAndroidProject\")\\n    }\\n    external fun initialize(caller: Any)\\n} Swift Stream IDE генерирует Kotlin-файлы для вас, поэтому все дальнейшие примеры будут на Kotlin. Сборка Swift-проекта Отлично, время сбилдить проект! На боковой панели переключайтесь на таб  Swift Stream  и нажимайте на  Project -> Build . Кнопка Build для сборки Swift-проекта При первой сборке необходимо будет выбрать схему  Debug  или  Release  во всплывающем окне. В дальнейшем её можно изменить нажав на соответствующую кнопку  Scheme  (на скриншоте выше). Выбор схемы сборки проекта В этот раз выберем  Debug  и начнётся процесс сборки. На боковой панели Swift Stream вы можете выбрать  Log Level , чтобы видеть больше или меньше деталей в процессе сборки: Normal Detailed  (по умолчанию) Verbose Unbearable  (когда вы хотите видеть абсолютно всё) По умолчанию на  Detailed  уровне во время сборки вы увидите следующий вывод в окне  Output :  🏗️ Started building debug\\n💁\\u200d♂️ it will try to build each phase\\n🔦 Resolving Swift dependencies for native\\n🔦 Resolved in 772ms\\n🔦 Resolving Swift dependencies for droid\\n🔦 Resolved in 2s918ms\\n🧱 Building `MyFirstAndroidProject` swift target for arm64-v8a\\n🧱 Built `MyFirstAndroidProject` swift target for `.droid` in 10s184ms\\n🧱 Building `MyFirstAndroidProject` swift target for armeabi-v7a\\n🧱 Built `MyFirstAndroidProject` swift target for `.droid` in 7s202ms\\n🧱 Building `MyFirstAndroidProject` swift target for x86_64\\n🧱 Built `MyFirstAndroidProject` swift target for `.droid` in 7s135ms\\n🧱 Preparing gradle wrapper\\n🧱 Prepared gradle wrapper in 1m50s\\n✅ Build Succeeded in 2m20s Как вы можете видеть, Swift даже \"на холодную\" скомпилировался довольно быстро, всего ~30 секунд на все три таргета (arm64-v8a, armeabi-v7a, and x86_64). Большую часть времени (1 мин. 50 сек.) занял первый запуск команды  gradle wrapper , но это делается лишь изредка. Отличная новость в том, что повторные запуски компиляции будут очень быстрыми, порядка 3 секунд на все три таргета! Это потому что уже всё в кэше. Команда сборки так же автоматически генерирует Gradle-файлы для проекта Java-библиотеки для вас. Их теперь можно найти в папке  Library . Java/Kotlin проект Исходный код Swift Stream генерирует начальный шаблонный код для вашей библиотеки, который вдальнейшем вы будете развивать и поддерживать самостоятельно. import android.util.Log\\n\\nobject SwiftInterface {\\n    init {\\n        System.loadLibrary(\"MyFirstAndroidProject\")\\n    }\\n\\n    external fun initialize(caller: Any)\\n\\n    external fun sendInt(number: Int)\\n    external fun sendIntArray(array: IntArray)\\n    external fun sendString(string: String)\\n    external fun sendDate(date: Date)\\n    external fun ping(): String\\n    external fun fetchAsyncData(): String\\n} Gradle-файлы Swift Stream не только генерирует Gradle-файлы, но и управляет ими в дальнейшем. Он также создаёт Gradle-таргет под каждый таргет из вашего  Package.swift  и следит, чтобы Gradle-файлы были синхронизированы со Swift проектом. В  Library/settings.gradle.kts , происходит управление списком таргетов внутри специальных тэгов: // managed by swiftstreamide: includes-begin\\ninclude(\":myfirstandroidproject\")\\n// managed by swiftstreamide: includes-end В каждом  Library/<target>/build.gradle.kts  файле, происходит автоматическое управление зависимостями необходимыми для корректной работы Swift кода в этом таргете.  runtime-libs:core  является обязательной зависимостью, а остальные  runtime-libs:  внутри специальных тегов могут меняться в зависимости от вашего Swift кода: implementation(\"com.github.swifdroid.runtime-libs:core:6.1.3\")\\n// managed by swiftstreamide: so-dependencies-begin\\nimplementation(\"com.github.swifdroid.runtime-libs:foundation:6.1.3\")\\nimplementation(\"com.github.swifdroid.runtime-libs:foundationessentials:6.1.3\")\\nimplementation(\"com.github.swifdroid.runtime-libs:i18n:6.1.3\")\\n// managed by swiftstreamide: so-dependencies-end По умолчанию, эти зависимости подтягиваются автоматически из SwiftDroid  runtime-libs  репозитория на JitPack, и репозиторий обновляется вместе с выходом новой версии Swift. Это означает, что вам не нужно возиться с ручным копированием .so файлов из Android SDK бандла! Если вам всё-таки нужно больше контроля, то вы можете включить ручной режим, но всё ещё без ручного копирования каждого файла. Swift Stream IDE использует конфигурационный файл ( .vscode/android-stream.json ) в котором вы можете переключить режим  soMode : \"soMode\": \"Packed\" \"Packed\"  выбран по умолчанию и он означает, что Gradle импортирует всё из JitPack. Вы можете переключиться на  \"PickedManually\"  и перечислить только нужные вам  .so  файлы: \"soMode\": \"PickedManually\",\\n\"schemes\": [\\n    {\\n        \"title\": \"MyFirstAndroidProject Debug\",\\n        \"soFiles\": [\\n            \"libandroid.so\",\\n            \"libc.so\",\\n            \"libm.so\"\\n        ]\\n    }\\n] В этом же конфигурационном файле вы можете изменить и другие настройки проекта: \"packageName\": \"com.habr.swiftlib\",\\n\"compileSDK\": 35,\\n\"minSDK\": 24,\\n\"javaVersion\": 11, А ещё здесь же вы можете передать кастомные аргументы для команды  swift build : \"schemes\": [\\n    {\\n        \"title\": \"MyFirstAndroidProject Debug\",\\n        \"swiftArgs\": []\\n    }\\n] Сборка Gradle проекта Наконец-то мы добрались до сборки Android-библиотеки в файл  .aar , для этого в боковой панели Swift Stream выберите  Java Library Project -> Assemble . Кнопка для сборки Gradle проекта Эта кнопка запустит либо команду  gradlew assembleDebug  или  gradlew assembleRelease , которая сформирует вашу библиотеку для локального использования или дальнейшей публикации. Добавим эту библиотеку в Android проект (локально) Это самая веселая часть, т.к. мы увидим работу свифтв в вашем реальном Android приложении! Откройте существующий или создайте новый проект в Android Studio. Как только вы в проекте, первым шагом нужно добавить JitPack в качестве репозитория. Найдите ваш  settings.gradle.kts  и убедитесь, что вы подключили JitPack: dependencyResolutionManagement {\\n    repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)\\n    repositories {\\n        google()\\n        maven { url = uri(\"https://jitpack.io\") } // <-- это оно\\n        mavenCentral()\\n    }\\n} Далее нужно прописать зависимости в  build.gradle.kts  модуля вашего приложения (это может быть  app/build.gradle.kts ). Здесь нужно подключить и ваш .aar файл и все необходимые свифту рантайм-зависимости: dependencies {\\n    implementation(files(\"libs/myfirstandroidproject-debug.aar\"))\\n    implementation(\"com.github.swifdroid.runtime-libs:core:6.1.3\")\\n    implementation(\"com.github.swifdroid.runtime-libs:foundation:6.1.3\")\\n    implementation(\"com.github.swifdroid.runtime-libs:foundationessentials:6.1.3\")\\n    implementation(\"com.github.swifdroid.runtime-libs:i18n:6.1.3\")\\n    // the rest of dependencies\\n} Важно : Вам необходимо вручную прописывать здесь  runtime-libs:  зависимости потому что Gradle не может получить их из  .aar  файла. Но такого не будет если вы будете распространять свою библиотеку через репозиторий. Получаем .AAR файл После сборки Gradle проекта (выше) у нас получился  .aar  файл библиотеки, который вы сможете найти по адресу: Library/myfirstandroidproject/build/outputs/aar/myfirstandroidproject-debug.aar Скопируйте этот файл. Далее в папке вашего Android приложения перейдите в папку модуля (например,  app/ ) и создайте там папку  libs  (она должна расположиться там же где лежит  build.gradle.kts . Вставьте ваш  .aar  файл в папку  libs . И начнется магия! 🚀 Отлично, у нас всё есть! Теперь в коде вашего проекта, где-нибудь в классе  Application  или в методе  onCreate  вашей  Activity , инициализируйте Swift: SwiftInterface.initialize(this) Синхронизируйте Gradle и запускайте ваш Android проект на устройстве или эмуляторе. Момент истины : откройте LogCat и отфильтруйте по слову \"SWIFT\". Вы должны увидеть это славное послание:  I  [🐦\\u200d🔥 SWIFT] 🚀 Привет, Хабр! Урра!!! Ваш Swift код запустился на Android! Дальнейшие итерации разработки Когда вы делаете изменения в вашем Swift код, вам придётся проделать следующее: На панели Swift Stream нажать  Project -> Build Далее, нажать  Java Library Project -> Assemble Скопировать новый  .aar  файл из папки  outputs/aar  в папку вашего Android проекта ( app/libs ), перезаписав старый файл Вот и всё! Теперь вы кроссплатформенный Swift-разработчик! Пример работы с JNI Переходим к самой интересной части, к коду! Поговорим о том как можно коммуницировать между Swift и Java/Kotlin. Как я говорил выше, мы будем всё писать на Kotlin, просто потому что это стандарт для Android разработчиков в наши дни. Мы покроем некоторые простые, но самые насущные сценарии в этот раз, а погрузимся глубже в следующих публикациях. ⚠️  Напоминание : необходимо вызвать  SwiftInterface.initialize(this)  перед вызовом других нативных методов! Отправляем Int из Kotlin в Swift Начнём с простого. Задекларируйте метод в  SwiftInterface.kt : external fun sendInt(number: Int) Со стороны свифта имплементируйте следующее: #if os(Android)\\n@_cdecl(\"Java_com_habr_swiftlib_myfirstandroidproject_SwiftInterface_sendInt\")\\npublic func sendInt(\\n    envPointer: UnsafeMutablePointer<JNIEnv?>,\\n    clazzRef: jobject,\\n    number: jint\\n) {\\n    let logger = Logger(label: \"🐦\\u200d🔥 SWIFT\")\\n    logger.info(\"#️⃣ sendInt: \\\\(number)\")\\n}\\n#endif Вызовите его из вашего приложения: SwiftInterface.sendInt(123) Проверьте LogCat:  I  [🐦\\u200d🔥 SWIFT] #️⃣ sendInt: 123 Это было просто, да? :) Отправляем IntArray из Kotlin в Swift Задекларируйте метод: external fun sendIntArray(array: IntArray) Со стороны свифта принимайте массив: #if os(Android)\\n@_cdecl(\"Java_com_habr_swiftlib_myfirstandroidproject_SwiftInterface_sendIntArray\")\\npublic func sendIntArray(\\n    envPointer: UnsafeMutablePointer<JNIEnv?>,\\n    clazzRef: jobject,\\n    arrayRef: jintArray\\n) {\\n    // Создаём легковесный логгер\\n    let logger = Logger(label: \"🐦\\u200d🔥 SWIFT\")\\n    // Получаем доступ к текущему окружению\\n    let localEnv = JEnv(envPointer)\\n    // Defer-блок отработает для удаления локального референса\\n    defer {\\n        // Удаляем локальный референс на объект\\n        localEnv.deleteLocalRef(arrayRef)\\n    }\\n    // Получаем длину массива\\n    logger.info(\"🔢 sendIntArray 1\")\\n    let length = localEnv.getArrayLength(arrayRef)\\n    logger.info(\"🔢 sendIntArray 2 length: \\\\(length)\")\\n    // Получаем элементы массива\\n    var swiftArray = [Int32](repeating: 0, count: Int(length))\\n    localEnv.getIntArrayRegion(arrayRef, start: 0, length: length, buffer: &swiftArray)\\n    // Теперь можно использовать `swiftArray` как обычную Swift array\\n    logger.info(\"🔢 sendIntArray 3 swiftArray: \\\\(swiftArray)\")\\n}\\n#endif Вызовите из вашего приложения: SwiftInterface.sendIntArray(intArrayOf(7, 6, 5)) Проверьте LogCat:  I  [🐦\\u200d🔥 SWIFT] 🔢 sendIntArray: 1\\n I  [🐦\\u200d🔥 SWIFT] 🔢 sendIntArray: 2 length: 3\\n I  [🐦\\u200d🔥 SWIFT] 🔢 sendIntArray: 3 swiftArray: [7, 6, 5] Отправляем String из Kotlin в Swift Задекларируйте метод: external fun sendString(string: String) Со стороны свифта: #if os(Android)\\n@_cdecl(\"Java_com_habr_swiftlib_myfirstandroidproject_SwiftInterface_sendString\")\\npublic func sendString(envPointer: UnsafeMutablePointer<JNIEnv?>, clazzRef: jobject, strRef: jobject) {\\n    // Создаём легковесный логгер\\n    let logger = Logger(label: \"🐦\\u200d🔥 SWIFT\")\\n    // Получаем доступ к текущему окружению\\n    let localEnv = JEnv(envPointer)\\n    // Defer-блок отработает для удаления локального референса\\n    defer {\\n        // Удаляем локальный референс на объект\\n        localEnv.deleteLocalRef(strRef)\\n    }\\n    // Оборачиваем JNI string референс в `JString` объект\\n    // и получаем нативную Swift строку\\n    logger.info(\"✍️ sendString 1\")\\n    guard let string = strRef.wrap().string() else {\\n        logger.info(\"✍️ sendString 1.1 exit: unable to unwrap jstring\")\\n        return\\n    }\\n    // Используем нативную строку как только пожелаем\\n    logger.info(\"✍️ sendString 2: \\\\(string)\")\\n}\\n#endif Вызовите из вашего приложения: SwiftInterface.sendString(\"С любовью из Java\") Проверьте LogCat:  I  [🐦\\u200d🔥 SWIFT] ✍️ sendString 1\\n I  [🐦\\u200d🔥 SWIFT] ✍️ sendString 2: С любовью из Java Отправляем Date объект из Kotlin в Swift Задекларируйте метод: external fun sendDate(date: Date) Со стороны свифта: #if os(Android)\\n@_cdecl(\"Java_com_habr_swiftlib_myfirstandroidproject_SwiftInterface_sendDate\")\\npublic func sendDate(envPointer: UnsafeMutablePointer<JNIEnv?>, clazzRef: jobject, dateRef: jobject) {\\n    // Создаём легковесный логгер\\n    let logger = Logger(label: \"🐦\\u200d🔥 SWIFT\")\\n    // Получаем доступ к текущему окружению\\n    let localEnv = JEnv(envPointer)\\n    // Defer-блок отработает для удаления локального референса\\n    defer {\\n        // Удаляем локальный референс на объект\\n        localEnv.deleteLocalRef(dateRef)\\n    }\\n    // Конвертируем локальный референс на JNI date объект в глобальный\\n    logger.info(\"📅 sendDate 1\")\\n    guard let box = dateRef.box(localEnv) else {\\n        logger.info(\"📅 sendDate 1.1 exit: unable to box Date object\")\\n        return\\n    }\\n    // Создаём `JObject` из глобального референса на date объект\\n    logger.info(\"📅 sendDate 2\")\\n    guard let dateObject = box.object() else {\\n        logger.info(\"📅 sendDate 2.1 exit: unable to unwrap Date object\")\\n        return\\n    }\\n    // Вызываем метод `getTime` на объекте и получаем миллисекунды с начала эпохи\\n    logger.info(\"📅 sendDate 3\")\\n    guard let milliseconds = dateObject.callLongMethod(name: \"getTime\") else {\\n        logger.info(\"📅 sendDate 3.1 exit: getTime returned nil, maybe wrong method\")\\n        return\\n    }\\n    // И распоряжаемся миллисекундами как только пожелаем :)\\n    logger.info(\"📅 sendDate 4: \\\\(milliseconds)\")\\n}\\n#endif Вызовите из вашего приложения: SwiftInterface.sendDate(Date()) Проверьте LogCat:  I  [🐦\\u200d🔥 SWIFT] 📅 sendDate 1\\n I  [🐦\\u200d🔥 SWIFT] 📅 sendDate 2\\n I  [🐦\\u200d🔥 SWIFT] 📅 sendDate 3\\n I  [🐦\\u200d🔥 SWIFT] 📅 sendDate 4: 1757533833096 Получаем Date объект из Kotlin в Swift Задекларируйте метод: external fun sendDate(date: Date) Со стороны свифта напишем метод возвращающий строку: #if os(Android)\\n@_cdecl(\"Java_com_habr_swiftlib_myfirstandroidproject_SwiftInterface_ping\")\\npublic func ping(envPointer: UnsafeMutablePointer<JNIEnv?>, clazzRef: jobject) -> jobject? {\\n    // Обернём Swift строку в `JSString` вернём её JNI референс\\n    return \"🏓 Pong from Swift!\".wrap().reference()\\n}\\n#endif Вызовите из вашего приложения: Log.i(\"HELLO\", \"Pinging: ${SwiftInterface.ping()}\") Проверьте LogCat:  I  Pinging: 🏓 Pong from Swift! Исполняем асинхронный Swift код из Kotlin Задекларируйте метод: external fun fetchAsyncData(): String Здесь важно знать, что оператор  @_cdecl  не поддерживает асинхронные методы. Поэтому мы будем использовать семафор вокруг асинхронной задачи. Этот подход работает в случае вызова метода с семафором из  не-UI потока , в противном случае вы столкнетесь с дедлоком. Мы разберемся как иметь дело с UI в следующих публикациях. #if os(Android)\\n@_cdecl(\"Java_com_habr_swiftlib_myfirstandroidproject_SwiftInterface_fetchAsyncData\")\\npublic func fetchAsyncData(\\n    env: UnsafeMutablePointer<JNIEnv>,\\n    obj: jobject\\n) -> jstring? {\\n    // Создаём семафор, чтобы ожидать асинхронный таск\\n    let semaphore = DispatchSemaphore(value: 0)\\n    // Создаём переменную для результата\\n    var result: String? = nil\\n    // Стартуем асинхронный таск\\n    Task {\\n        // Симулируем длительную асинхронную операцию\\n        try? await Task.sleep(nanoseconds: 5_000_000_000) // 5 seconds\\n        // Записываем результат в переменную\\n        result = \"Async data fetched successfully!\"\\n        // Отпускаем семафор\\n        semaphore.signal()\\n    }\\n    // Блокируем текущий поток семафором в ожидании результата\\n    semaphore.wait()\\n    // Проверяем доступен ли результат\\n    guard let result = result else { return nil }\\n    // Оборачиваем строку в `JSString` и возвращаем её JNI референс\\n    return result.wrap().reference()\\n}\\n#endif Вызовите из вашего приложения (не на UI потоке): CoroutineScope(Dispatchers.IO).launch {\\n    Log.i(\"ASYNC\", \"Swift async call started\")\\n    try {\\n        val result = SwiftInterface.fetchAsyncData()\\n        Log.i(\"ASYNC\", \"Swift returned: $result\")\\n    } catch (e: Exception) {\\n        // Handle error\\n    }\\n    Log.i(\"ASYNC\", \"Swift async call finished\")\\n} Проверьте LogCat:  I  Swift async call started\\n I  Swift returned: Async data fetched successfully!\\n I  Swift async call finished Обернём Java класс в Swift Чтобы использовать какой-либо Java класс удобно, нам нужно написать для него обёртку. Давайте разберёмся как это сделать на примере  java/util/Date : public final class JDate: JObjectable, Sendable {\\n    /// Название JNI класса\\n    public static let className: JClassName = \"java/util/Date\"\\n\\n    /// Обёртка над глобальным JNI рефернсом, содержит в себе также метаданные класса.\\n    public let object: JObject\\n\\n    /// Конструктор для ситуации когда вы уже имеете `JObject`.\\n    /// \\n    /// Это может быть полезно когда вы получаете `Date` объект из Java.\\n    public init (_ object: JObject) {\\n        self.object = object\\n    }\\n\\n    /// Этот конструктор создаёт `Date` объект в JNI без аргументов\\n    public init? () {\\n        #if os(Android)\\n        guard\\n            // Получаем доступ к текущему окружению\\n            let env = JEnv.current(),\\n            // Находим класс `java.util.Date`\\n            // и загружаем его напрямую или из кэша\\n            let clazz = JClass.load(Self.className),\\n            // Вызываем создание нового объекта `java.util.Date` в JNI\\n            // и получаем его глобальный референс\\n            let global = clazz.newObject(env)\\n        else { return nil }\\n        // Сохраняем объект, чтобы использовать его в методах\\n        self.object = global\\n        #else\\n        // Для не-Android платформ возвращаем nil\\n        return nil\\n        #endif\\n    }\\n\\n    /// Этот конструктор создаёт `Date` объект в JNI с аргументом\\n    public init? (_ milliseconds: Int64) {\\n        #if os(Android)\\n        guard\\n            // Получаем доступ к текущему окружению\\n            let env = JEnv.current(),\\n            // Находим класс `java.util.Date`\\n            // и загружаем его напрямую или из кэша\\n            let clazz = JClass.load(Self.className),\\n            // Вызываем создание нового объекта `java.util.Date` в JNI,\\n            // передавая в него параметр с миллисекундами,\\n            // и получаем его глобальный референс\\n            let global = clazz.newObject(env, args: milliseconds)\\n        else { return nil }\\n        // Сохраняем объект, чтобы использовать его в методах\\n        self.object = global\\n        #else\\n        // Для не-Android платформ возвращаем nil\\n        return nil\\n        #endif\\n    }\\n} Выше показан необходимый минимум, чтобы начать работать с этим классом. Этот код даёт вам инициализировать  java.util.Date  объект из ничего или обернуть уже имеющийся референс. Окей, скелет у нас есть. Теперь ему нужны мышцы, давайте напишем ему некоторые методы! /// Returns the day of the week represented by this date.\\npublic func day() -> Int32? {\\n    // Удобный вызов `java.util.Date.getDay()`\\n    object.callIntMethod(name: \"getDay\")\\n} Уверен, что вы уловили идею! Теперь сами напишите аналогичным образом методы  getHours ,\\xa0 getMinutes ,\\xa0 getSeconds  и\\xa0 getTime . Немного иной метод с входящим параметром мы снова напишем вместе: /// Tests if this date is before the specified date.\\npublic func before(_ date: JDate) -> Bool {\\n    // Удобный вызов `java.util.Date.before(Date date)`\\n    // который принимает `Date` объект\\n    // и возвращает boolean результат\\n    object.callBoolMethod(name: \"before\", args: date.object.signed(as: JDate.className)) ?? false\\n} И, как вы уже догадались, следующий метод  after  вы напишете самостоятельно по аналогии с  before . И, полагаю, чтобы нам сделать наш класс действительно полезным нам пригодится метод возвращающий Swift  Date : /// Converts this java `Date` object to a Swift `Date`.\\npublic func date() -> Date? {\\n    // Получим миллисекунды с начала эпохи методом `getTime`\\n    guard let time = time() else { return nil }\\n    // Конвертируем миллисекунды в секунды и создаём Swift `Date` объект\\n    return Date(timeIntervalSince1970: TimeInterval(time) / 1000.0)\\n} Теперь у вас есть базовое понимание того как Swift работает с Java/Kotlin через JNI! Я надеюсь, что вам удалось успешно скомпилировать и протестировать код в вашем Android проекте. Это всё на сегодня, друзья. Для более глубокого погружения в мир JNI вы можете ознакомиться с детальным  README в репозитории JNIKit  на Github. Будем рады вам в нашем комьюнити в  Telegram  и  Discord , не стесняйтесь задавать вопросы! Подпишитесь, чтобы не пропустить следующие публикации! Мы обязательно поговорим о том как опубликовать вашу Swift библиотеку на JitPack, погрузимся в более сложные JNI случаи, и... конечно же построим UI! До связи!', hub='разработка мобильных приложений'), Page(id='946440', title='Что скрыл сайт? Архивные копии в OSINT', content='В арсенале OSINT-специалиста множество блестящих инструментов: VirusTotal для проверки безопасности, SpiderFoot для автоматизации, DNSDumpster для анализа инфраструктуры, Urlscan, Web-check и Pulsedive для всестороннего сканирования. Но у них всех есть критический недостаток: они показывают сайт только сейчас. Что если ключевая информация — старый адрес, удаленный контакт, измененное описание услуг, прошлые метрики аналитики — исчезла месяц или год назад? Стандартные инструменты оставят вас в неведении. Для подлинного понимания эволюции веб-ресурса нам нужна машина времени. И она существует – это Wayback Machine от Internet Archive и его мощный, но часто недооцененный CDX API. В отличие от поверхностного просмотра через веб-интерфейс, CDX API предоставляет программный доступ к индексу всех сохраненных копий (снимков) сайта в архиве. Это позволяет: 1.  Получать полный список снимков.  Все даты и версии страниц, CSS, JS, изображений. 2.  Фильтровать по времени.  Изучать сайт в конкретный год, месяц или даже день. 3.  Анализировать изменения.  Сравнивать содержимое, заголовки, метатеги, скрипты между разными эпохами. Шаг 1: Запрос к CDX API – получаем лог изменений Базовый запрос для основного URL: https://web.archive.org/cdx/search/cdx?url=https://tomhunter.ru/ Этот URL вернет текстовую таблицу со всеми сохраненными копиями главной страницы. Нужен JSON? Добавьте &output=json:https://web.archive.org/cdx/search/cdx?url=https://tomhunter.ru/&output=json Захват ВСЕХ поддоменов и файлов: Используйте маску *.domain и параметр collapse=urlkey для уникальных URL: https://web.archive.org/cdx/search/cdx?url=*.tomhunter.ru&collapse=urlkey Фильтрация по времени – ваша суперсила. Изучаем сайт в 2020 году? Добавим следующую строку, где параметры from и to используют формат YYYY, YYYYMM, YYYYMMDD, YYYYMMDDHH. https://web.archive.org/cdx/search/cdx?url=https://tomhunter.ru/&from=2020&to=2021 Шаг 2: Google Sheets – ваша аналитическая станция Скопируйте сырые данные (текст или JSON) из CDX API. Вставьте в новый лист Google Таблиц. Разделите данные: используйте формулу =SPLIT(A1, \" \") (где A1 – ячейка с данной строкой, пробел – разделитель). Колонки станут вашими полями: URL, временная метка, MIME-тип, код ответа и так далее. Шаг 3: Собираем живые ссылки на архивные копии Ключевые колонки после разделения – временная метка (timestamp) и оригинальный URL. Создайте гиперссылку: =HYPERLINK(\"https://web.archive.org/web/\"&C1&\"/\"&D1) C1 – ячейка с временной меткой (например, 20200102131415). D1 – ячейка с оригинальным URL (например,  https://tomhunter.ru/ ). Эта формула создаст кликабельную ссылку на конкретную сохраненную версию страницы в Wayback Machine. Шаг 4: Извлекаем контент в таблицу Теперь используем ссылки для импорта. Заголовок страницы (Title): =IMPORTXML(I1, \"//title\") (где I1 – ячейка с гиперссылкой из шага 3) Тело страницы (Body Content): =JOIN(CHAR(10), IMPORTXML(I1, \"//body\")) IMPORTXML извлекает весь текст внутри тега <body>. JOIN(CHAR(10), ...) объединяет элементы (если их несколько) с переносами строк для читабельности. Шаг 5: Охота за скрытыми данными – регулярные выражения Собранный текст (особенно из колонки Body) – золотая жила. Автоматизируйте поиск. Например, для выгрузки адреса электронной почты можно использовать формулу: =REGEXEXTRACT(K1; \"[A-z0-9._%+-]+@[A-z0-9.-]+\"), (где K1 – ячейка с текстом). А для копирования никнеймов контактных лиц – даже проще: =REGEXEXTRACT(K1; \"[A-z0-9._%+-]+), (где K1 - ячейка с текстом). Что мы можем увидеть? Анализируя исторические данные по колонкам, выявляем: 1.  Смену контактов.  Исчезнувшие email, телефоны, адреса офиса. 2.  Эволюцию описаний.  Как менялись услуги, миссия компании, ключевые клиенты. 3.  Метрики аналитики.  Появление, смена или удаление кодов Яндекс.Метрики или Google Analytics (прямо в HTML). 4.  Геолокация.  Изменение координат на встроенных картах (Google Maps, Yandex Maps). Описанную методику можно применить не только для сбора данных об архивных копиях сайтов. Она также работает для просмотра изменений социальных профилей и даже Telegram-каналов.\\xa0 CDX API Wayback Machine – это не просто архив, а детальный журнал изменений веб-ресурса. Интегрируя его данные через Google Sheets и используя простые формулы (IMPORTXML, REGEXEXTRACT), вы превратите статичные снимки в динамичную историю. Это позволит выявить закономерности, восстановить удаленную информацию и получить глубокое понимание эволюции цели вашего OSINT-расследования. Перестаньте смотреть только на «сейчас». Начните исследовать «тогда» — и вы откроете совершенно новый уровень аналитики.', hub='информационная безопасность'), Page(id='946444', title='Как высчитать дно Биткоина. Разбираем индикатор Hash Ribbons: Математика и код', content='На криптовалютном рынке цены очень хаотично движутся, прогнозы сбываются не каждый раз (особенно если вы полагаетесь на мнение инфлов), а новостной фон подливает масла в огонь. В этом хаосе многие пытаются найти хоть какие-то закономерности, опираясь не на догадки, а на измеримые данные. Сегодня мы как раз и займемся таким анализом — разберем популярный в узких кругах индикатор  Hash Ribbons . Но сделаем это не как трейдеры в поисках луд успеха, а разберем, что там под условно капотом. Важный дисклеймер:  Эта статья — исключительно техническое и математическое исследование. Это  не финансовая рекомендация, не торговый совет  и не попытка предсказать будущее. Моя цель — разобрать логику и код конкретного инструмента, а не предложить готовую стратегию. Любые действия на финансовых рынках сопряжены с высоким риском:). Итак, наша задача — взять экономическую гипотезу, посмотреть, как ее формализовали с помощью математики, и как в итоге она превратилась в код на Pine Script, который может запустить любой желающий. Концепция. Чтобы понять логику индикатора, для начала нужно разобраться с двумя вещами: что такое хешрейт и кто такие майнеры. Если говорить максимально просто,  хешрейт (Hash Rate)  — это общая вычислительная мощность всех устройств, которые участвуют в работе сети Bitcoin. Это своего рода \"пульс\" сети. Чем он выше, тем больше вычислительных ресурсов задействовано, тем безопаснее и стабильнее сеть. Майнеры — это те, кто предоставляет эти вычислительные мощности. Для них это бизнес: они тратят деньги на оборудование и электричество, а взамен получают вознаграждение в виде новых биткоинов и комиссий за транзакции. И как в любом бизнесе, здесь есть своя точка безубыточности. В этом и кроется суть гипотезы, на которой построен индикатор Hash Ribbons: Майнеры несут реальные расходы.  Главная статья —     электричество. Когда рыночная цена Bitcoin     падает, доход от майнинга снижается. Если цена падает ниже точки     безубыточности, наименее эффективные майнеры (те, у кого дорогое     электричество или устаревшее оборудование) начинают работать в убыток. Чтобы остановить потери, они     отключают свои устройства. Этот процесс массового отключения и называется  «капитуляцией     майнеров» . С точки зрения данных, капитуляция майнеров выглядит как заметное и устойчивое падение общего хешрейта сети. Автор индикатора, Чарльз Эдвардс, предположил, что  момент, когда майнеры прекращают капитулировать и начинают снова включать оборудование (хешрейт восстанавливается), исторически был благоприятным для покупки актива. Мы не будем оценивать прибыльность этой гипотезы. Мы просто примем ее как данность и посмотрим, как ее можно измерить и превратить в алгоритм. Для начала взглянем на исходный код индикатора. Он написан на Pine Script — языке для платформы  TradingView . Вот так выглядит его \"шапка\" и основные настраиваемые параметры: //@version=5\\nindicator(\\'Hash Ribbons (Capriole Investments)\\', overlay=false)\\n\\n// INPUTS - Пользовательские настройки\\ntype = input.string(\\'Ribbons\\', options=[\\'Ribbons\\', \\'Oscillator\\'], title=\\'Plot Type\\') \\n// Позволяет переключать вид: \"ленты\" или осциллятор\\n\\nlen_s = input(30, \\'Hash Rate Short SMA (days).\\') \\n// Период для короткой (быстрой) скользящей средней хешрейта. По умолчанию 30 дней.\\n\\nlen_l = input(60, \\'Hash Rate Long SMA (days).\\') \\n// Период для длинной (медленной) скользящей средней. По умолчанию 60 дней.\\n\\nsignals = input(true, \\'Plot Signals\\') \\n// Включить/выключить отображение сигналов на графике (кружочки и надписи).\\n\\nplot_halvings = input(true, \\'Plot Halvings\\')\\n// Показывать ли на графике вертикальные линии в моменты халвингов Bitcoin.\\n\\nraw = input(false, \\'Plot Raw Hash Rate\\')\\n// Отображать \"сырой\" график хешрейта вместо индикатора.\\n\\nsource = input.string(\\'INTOTHEBLOCK:BTC_HASHRATE\\', options=[\\'QUANDL:BCHAIN/HRATE\\', \\'INTOTHEBLOCK:BTC_HASHRATE\\'], title=\\'Source Hash Rate\\')\\n// Источник данных. Позволяет выбрать, откуда брать информацию о хешрейте. Как видим, в основе лежат всего несколько ключевых параметров, главные из которых — это периоды для двух скользящих средних. Именно они и являются ядром математической модели индикатора. В следующей части мы разберем, как они работают. Математическая модель. Итак, у нас есть гипотеза и поток \"сырых\" данных — ежедневные значения хешрейта. Проблема в том, что эти данные \"шумные\": хешрейт может колебаться день ко дню по множеству причин. Нам нужно отделить значимый тренд от этого шума. Как это сделать? Самый простой и проверенный способ в анализе временных рядов —  скользящие средние (Moving Averages) . Если вы не сталкивались с ними, представьте, что вы анализируете среднюю температуру. Можно смотреть на температуру каждый день, но она будет скакать. А можно посчитать среднюю за последние 30 дней и среднюю за последние 60 дней. Сравнивая эти два сглаженных значения, вы сможете понять общий тренд: становится в целом холоднее или теплее, игнорируя при этом разовые потепления или похолодания. Индикатор Hash Ribbons использует тот же подход, применяя простую скользящую среднюю (SMA) к данным о хешрейте. В коде это выглядит так: // ЗАПРОС ДАННЫХ И РАСЧЕТ SMA\\n// Запрашиваем дневные данные о хешрейте из указанного источника. \\n// lookahead=barmerge.lookahead_on нужен, чтобы на текущем, еще не закрытом дневном баре, использовать данные за вчера. Это защищает от \"перерисовки\" индикатора в реальном времени.\\nlive_HR_short = request.security(source, \\'D\\', ta.sma(close, len_s), gaps=barmerge.gaps_off, lookahead=barmerge.lookahead_on)\\nlive_HR_long = request.security(source, \\'D\\', ta.sma(close, len_l), gaps=barmerge.gaps_off, lookahead=barmerge.lookahead_on)\\n\\n// Для исторических данных \"заглядывание вперед\" не нужно\\nhist_HR_short = request.security(source, \\'D\\', ta.sma(close, len_s), gaps=barmerge.gaps_off, lookahead=barmerge.lookahead_off)\\nhist_HR_long = request.security(source, \\'D\\', ta.sma(close, len_l), gaps=barmerge.gaps_off, lookahead=barmerge.lookahead_off)\\n\\n// ... далее идет код для склейки исторических и real-time данных ...\\n// В переменные HR_short и HR_long попадают итоговые значения SMA\\nHR_short := barstate.isrealtime ? live_HR_short : hist_HR_short\\nHR_long := barstate.isrealtime ? live_HR_long : hist_HR_long\\n Здесь len_s и len_l — это те самые периоды (30 и 60 дней по умолчанию), которые мы видели в настройках. ta.sma(close, len_s) — это встроенная функция Pine Script, которая и вычисляет среднее значение хешрейта за указанный период. Теперь у нас есть две линии: 1.\\xa0\\xa0\\xa0\\xa0  Быстрая SMA  (30 дней) — более чувствительна к свежим изменениям. 2.\\xa0\\xa0\\xa0\\xa0  Медленная SMA  (60 дней) — более инертная, показывает долгосрочный тренд. Именно взаимодействие этих двух линий и формирует логику индикатора): Начало «капитуляции» : Быстрая SMA пересекает медленную сверху вниз. Это означает, что краткосрочная тенденция стала хуже долгосрочной. Математически это и есть наш сигнал о начале проблем у майнеров. Конец «капитуляции» (Восстановление) : Быстрая SMA возвращается обратно и пересекает медленную снизу вверх. Тренд меняется на позитивный, хешрейт восстанавливается. В коде эти моменты отслеживаются с помощью специальных функций: // ОПРЕДЕЛЕНИЕ СОСТОЯНИЙ\\n// ta.crossunder() возвращает true в тот момент, когда первая линия пересекает вторую сверху вниз.\\ncapitulation = ta.crossunder(HR_short, HR_long)\\n\\n// ta.crossover() делает то же самое для пересечения снизу вверх.\\nrecovered = ta.crossover(HR_short, HR_long)\\n\\n// Также вводится промежуточное состояние, когда майнеры находятся в процессе капитуляции\\nminer_capitulation = HR_short < HR_long\\n Таким образом, мы формализовали нашу гипотезу. \"Капитуляция майнеров\" — это больше не абстрактное понятие, а конкретное, измеримое состояние системы: HR_short < HR_long. Но это еще не все. Индикатор не просто фиксирует эти пересечения, а строит на их основе более сложный, композитный сигнал. Композитный сигнал. Автор индикатора пошел дальше и добавил второе условие, которое завязано уже не на хешрейт, а на  ценовой моментум . Идея в том, чтобы совместить два события: восстановление хешрейта и начало роста цены. Для оценки ценового моментума используются те же SMA, но уже от цены закрытия (close) и с периодами 10 и 20 дней (s10 и s20). Финальная логика, формирующая сигнал \"Buy\", выглядит так: // ЛОГИКА СИГНАЛА \"BUY\"\\nbuy = false\\nbuy := ta.crossover(s10, s20) and ta.barssince(recovered) < ta.barssince(ta.crossunder(s10, s20)) and ta.barssince(recovered) < ta.barssince(capitulation) or s10 > s20 and ta.crossover(HR_short, HR_long) Это сложное логическое выражение, которое, по сути, проверяет два сценария: Сначала восстановился хешрейт,     и  только после этого  начала расти цена. Цена уже находится в растущем     тренде (s10     > s20), и в     этот момент происходит восстановление хешрейта, подтверждая позитивный     настрой. Таким образом, то, что называется сигналом \"Buy\", на самом деле является композитным событием, требующим совпадения нескольких факторов. Визуализация. Помимо основной логики, в коде есть несколько полезных дополнений, отвечающих за визуализацию и удобство использования. 1. Отрисовка сигналов Для отображения всех ключевых событий (начало капитуляции, восстановление, покупка) на графике используется функция plotshape. Она рисует цветные кружки в нужные моменты времени. // PLOT - SIGNALS\\n// Рисуем серый круг в момент начала капитуляции\\nplotshape(signals ? capitulation : na, style=shape.circle, location=location.top, color=color.new(color.gray, 50), size=size.normal, text=\\'Capitulation\\')\\n\\n// ... (аналогично для других состояний) ...\\n\\n// Рисуем синий круг для сигнала \"Buy\"\\nplotshape(signals ? buy_plot : na, style=shape.circle, location=location.top, color=color.new(color.blue, 0), size=size.normal, text=\\'Buy\\') 2. Режим осциллятора В настройках индикатора можно переключить вид с \"лент\" на \"осциллятор\". Это просто альтернативный способ визуализации. Вместо двух линий он показывает гистограмму, отражающую разницу между быстрой и медленной SMA в процентах. // OSCILLATOR\\ndelta = HR_short - HR_long\\ndiff = delta / HR_short * 100\\n\\n// PLOT - OSCILLATOR\\nplot(type == \\'Oscillator\\' ? diff : na, style=plot.style_columns, color=diff < 0 ? color.red : color.blue, title=\\'Oscillator\\')\\n Если гистограмма ниже нуля — хешрейт в \"зоне капитуляции\", если выше — в \"зоне роста\". 3. Отображение халвингов Индикатор также умеет отмечать на графике даты халвингов Bitcoin — событий, когда награда майнерам сокращается вдвое. Это чисто визуальная метка, она не влияет на расчеты, но помогает сориентироваться в макроциклах рынка. // HALVINGS\\nhalving_1 = timestamp(2012, 11, 28, 0, 0)\\n// ... (даты для 2-го и 3-го халвингов) ...\\n\\n// Закрашиваем фон красным в районе даты халвинга\\nbgcolor(h1_range and plot_halvings ? color.new(color.red,20) : na) Заключение Мы с вами проделали путь от абстрактной экономической гипотезы до ее конкретной реализации в коде. Мы увидели, как с помощью простого, но эффективного аппарата скользящих средних можно формализовать идею о \"капитуляции майнеров\" и превратить ее в измеримый и визуализируемый алгоритм. И здесь я хочу еще раз, в последний раз, вернуться к тому, с чего мы начали. Это не алгоритм для предсказаний.  Это всего лишь одна из сотен моделей, которая пытается найти логику в хаосе, и работает с числами. Рынок может и будет вести себя иррационально, и любой, даже самый продуманный алгоритм, может давать сбои. Данная статья — это образовательное упражнение, демонстрация того, как можно применять инженерный подход к анализу рыночных данных. Не более того. А теперь вопрос к вам.  Какие еще неценовые, фундаментальные метрики (on-chain данные) вы считаете интересными для анализа с точки зрения программирования? Сталкивались ли вы с похожими задачами моделирования? Вступайте в наш чат, 24/7 рабочая атмосфера, разбираем рыночные сетапы вместе с комьнити, так же разбираем ошибки, помогаем друг другу выходить из плохих сделок ссылка на  чат (ссылка) . Если статья показалась вам интересной, то буду благодарен за подписку на  тг канал (ссылка) . Делитесь идеями в комментариях!', hub='анализ и проектирование систем'), Page(id='946410', title='Maven: verify или clean install?', content='Если вы автоматически пишете  mvn clean install  при сборке проекта, то вы такой точно не один — эта команда прочно укоренилась в привычке Java-разработчиков. Но действительно ли она всегда необходима? Что, если вместо неё использовать  verify ? В новом переводе от команды  Spring АйО  мы погрузимся в детали жизненного цикла Maven, поведение реактора, инкрементные сборки и подводные камни использования  clean  и  install . А главное — попробуем понять, когда можно (и нужно)  обойтись без  install  и  clean , чтобы ускорить сборку и сократить расходы памяти. Как и многие разработчики, при необходимости собрать Maven-проект, я в первую очередь вызываю  mvn clean install . В конце концов, именно эту команду можно найти практически в каждом README или BUILD-файле — в этом нет сомнений. Неважно, одиночный это проект или мультимодульный — инструкция всегда одна и та же. Более того, если в сборке участвуют несколько проектов, то install вроде как строго необходим, не так ли? Иначе соседние модули не смогут найти зависимости друг от друга. Или мне так казалось. На  JCrete  2018 у меня была возможность пообщаться с Робертом Шолте ( @rfscholte ) о Maven. Я столкнулся с проблемой при сборке подмножества модулей в реакторе, и он показал мне флаги команд  -am -pl .  Комментарий от эксперта Spring АйО, Михаила Поливахи Роберт является контрибьютором в Maven, а также является \"PMC Member-ом\", т.е. входит в так называемый Project Management Committee Под \"реактором\" имеется в виду не Project Reactor, а имеется в виду система, которая занимается сборкой мульти-модульных/мульти-проектных\\xa0приложений. Эта система уже давно существует в Maven, и носит имя \"Reactor\". О ней мало говорят, но я посчитал нужным это пояснить, чтобы убрать недопонимание. Флаги  -am  и  -al  это флаги реактора, можете про них почитать, там всё относительно просто Часть нашего разговора позже была описана в этом посте. Он также отметил, что можно использовать  verify  вместо  clean install  или просто  install . Сначала я был озадачен — я даже не знал, что такая команда как verify вообще существует и зачем она нужна. Моя первая реакция на предложение перейти на неё была примерно такой: «Погоди, другие проекты ведь не скомпилируются — им нужно разрешать зависимости между собой». С ухмылкой на лице Роберт терпеливо начал объяснять основы жизненных циклов Maven, целей, привязок плагинов и того, как всё это исполняется внутри реактора. Всем участникам сессии очень понравилось, и полученные знания оказались невероятно полезными. Если попытаться кратко передать суть сказанного, получится примерно следующее: Плагины Maven (Mojos) предоставляют поведение через цели (goals). Цель — это точка входа для выполнения определённого действия. Maven запускает последовательность фаз жизненного цикла. Плагины могут привязывать свои цели к конкретным фазам, чтобы они автоматически выполнялись при наступлении соответствующей фазы. Последовательность фаз предопределена (строго говоря, она может быть изменена с помощью расширений, но на практике это случается редко). В стандартной (наиболее распространённой) последовательности фаз идут:  validate, initialize, generate-sources, process-sources, generate-resources, process-resources, compile, process-classes, generate-test-sources, process-test-sources, generate-test-resources, process-test-resources, test-compile, process-test-classes, test, prepare-package, package, pre-integration-test, integration-test, post-integration-test, verify, install, deploy, pre-clean, clean, post-clean, pre-site, site, post-site, site-deploy . Обратите внимание, что  install  следует сразу после  verify . Это значит, что каждый раз, вызывая  install , вы также вызываете и verify. Запомните этот момент. В ноябре 2019 года Роберт делал доклад о Maven на Devoxx BE ( видео ), суть которого заключалась в адаптации к новому поведению инструмента. Конечно же, он упомянул тему  clean install  vs  verify . Исторически (во времена Maven 2) нам действительно приходилось постоянно использовать  install , если один модуль в мультипроектной сборке зависел от другого, потому что тогда поддержка таких сборок в Maven была на начальной стадии. Более того, изначально она реализовывалась не в ядре Maven, а через плагин  maven-reactor-plugin . Опыт, полученный при разработке этого плагина, позже был интегрирован в ядро Maven 3. Теперь, когда реактор стал частью ядра, его возможности глубже связаны с остальной инфраструктурой, в частности — артефакты, созданные в рамках текущей сессии (реактора), прикрепляются к ней, и другие модули могут получать эти артефакты напрямую, а не запрашивать их из репозиториев. И вот эта логика начинает работать уже на этапе  verify . Ещё один неочевидный момент — порядок вызова целей внутри реактора. Если мы вызываем несколько целей, они исполняются в заданном порядке для каждого модуля. То есть при вызове clean install в структуре проекта вроде .\\n├── pom.xml\\n├── project1\\n│   └── pom.xml\\n├── project2\\n│   └── pom.xml\\n├── project3\\n│   └── pom.xml\\n└── project4\\n    └── pom.xml цели вызываются так: :clean\\n:install\\n:project1:clean\\n:project1:install\\n:project2:clean\\n:project2:install\\n:project3:clean\\n:project3:install\\n:project4:clean\\n:project4:install Вместо вызова всех целей сначала по одной для каждого проекта, например: :clean\\n:project1:clean\\n:project2:clean\\n:project3:clean\\n:project4:clean\\n:install\\n:project1:install\\n:project2:install\\n:project3:install\\n:project4:install Именно из-за такого поведения, если мы вызываем  verify  для такой сборки, артефакты, сгенерированные project1, становятся доступны для всех остальных проектов, которым они могут понадобиться, без запроса этих артефактов из репозитория.  Комментарий от эксперта Spring АйО, Михаила Поливахи Автор не акцентирует внимание, но это важно. По сути, из всего, что описано выше, вы должны сделать простой вывод: В Maven 3 (а у вас у всех Maven 3, поверьте), собрать мультимодульное/мультипроектное приложение можно как с помощью  install , так и с помощью  verify . Просто  install  делает дополнительный шаг - инсталлирует jar в локальный .m2 кеш каждый проект, что было необходимо в Maven 2, но в Maven 3 уже нет.\\xa0 Поэтому, можете просто использовать  mvn verify , он должен работать быстрее, чем  install . Ещё один плюс отказа от  clean install  — в Maven 3 появилась поддержка инкрементных сборок, а команда clean эту возможность сводит на нет. Важно понимать, что хотя Maven и поддерживает инкрементные сборки, именно плагины должны обеспечивать корректную работу этой функциональности. Поэтому, если вы сталкиваетесь с проблемами при использовании инкрементных сборок, сообщите об этом команде Maven или авторам плагинов — возможно, где-то что-то реализовано неправильно. Комментарий от эксперта Spring АйО, Михаила Поливахи Инкрементальные билды вообще довольно распространенная практика в билд системах. например в Gradle или Make на них очень много построено. Просто в Maven, как показывает эмпирический опыт, люди редко используют их функционал. Хорошо, пора перейти к цифрам. Я решил провести эксперимент на наборе популярных проектов, которые ежедневно собираются десятки (если не сотни) раз, используют множество плагинов и настроек: проверки качества кода, annotation процессоры, генерация исходников и так далее. Я выбрал следующие шесть проектов (показано количество только Java-кода): Проект Модулей Файлов Пустых строк Комментариев Кода Guava 6 1976 65074 139019 375825 Byte Buddy 9 1128 33408 76365 159056 jOOQ 9 1772 73330 206477 194156 Sentinel 80 1064 12772 28026 58270 Helidon 202 3040 52731 120873 223469 Quarkus 737 6849 80264 46954 361403 Подсчёт строк кода был выполнен с помощью  https://github.com/AlDanial/cloc  после клонирования проектов. Все сборки выполнялись на Java 15, за исключением Guava и jOOQ — они собирались на Java 8. Сырые данные можно найти в этом  gist . Для всех сборок использовались следующие команды: mvn verify -DskipTests  \\nmvn clean                    \\nmvn verify -DskipTests       \\nmvn verify -DskipTests       \\nmvn verify -DskipTests       \\nmvn clean                    \\nmvn verify -DskipTests       \\nmvn clean verify -DskipTests \\nmvn clean                    \\nmvn install -DskipTests      \\nmvn install -DskipTests      \\nmvn install -DskipTests      \\nmvn clean                    \\nmvn install -DskipTests      \\nmvn clean install -DskipTests Первая команда  mvn verify  предназначена для тестирования сборки проекта и одновременного скачивания всех необходимых зависимостей и плагинов в локальный репозиторий Maven — так сетевой доступ не будет влиять на время сборки. Затем репозиторий очищается, и начинаются собственно измерения. Я решил пропустить тесты, поскольку они меня в этом эксперименте не интересуют. Позже в посте я ещё вернусь к этому решению. Полученные измерения представлены на следующих графиках: синие линии — вызовы verify, красные — вызовы  install . Теоретически красные линии должны быть немного длиннее синих, но лишь незначительно. Все значения приведены в секундах. Guava  соответствует ожиданиям: вызовы №2 и №3 выполняются быстрее, чем первый — примерно на 50%, поскольку сгенерированные результаты (классы, обработанные ресурсы и т.д.) скорее всего остались нетронутыми и могут быть переиспользованы. Тем не менее какая-то работа всё же выполняется — возможно, проверки качества кода. Затраты на очистку между сборками невелики: очистка репозитория занимает около 4 секунд, и это время включено в последние два измерения. Byte Buddy  выполняет одни и те же действия независимо от предыдущих результатов. Это может означать, что сборка не использует поддержку инкрементной сборки, реализованную в Maven, либо по своей специфике Byte Buddy вынужден выполнять одни и те же действия каждый раз. jOOQ  — весьма интересный случай. Обратите внимание: вызовы №2 и №3 выполняются на 85% быстрее. Эта сборка не тратит время на выполнение ненужных задач. Я был приятно удивлён этими цифрами — похоже, Лукас явно знает, что делает. Комментарий от эксперта Spring АйО, Михаила Поливахи Под Лукасом имеется в виду автор Jooq - Lukas Eder Sentinel  демонстрирует ожидаемую экономию времени: от 40% до 50% на вызовах №2 и №3. Helidon , похоже, также выполняет одни и те же действия независимо от результатов предыдущих сборок. И наконец,  Quarkus  (в котором сейчас более  745  модулей!). Пришлось использовать дополнительный профиль (quickly), чтобы отключить проверки качества кода, форматтеры и прочее, иначе сборка заняла бы более 20 минут. Quarkus показывает экономию до 60% времени при вызовах №2 и №3. Вы можете провести свои собственные замеры. Эти данные — всего лишь выборка и не претендуют на абсолютную истину, но они отражают определённые тенденции. Они не дают серьёзных оснований утверждать, что  install  значительно медленнее, чем verify — различия, как правило, несущественны. Тем не менее я всё же рекомендую вам провести измерения в своих проектах — возможно, именно для вас разница окажется значимой. А вот с использованием  clean  и инкрементных сборок всё куда очевиднее. В идеале вы вообще не должны вызывать  clean , оставляя его только для особых случаев, когда действительно необходимо удалить промежуточные результаты. Например, бывают ситуации, когда тесты требуют \"чистой\" среды — определённых директорий или ресурсов. И да, плагин EAR на данный момент действительно сломан, и для него по-прежнему приходится использовать clean  install . Есть и другие оправданные сценарии использования  install  в мультимодульной сборке. Например, может быть тест, которому нужно разрезолвить артефакт для работы — и тогда публикация через  install  становится быстрым решением. Другой вариант — использовать плагин mrm-maven-plugin (Mock Repository Manager), который предоставляет артефакты реактора как будто они доступны из Maven-совместимого репозитория. При этом тесты нужно будет адаптировать для обращения к этому репозиторию. Однако плагин используется редко — ведь проще запустить  clean install -DskipTests , чем добавлять новый плагин. Ещё один распространённый сценарий — запуск целей на одном модуле вне контекста реактора. Как известно, все цели в Maven запускаются во всех модулях, участвующих в реакторе. Допустим, мы хотим запустить тесты только в project3: $ mvn -am -pl :project3 test Эта команда запустит фазу test для всех проектов в реакторе: root, project1, project2 и project3. Хотя мы хотели проверить только project3, нам всё равно нужны POM и JAR-файлы всех его зависимостей. Проблема в том, что project3 зависит от project2, а тот — от project1, и все они унаследованы от root. Альтернатива — вызвать install в корне: $ mvn -am -pl :project3 install -DskipTests\\n$ cd project3\\n$ mvn test И далее запускать test или другие цели только для project3, пока не достигнем нужного результата. Похоже, Maven изначально задумывался для запуска именно из корня проекта, а не из подмодуля. Итак, опровергнута ли идея о том, что  verify  лучше  clean install , исходя из наших измерений? Кажется, что да. Но (а \"но\" всегда есть) — всё зависит от контекста. Помните, что install отвечает за копирование артефактов из реактора в локальный репозиторий. Если вам не нужны эти файлы в репозитории, и  verify  даёт те же наблюдаемые результаты (код работает, тесты проходят), то нет смысла вызывать  install . Вы сэкономите дисковое пространство. Если на вашем компьютере это не критично, возможно, стоит подумать о CI-среде. Частая проблема: после вызова  install  и запуска тестов на одном модуле используется устаревший артефакт зависимости — мы забыли сделать  install  на другом модуле, и теперь тратим время на отладку несуществующей ошибки, вызванной бинарной несовместимостью из-за использования устаревшей зависимости. В итоге:  вызов  verify  вместо  install  чаще всего даёт те же преимущества с меньшими недостатками. Есть сценарии, где без  install  не обойтись. Постоянное использование  clean  — тоже не лучшая идея, но иногда оно необходимо. Задаётесь вопросом, когда можно обойтись без него, а когда нельзя? Учитывая показанные здесь тенденции (и возможные замеры в вашем проекте), выигрыш по времени может быть незначительным — возможно, проще продолжать использовать clean install и не забивать себе голову. Возможно. А возможно — и нет. Лично мне нравится \"копать глубже\", изучать внутреннюю кухню Maven и находить способы сделать сборку быстрее и эффективнее. Мне интересно использовать инструмент шире, чем просто набор из трёх команд. Я не против поэкспериментировать. Но я также понимаю, что не все горят желанием поступать так же — да и не всегда обстоятельства позволяют. Напоследок:  попробуйте использовать  verify . Возможно, он лучше подходит для ваших задач, чем  install . Пишем код дальше! Присоединяйтесь к\\xa0русскоязычному сообществу разработчиков на\\xa0Spring Boot в\\xa0телеграм\\xa0—\\xa0 Spring АйО , чтобы быть в курсе последних новостей из мира разработки на Spring Boot и всего, что с ним связано.', hub='kotlin'), Page(id='946430', title='ТОП-10 канбан-досок для управления задачами в 2025 году', content='Управление проектами с помощью канбан-метода — это не только стикеры с задачами, которые команда передвигает между этапами. Сегодня командам мало простой доски в офисе, чтобы визуализировать все бизнес-процессы. На смену приходят многофункциональные канбан-сервисы, которые точно отображают воркфлоу, а не только список задач.\\xa0 Как выбрать инструмент, где будут не только канбан-доски для планирования задач, но и другие важные функции для комплексного управления проектами — разбираем в этой статье.\\xa0 Что такое канбан-доска Канбан-доска (англ. kanban board) — это целая система, которая визуализирует реальные процессы работы команды. Самая простая канбан-доска состоит из трех колонок: «Очередь», «В работе» и «Готово».  Доска может включать в себя разные колонки в зависимости от workflow компании. Например, колонки согласования и проверки. В IT-разработке стандартные  канбан-доски  имеют колонки: бэклог , кодирование, ревью, тестирование, релиз. Доска задач для сотрудников должна содержать реальные этапы работа, которые команда действительно выполняет в процессе выполнения поручения.\\xa0 Также она может содержать дорожки, где будут обозначены приоритеты, команды, проекты и другие важные критерии распределения задач.  Каждая карточка на доске хранит всю информацию о конкретной задаче: ответственный, участники, дедлайн, ссылки и файлы. В некоторых таск-трекерах есть даже чат обсуждения задач внутри карточки.  Пример карточки в системе управления проектами Kaiten Зачем нужна канбан-доска С помощью нее сотрудники отслеживают, на каком этапе находится задача, кто за нее ответственен, какой срок у поручения.\\xa0 Доска для проектов показывает руководителю общий прогресс работы команды, помогает распределять нагрузку между сотрудниками, анализировать трудозатраты и эффективно прогнозировать сроки проектов. Но глобальная задача канбан-доски — превращать хаос в порядок. Представим, что работа команды — это кухня ресторана. Что будет, если клиент будет просто приходить на кухню и выкрикивать свой заказ? Начнется хаос. Где-то повара забудут добавить соль, где-то пережарят, а чей-то заказ и вовсе не будет готов, потому что сотрудники могут просто забыть про блюдо. Избежать такого хаоса в процессах помогает доска с канбан-колонками и карточками задач. Подобная система работы помогает: не упустить из виду ни одну задачу — все они хранятся на доске, за каждой закреплен ответственный сотрудник, которую должен довести задачу до конца; успеть все сделать в срок — все  дедлайны  хранят карточки задач; расставить приоритеты — все новые задачи уходят в них очереди, команда продолжает заниматься текущими задачами. Если появляется «горящее» поручение —\\xa0 сотрудники могут легко сфокусировать на нем, но не потерять из виду другие дела; отследить «узкие горлышки» процесса — руководитель увидит, на каком этапе, колонке, застревает задача и сможет направить туда внимание; прогнозировать следующие проекты — тимлид может собрать аналитику с онлайн-доски и использовать ее для построения плана выполнения будущих задач.\\xa0 Как выбрать канбан-доску Выбор инструмента с канбан-доской зависит от потребностей команды. Если сотрудников пока немного, можно ограничиться простым таск-трекером с колонками. Для крупной команды, где работают десятки, сотни или тысячи людей нужен более продвинутый инструмент с автоматизацией и глубокой аналитикой работы.\\xa0\\xa0 Рассмотрим несколько обязательных критериев, которые сделают работу на канбан-доске удобной для любой команды: визуальная простота и удобный интерфейс . Рабочее пространство обязано помогает визуализировать процессы, делать их прозрачными и понятными, а не усложнять процесс работы. Поэтому доска должна быть понятной для всех сотрудников и упрощать их работу; гибкость настроек колонок и дорожек.  Команда может легко удалять, добавлять и перемещать колонки и дорожки, подстраивая доску под уникальные бизнес-процессы; совместная работа в реальном времени.  Все изменения, вносимые одним участником доски сразу отображаются на экранах остальных. Так, команда сможет работать слаженно, как единый организм, без сбоев и непонимания процессов коллег;\\xa0 WIP-лимиты . Это ограничение количества задач на определенном этапе. Помогает сотрудникам не перегружаться и фокусироваться на текущих задачах; файлы и чек-листы внутри карточек.  Чтобы сотрудники не тратили время на поиск информации по каждой задаче, можно прикрепить все документы и файлы к карточке. Чек-листы помогут не потерять мелкие детали поручения. Например, сотрудник не забудет отправить статью на согласование редактору перед релизом, если карточка содержит чек-лист с этим пунктом; автоматизация . Даже в небольшой команде много рутинных задач, которые можно передать системе управления проектами. Например, при передвижении карточки на новый этап, система может сама прикрепить к нему необходимые документы, чек-листы и тегнуть исполнителя. Тимлиду не нужно вовлекаться в микроменеджмент.\\xa0 10 лучших канбан-досок с задачами для сотрудников Kaiten Kaiten  — единая система управления проектами, основанная на канбан-методологии. Это единственный таск-трекере в подборке, который был создан специально под работу по канбан-методу.\\xa0 Особенности Система включает в себя все необходимые для работы инструменты: гибкую настройку канбан-досок онлайн; простую в настройке, но многогранную автоматизацию, с помощью которой можно создать более 300 различных сценариев автоматической работы; отображение пространств в виде канбан-досок, таблицы, таймлайна, календаря;\\xa0 размещение нескольких досок на одном пространстве, чтобы удобно отслеживать задачи по всем проектам и командам; возможность создавать верхнеуровневые доски для руководителей; гибкое распределение личных и групповых доступов; аналитику работы команды. Доступны более 10 отчетов: График сгорания задач, Накопительная диаграмма потока, Суммарный отчет, Контрольная диаграмма и др.; Диаграмма Ганта, в работе с которой можно гибко менять сроки и связи между задачами; дополнительный модуль работы для  Scrum-команд , с помощью которого можно гибко настраивать спринты и собирать аналитику о проделанной работе за выбранный период: хранилище файлов в виде раздела «Документы»; автоматический перенос задач их ушедших зарубежных систем с помощью встроенного приложения.\\xa0 Недостатки система Пользователи отмечают скучный серый интерфейс в сравнении с другими платформами. Нет возможности создавать видеочат, как в аналогичных системах.\\xa0 Цена \\xa0 В Kaiten есть 4 тарифа, которые отличаются числом пользователей, доступных для создания пространств, пользовательских полей и досок: Бесплатный тариф; Старт — 185 рублей за пользователя; Стандарт — 430 рублей за сотрудника, 2 дополнительных модуля; Бизнес — 580 рублей за пользователя, 6 модулей на выбор.\\xa0 Shtab Shtab  — отечественная платформа для работы с канбан-досками онлайн. Есть встроенный учет времени и планировщик задач для каждого сотрудника. Также можно включить работу с финансами команды при установке дополнительного модуля. Особенности: широкие возможности автоматизации с простой настройкой; хранение проектов в папках, которые можно располагать по определенной иерархии; распределение доступов между членами команды; яркие обложки карточек и метки, чтобы исполнители могли быстро ориентироваться в задачах; подзадачи отображаются прямо на доске; база данных, где можно хранить документы и файлы компании; отображение задач в Матрицы Эйзенхауэра; автоматические скриншоты рабочих пространств для мониторинга работы сотрудников и состояния пространств; аналитика работы каждого пользователя отдельно.\\xa0 встроенный Pomodoro-таймер. Недостатки : новичкам сложно разобраться в системе из-за множества настроек и функция платформы; нет канбан-аналитики работы сотрудников; нет видеочата.\\xa0 Цена При оплате за месяц действуют тарифы: бесплатный вечный тариф; 190 рублей за участника; 390 рублей за пользователя; 600 рублей за пользователя. Все тарифы отличаются числом пользователей, доступностью автоматизации и объем хранилища данных.\\xa0 Teamly\\xa0 Платформа  для синхронизации работы команды, визуализации процессов и хранения документов. Создана не только, чтобы управлять командой, но и эффективно обучать ее с помощью систематизированной базы знаний.\\xa0 Особенности простое создание карточек и гибкая настройка пространств; 10+ цветовых тем интерфейсов; встроенные в карточки формула расчета; отображение задач в виде задача; встроенный импорт данных в виде CSV-файлов; быстрая настройка связей между задачами в виде списка; функции для работы и планирования спринтов; автоматические уведомления исполнителям о действиях в карточках и сроках; ИИ-агент для работы с базы знаний и задачами; надежная коробочная версия с гибкими настройками облачной.\\xa0 Недостатки \\xa0 доски невозможно разделить на дорожки; пользователи отмечают нестабильную работу сервиса; сложная интеграция с другими платформами; выгрузка данных только через CSV-файлы. Цена Стоимость платформы рассчитывается из числа редакторов пространств, а не пользователей.\\xa0\\xa0 Например : годовая лицензия для 20 редукторов и 100 пользователей стоит\\xa0 65 940 рублей на Базовой тарифе; 71 940 рублей за год при том же количество читателей и редакторов, но на Профессиональном тарифе; 311 940 рублей за 1000 сотрудников на том же тарифном плане. При этом Базовый тариф недоступен при подключении более 200 пользователей.\\xa0 Также есть тарифный план Enterprise для крупных команд, который рассчитывается по запросу.\\xa0 Если на пространстве работает 5 редакторов и 0 пользователей, доступен Бесплатный тариф.\\xa0 Aspro.Agile  Сервис  для упрощения и визуализации бизнес-процессов с помощью kanban boards. Больше похож на таск-трекер, нежели на глобальную систему управления процессами. Особенности : встроенный тайм-трекер, который сотрудники могут запускать и завершать в период работы с задачами; Scrum-доски для работы по спринтам; прямой перенос данных из Jira: отдельные функции для управления бэклогом команд; база знаний для обмена опытом между коллегами и упрощенного онбординга сотрудников. Также можно открыть доступ клиентам к документации, чтобы разгружать техническую поддержку бизнеса; автоматизация простых операций — назначить исполнителя, отправить уведомления, добавить комментарий к карточке задачи и др.; внутренний мессенджер для коммуникации команды. Минусы платформы: подходит только для маленьких команд с простыми бизнес-процессами; нет автоматической подробной отчетности о работе сотрудников; импорт задач доступен только из Jira. Стоимость Доступны 4 тарифа: Бесплатный: доступно 3 пространства и подключение 3 сотрудников; Стандартный: 1990 рублей в месяц, 10 пользователей, 5 пространств; Про: 5 090 рублей в месяц, 30 пользователей, бесконечное число пространств; Премиум: стоимость можно узнать только после разговора с менеджером и просчета.\\xa0 Тарифы также отличаются доступностью функций. Например, в бесплатном тарифе нет автоматизации, базы знаний, работы с API и вебхуков.\\xa0 TeamStorm Сервис  для работы с бизнес-целям, визуализации процессов, мониторинга работы сотрудников. Подходит для команд, которые работают по вехам и  OKR-методологии .\\xa0 Особенности : простая реализация канбан-досок, в которой разберется любой сотрудник; встроенные Agile-отчеты; возможность ресурсного планирования прямо на платформе; построение рабочих процессов с помощью блок-схем; учет трудозатрат каждого сотрудника и команды; встроенный тип пространства для работы по OKR-методу. база знаний с широкими возможностями иерархии папок и документов; настройка ролей и групповых доступов; портфель проектов для руководителя; интерактивная диаграмма Ганта;\\xa0 автоматизация с помощью скриптов. Минусы : доски не делятся на дорожки — только на колонки по этапам работы; узкий набор интеграций; нужно оплачивать дополнительно доступ для гостевых пользователей; всего несколько ответов о работе команды. Стоимость Нет бесплатного доступа, но можно заказать демонстрацию системы. За одного сотрудника придется заплатить 500 рублей. Можно купить бессрочную лицензию, ее стоимость открывается по запросу. Чтобы подключить гостевого пользователя, нужно заплатить 42 рубля за человека.\\xa0 Битрикс24 Многофункциональная платформа  для управления командами в разных сферах. Позволяет планировать и выполнять задачи не только внутри команды, но и общаться с клиентами бизнеса. Особенности : есть чат внутри каждой карточки и встроенный мессенджер для сотрудников; доступен видеочат до 100 человек; отдельная колонка для просроченных задач; встроенная CRM-система для управления заявками и возможностью автоматизации работы с клиентами; сквозная аналитика для работы маркетинга; автоматизация задач по определенным триггерам; отдельная лента новостей компании; база данных для хранения документов; AI-ассистент для работы с задачами;\\xa0 широкий набор интеграций: подключение телефонии, готовые интеграции и обмен данными с 1С.\\xa0 Минусы : многослойная система, неудобная для перехода между своими задачами и проектами; сложно настраивать систему работы и интеграции – нужен отдельный специалист для настройки за доплату.\\xa0 Стоимость Тарифы системы: бесплатный — любое число пользователей, но с ограниченными функциями; 1990 рублей/месяц — на 5 пользователей и 24 Гигабайта хранения информации; 5 590 рублей за подключение 50 сотрудников и 100 ГБ данных; 11 190 рублей в месяц за 100 сотрудников и 1024 Гб. Есть отдельный тариф Энтерпрайз для больших команд за 27 190 рублей в месяц.\\xa0 ЛидерТаск Простой таск-трекер  со встроенной CRM-системой. Отслеживание и постановка задач происходят на стандартных канбан-досках. Внутри есть ежедневник для каждого сотрудника с его текущими задачами. Особенности : гибкое распределение ролей и настройка доступов; быстрая синхронизация работы команды на канбан-досках; помощь в развертывании на своем сервере; удобное мобильное приложение для планирования личных и командных задач; можно поставить задачу в голосовом сообщении в телеграм-боте; чек-листы в карточках; отдельный список поручений «Фокус» для приоритетных задач с соответствующей меткой; чат и файлы внутри карточки с описанием задачи; настройка повторяющихся задач; вложенность задач и проектов без лимита; восстановление удаленных задач из корзины; создание дерева задач для выстраивания иерархии. Минусы : даже простые функции требуют оплаты; баги системы; мобильное приложение не обладает всеми функциями ПК-версии; сложная синхронизация с Linux-системой.\\xa0 Стоимость \\xa0 Есть бесплатный тариф для одного устройства с ограниченным набором функций. В нем можно создать только 100 карточек, 10 проектов и 3 доски.\\xa0 Для личного полного пользования нужно оплатить 3199 рублей за год. Чтобы использовать в команде — 4 999 рублей за человека в год.\\xa0 ПланФикс Гибкая система  для общения между коллегами и сотрудников с клиентами. Есть встроенная упрощенная CRM-система.\\xa0 Особенности: пространства в виде канбан-доски и таблицы; глубокая аналитика работы команды; переписка с клиентами прямо из ПланФикс. При этом клиенты видят сообщения менеджера в их мессенджере или на почте; кастомизация интерфейса под цвета компании; обширная база знаний, где пользователи могут найти все необходимое для работы с задачами и настройки пространств; распределение задач по Матрице Эйзенхауэра; уведомление руководителей о просрочке задач; предупреждения о приближающемся дедлайне; кастомизация системы приоритетов под каждую команду;\\xa0 оперативная служба поддержки; возможность настроить систему под любой бизнес-процесс. Минусы : устаревший интерфейс; сложная настройка пространств — нужен отдельный специалист; медленная синхронизация пространств; баги в системе. Стоимость Есть бесплатный тариф, которым могут пользоваться до 5 сотрудников.\\xa0 При оплате за год цена рассчитывается в зависимости от размера команды: от 1 до 99 сотрудников: 360 рублей за сотрудника в месяц; до 250 человек: 600 рублей за пользователя. до 5 000 сотрудников: 840 рублей в месяц с человека. Чем дешевле тариф, тем меньше система предоставляет функции и места для хранения данных. Projecto Система , которая включает в себя управление задачами, документооборот.\\xa0 Особенности: отображение статусов внутри карточки; построение иерархии делегирования; автоматический список задач каждого сотрудника; кастомизированные отчеты; настройка приватных задач; несколько вариантов отображения пространств; хранения задач в черновиках до их публикации на доске; лента новостей бизнеса внутри таск-трекера и объявления; уведомления в углу экрана ПК, по почте и push-сообщения на телефоне; напоминания о днях рождений сотрудников; группировка задач; фильтрация карточек по статусу, срокам, исполнителям и другим параметрам; отчетность по числу активных пользователей; сохранения истории действий; подсказка для поиска похожих задач при создании новой. Минусы : подходит только для малых и средних команд; слабая автоматизация процессов; нет подробных канбан-отчетов; нет полноценного бесплатного тарифа. Цена Стоимость зависит от числа пользователей: от 1 до 100 сотрудников: 400 рублей за пользователя при оплате за месяц; от 101 до 200: 380 рублей/месяц с человека; от 201 до 1000: 360 рублей/месяц с человека; больше 1000: стоимость обсуждается отдельно.\\xa0 Если оплатить лицензию за год, то стоимость будет составлять 320, 304 и 288 рублей в месяц за сотрудника.\\xa0 Вместо бесплатного тарифа есть демоверсия со всеми функциями, которая сбрасывается раз в сутки.\\xa0 Yandex Tracker Простой сервис  для управления задачами от IT-гиганта российского рынка. Подходит для небольших команд с простыми процессами и для планирования личных дел.\\xa0 Особенности :\\xa0 можно создать до 2000 карточек на одной доске; стандартные функции карточек: назначение исполнителей, описание задачи, чат внутри карточки задачи; возможно группировки карточек по процессам, проектам и командам; встроенная Диаграмма Ганта проектов; мониторинг времени сотрудников, которое они потратили на задачу; функция фильтрации задач по приоритетам и статусам; несколько отчетов для отслеживания эффективности команды; шаблоны, которые могут создавать сами пользователи под свои задачи; уведомления о любых действиях в карточках; простая интеграция с сервисами Яндекса; настройка родительских и дочерних связей между задачами.\\xa0 Минусы : нельзя создавать несколько досок на одном пространстве; нет дорожек на досках; нет сложных автоматизаций и глубокой аналитики; чтобы зарегистрироваться, нужна почта Яндекса; хранения данных происходит в отдельном сервисе Yandex Cloud; ограничение по созданию задач. Стоимость Сервис доступен при подключении системы для бизнеса Яндекс 360. Ее тарифы, включающие в себя Яндекс Трекер: 569 рублей за сотрудникав месяц; 779 рублей за человека; 1539 за сотрудника. Тарифы различаются объемом хранилища данных, ограничениями по времени видеозвонков и числом пользователей. Также на некоторых тарифа нет SSO-доступов. Протестировать систему можно за 14 дней, оплатив 1 рублей за пользователя.\\xa0 Сравнительная таблица\\xa0 Сервис Стоимость Бесплатный тариф Особенности Минусы Kaiten От 185 рублей за пользователя\\xa0 Есть бесплатный тариф, неограниченный по времени и хранилищу данных. Можно подключить 5 пользователей, создать 3 пространства и 5 досок.\\xa0\\xa0\\xa0 создан для работы по канбан-методу; несколько досок на одном пространстве; 300+ сценариев автоматизации; глубокая отчетность о работе команды.\\xa0\\xa0 серый интерфейс; нет видеочата. Shtab От 190 рублей/сотрудник Вечный бесплатный тариф на 5 пользователей\\xa0 встроенная Матрица Эйзенхауэра; отображение подзадач на доске; таймер pomodoro; аналитика работы каждого сотрудника в отдельности.\\xa0 нет автоматических\\xa0 канбан-отчетов для всей команды; сложный для новичка интерфейс; нет видеочата Teamly От 299 рублей за редактора пространства, если нужно подключать пользователей Тариф на 5 редакторов без пользователей\\xa0 встроенные формулы расчета в карточке; ИИ-ассистент для работы в пространствах и базе знаний; импорт CSV-данных. нет дорожек на досках; сложная настройка интеграций.\\xa0 Aspro.Agile\\xa0 От 1990 рублей/месяц за 3 пользователя.\\xa0 Бесплатный тариф для 3 сотрудников без автоматизации, API и базы знаний.\\xa0 встроенный учет времени; scrum-доски для спринтов; широкие возможности управления бэклогом; мессенджер внутри платформы. нет автоматических отчетов о работе команды; импорт задач только из Jira: недостаточно функций для визуализации сложных процессов. TeamStorm От 500 рублей за активного пользователя, 42 рубля за гостевого.\\xa0 Нет, только демонстрация инструменты для работы по OKR; диаграмма Ганта; Agile-отчетность; блок-схемы для визуализации процессов. за гостевых пользователей нужно платить; мало интеграций; нет дорожек на канбан-досках. Битрикс24 От 1990 рублей за 5 сотрудников Есть вечный бесплатный тариф с ограниченными функциями встроенная CRM-система; мессенджер внутри платформы; новостная лента; широкий набор интеграций, в том числе телефония.\\xa0 сложный интерфейс; для настройки и интеграции нужен специалист. ЛидерТаск От 3199 рублей в год для личного пользования Есть, но только на 100 карточек, 3 доски и 10 проектов ежедневник внутри платформы; постановка задач голосом через тг-бот; отдельный список для приоритетных задач; гибкая система ролей и доступов. баги системы; ограниченные функции мобильного приложения; элементарные функции недоступны без оплаты. ПланФикс \\xa0 От 360 рублей за сотрудника\\xa0 Есть на 5 пользователей общение с клиентами напрямую из системы; Матрица Эйзенхауэра; глубокая аналитика; настройка кастомной системы приоритетов. пользователи отмечают баги и слабую синхронизацию; устаревший интерфейс.\\xa0 Projecto От 288 рублей с сотрудника при покупке годовой лицензии Нет, есть демоверсия с удалением всех данных каждые сутки система подходит для документооборота; гибкая иерархия делегирования и ролей; отдельный список задач для каждого пользователя; отчетность по числу пользователей. нет бесплатного тарифа; нет канбан-отчетности; мало автоматизаций. Yandex Tracker От 569 рублей с человека в месяц Нет. Есть тестовый вариант на 14 дней простой интерфейс; прямая интеграция с сервисами Яндекса; создание шаблонов для задач; группировки задач; фильтрация карточек.\\xa0 скромный набор функций; нужна Яндекс Почта для регистрации; до 2000 задач на одной доске; мало отчетов и сценариев автоматизации. Вывод Чтобы выбрать канбан-доску для работы команды, нужно учитывать потребности бизнеса, число сотрудников, необходимые функции для работы и бюджет на покупку лицензий.\\xa0 На отечественном рынке есть много качественных решения для управления проектами: от простого Яндекс Трекера до многофункциональных Kaiten и Битрикс24 — каждая команда найдет подходящий под своих бизнес-процессы инструмент.\\xa0', hub='управление персоналом'), Page(id='946354', title='Law & Practice Ensemble RAG. Как создать ИИ-ассистента, помогающего решать многоаспектные юридические задачи', content='Автор статьи: Сергей Слепухин ( @Sergey_Slepukhin_1988 ) В\\xa0первой части   мы кратко рассмотрели предпосылки и последствия ИИ‑трансформации деятельности юристов, а\\xa0также предложили вариант архитектуры продвинутой RAG‑системы, учитывающей особенности юридической предметной области. Во\\xa0этой части  мы дадим обзор общих и юридических бенчмарков, которые целесообразно учитывать при\\xa0оценке технических компонент RAG, а\\xa0также системы, в\\xa0целом. В\\xa0заключение рассмотрим, как\\xa0самостоятельно подготовить тестовый датасет для\\xa0оценки RAG‑системы с\\xa0помощью фреймворка RAGAS и разберем итоговые результаты эксперимента.  Общие и юридические бенчмарки Данная тема\\xa0— бенчмаркинг LLMs и других компонентов RAG‑систем в\\xa0юридическом домене на\\xa0русском языке, насколько нам известно, более или\\xa0менее глубоко или\\xa0системно еще не\\xa0обсуждалась на  habr , поэтому уделим ей должное внимание.  Бенчмарки  ( от\\xa0англ. benchmark )\\xa0— стандартизированные способы оценки качества и производительности работы системы на\\xa0различных задачах. Оценивать, в\\xa0общем, можно следующие технические компоненты RAG : бенчмарки моделей эмбеддингов; бенчмарки для\\xa0векторных хранилищ; бенчмарки для\\xa0LLM. 1. Бенчмарки моделей эмбеддингов :  для\\xa0оценки производительности и качества моделей эмбеддингов на\\xa0различных задачах поиска по\\xa0разным наборам данных.  Именно модель эмбеддингов вносит наиболее важный вклад в\\xa0качество работы модуля Retrieval . Выбор векторного хранилища, к\\xa0которому будет обращаться эмбеддер, также важен, однако он влияет, главным образом, на\\xa0производительность поиска. Основным инструментом для\\xa0оценки качества поиска является  MTEB  ( Massive Text Embedding Benchmark, MTEB ). Бенчмарк MTEB\\xa0— комплексный стандарт для\\xa0тестирования моделей sentence‑эмбеддингов, позволяющий оценивать их производительность на\\xa0более чем 50-и различных датасетах ( среди которых есть и юридические, правда, на\\xa0английском языке, об\\xa0см. ниже по\\xa0тексту ).  Для\\xa0оценки RAG‑системы важны задачи, связанные с\\xa0информационным поиском. В\\xa0категориях MTEB: Retrieval: найти релевантные документы по\\xa0запросу; Reranking: улучшить порядок результатов поиска; STS (Semantic Textual Similarity): оценить степень семантического сходства пар; Summarization. Для\\xa0каждой из\\xa0категорий вычисляется средний ранг модели по\\xa0всем датасетам в\\xa0этой категории. Рейтинги моделей публикуются на\\xa0Hugging Face и доступны по  ссылке . В 2024\\xa0году появилась возможность оценки и русскоязычных эмбеддеров с\\xa0помощью  ruMTEB  на 23\\xa0задачах, среди которых также есть и   задача   Retrieval. Также   MTEB   содержит специализированный бенчмарк для\\xa0юридического домена\\xa0—  MTEB (Law, v1 ), который  позволяет оценивать модели на 8\\xa0задач , в\\xa0частности: LegalBenchConsumerContractsQA  (Retrieval): датасет, который содержит 400\\xa0пар вопросов‑ответов (да / нет) в\\xa0сфере потребительских договоров (consumer contracts); AILACasedocs  (Retrieval): датасет состоит из\\xa0текстов решений Верховного суда Индии, позволяет оценивать поиск релевантных текстов судебных решений по\\xa0запросу; LegalSummarization  (Retrieval): датасет состоит из 439\\xa0пар контрактов и их суммаризаций. Датасеты в\\xa0MTEB (Law, v1) на\\xa0русском языке отсутствуют (только на\\xa0английском, немецком и китайском языках).  В\\xa0текущих условиях, полагаем, логично\\xa0было\\xa0бы взять в\\xa0качестве эмбеддера‑компонента RAG‑системы какую‑нибудь мультиязычную модель, поддерживающую русский и английский языки, а\\xa0также входящую в\\xa0топ-10\\xa0на\\xa0общем MTEB и одновременно имеющую наиболее высокий рейтинг на\\xa0MTEB (Law, v1). Современные трансформерные мультиязычные модели обучались на\\xa0множестве параллельных корпусов (задача text2text), соответственно можно предположить, что\\xa0такой эмбеддер сможет уловить некие общие, кросс‑языковые юридические паттерны. К\\xa0таким моделям, например, относятся  BGE‑M3  (также входит в\\xa0топ-10\\xa0рейтинга на\\xa0ruMTEB для\\xa0задачи Retrieval) или  e5-large . 2. Бенчмарки для\\xa0векторных хранилищ :  для\\xa0оценки производительности и качества задач векторного поиска, шире\\xa0— информационного поиска. При\\xa0этом биенкодер  cointegrated/LaBSE-en-ru , который мы выбрали в\\xa0качестве эмбеддера, конечно, не\\xa0может соперничать с\\xa0современными моделями: LaBSE вообще не\\xa0представлена в\\xa0лидерборде   MTEB (Law, v1). Однако для\\xa0того, чтобы оценить работу RAG‑системы в\\xa0целом,  cointegrated/LaBSE-en-ru , на\\xa0наш взгляд\\xa0— оптимальный baseline :  BGE‑M3  и  e5-large  имеют большую размерность (1024), требуют больше ресурсов для\\xa0развертки, более медленный инференс по\\xa0сравнению с\\xa0LaBSE, и так далее. Основной инструмент  для\\xa0бенчмаркинга\\xa0—  ANN‑Benchmarks  ( link ). Инструмент позволяет оценивать производительность работы алгоритмов из\\xa0семейства ANN для\\xa0аппроксимированного поиска ближайших соседей ( Approximate Nearest Neighbor, ANN ). Векторное хранилище FAISS, которое является одним из\\xa0ключевых компонентов в\\xa0нашей RAG‑системе, в\\xa0значительной степени опирается именно на\\xa0алгоритмы ANN. Метрики ANN‑Benchmarks позволяют, главным образом, измерить полноту (Recall) и скорость (per sec) поиска на\\xa0больших наборах данных. Для\\xa0RAG‑системы, работающей в\\xa0юридическом домене, на\\xa0наш взгляд, более важна полнота (по сравнению с\\xa0точностью): критически важный контекст не\\xa0должен\\xa0быть упущен. Ложные срабатывания, безусловно, также необходимо минимизировать, так как, если система будет недостаточно точной, LLM просто запутается в\\xa0нерелевантном и противоречивом контексте. Также, наряду с\\xa0бенчмарками, на\\xa0практике важно учитывать  показатели производительности  ( Performance Metrics measure efficiency ): задержка при\\xa0выполнении запроса  (time per search) и  пропускная способность  (queries handled per second) для\\xa0приложений, работающих в\\xa0режиме реального времени, критически важны; время индексирования  (indexing time)\\xa0— время, необходимое для\\xa0создания структуры данных,\\xa0— также имеет большее значение для\\xa0юридического домена, так как\\xa0с\\xa0ростом количества пользователей и охвата поддоменов неизбежно придется обрабатывать большие и часто обновляемые наборы текстовых данных; объём занимаемой оперативной памяти  (memory footprint);  масштабируемость  (scalability): это показатель необходимо протестировать в\\xa0зависимости от\\xa0размера набора передаваемых данных (например, обработка 1000\\xa0векторов, 1\\xa0млн или 100\\xa0млн векторов\\xa0— разные задачи) и размерности векторов (например, 768\\xa0или 1536\\xa0измерений), поскольку производительность часто снижается при\\xa0увеличении этого показателя. Исходя из\\xa0параметров производительности, FAISS, на\\xa0наш взгляд, является оптимальным решением, по\\xa0крайней мере, в\\xa0начале пути :  не\\xa0являясь решением «из коробки», оно требует дополнительной настройки перед развертыванием, тем не\\xa0менее оно бесплатное; требуют бОльшого объёма ОЗУ, чем основные конкуренты на\\xa0ANN (например, Annoy), однако обеспечивает более высокую скорость обработки данных (особенно, если подключить графический процессор), более того, является одним из\\xa0лидеров по\\xa0скоростным характеристикам, вне зависимости от\\xa0алгоритмов «под капотом». 3. Бенчмарки для\\xa0LLM.  Здесь, как\\xa0и в\\xa0случае с\\xa0моделями эмбеддингов, можно условно выделить: бенчмарки общего назначения ; бенчмарки специального назначения  для\\xa0оценки юридических способностей модели. 3.1. Бенчмарки общего назначения  позволяют оценивать LLMs на 2-х общих категориях задач: Language Understanding  — способность «понимать» естественный язык и использовать это «понимание» в\\xa0работе; Reasoning  — способность «рассуждать» и делать выводы на\\xa0основе вводных данных и цепочек рассуждений. Language Understanding 1. GLUE и SuperGLUE  ( от\\xa0англ. General Language Understanding Evaluation ): включает девять разнородных задач (грамматическая правильность предложений, классификация текста, парафраз, логический вывод и др.) для\\xa0оценки общего понимания языка моделями ( GLUE  стал одним из «пионеров» в\\xa0бенчмаркинге LLM в 2018\\xa0году, но\\xa0на\\xa0сегодняшний день несколько устарел и уже редко используется в\\xa0реальных задачах, так как\\xa0не\\xa0представляет особой сложности для\\xa0топовых больших языковых моделей), для\\xa0русского языка\\xa0был специально разработан и широко использовался в\\xa0свое время бенчмарк  Russian SuperGLUE . 2. MMLU  ( от\\xa0англ. Massive Multitask Language Understanding )\\xa0— основный бенчмарк для\\xa0оценки способности «понимания» LLM на\\xa0сегодняшний день, позволяет протестировать профессиональные знания модели, приобретенные в\\xa0процессе предобучения в\\xa0различных областях: содержит большое количество сложных вопросов из\\xa0экспертных областей знания, в  оригинальной статье  сказано, что\\xa0тест включает 57\\xa0заданий, в\\xa0том числе по\\xa0элементарной математике, истории США, информатике,  юриспруденции  и другим предметам; справедливо отмечается, что\\xa0для\\xa0понимания права необходимо знать, как\\xa0применять правила и стандарты в\\xa0сложных ситуациях, а\\xa0также давать ответы с\\xa0оговорками и пояснениями; однако юридические знания оценивались крайне фрагментарно, только на\\xa0английском языке, на\\xa0законах и прецедентах, действующих в\\xa0правовой системе США, в\\xa0виде ответов на\\xa0вопросы в\\xa0тестовой форме, например: вопросы предполагают несколько вариантов ответов (обычно четыре), что\\xa0позволяет минимизировать влияние случайно правильных ответов; содержит тесты, которые позволяют оценить общие навыки рассуждения и анализа. 3.  MERA   ( от\\xa0англ. Multimodal* Evaluation for Russian‑language Architectures )\\xa0— основный бенчмарк для\\xa0оценки способности «понимания» LLMs  русского языка  на\\xa0сегодняшний день. Содержит 23\\xa0задачи с\\xa0открытыми и закрытыми датасетами, представленными в\\xa0виде инструктивного набора данных.  Для\\xa0оценки способностей модели  » понимать »  некоторые элементы юридического контекста можно использовать   задачу   RCB : Russian Commitment Bank  (RCB)\\xa0— это набор из\\xa0естественных контекстов, в\\xa0которых содержится или\\xa0отсутствует причинно‑следственная связь, а\\xa0также размечены дополнительные дискурсивные характеристики\\xa0— вопрос, модальность, отрицание предшествующих условий; тип задачи \\xa0— NLI, textual entailment: оценивает способность делать логические выводы, классифицируя отношения между двумя предложениями как\\xa0согласованность, противоречие или\\xa0отсутствие логической связи; разработчики отмечают : «Датасет в\\xa0бенчмарке Russian SuperGLUE один из\\xa0немногих, для\\xa0которых всё ещё сохраняется значительный разрыв между оценками моделей и человеческой», хотя на\\xa0момент написания статьи задачу RCB лучше решают уже 3-и модели (общий разрыв внутри домена, в\\xa0любом случае, остается пока значительным): используемая нами модель gpt-4o‑mini пока на 21\\xa0месте , что\\xa0для\\xa0относительно не\\xa0новой и маленькой LLM также неплохо; создание датасета : все примеры\\xa0были собраны из\\xa0открытых новостных источников и\\xa0литературных журналов, на\\xa0основе корпуса Taiga (преимущественно не\\xa0специальные юридические источники), а\\xa0затем вручную перепроверены и дополнены человеческой оценкой на\\xa0Yandex.Toloka (то есть оценивался, скорее всего НЕ\\xa0юристами). Language Reasoning 1. BIG‑bench  (Beyond the Imitation Game) на\\xa0сегодняшний день\\xa0— основной бенчмарк для\\xa0оценки Reasoning : философия и цель отражены в\\xa0расшифровке аббревиатуры его названия\\xa0— « За\\xa0пределами игры в\\xa0имитацию »; содержит более 200\\xa0задач , среди которых, например, formal_fallacies_syllogisms_negation (выявление логических ошибок в\\xa0силлогизмах), logical_deduction (задачи на\\xa0логический вывод), strategyqa (ответы на\\xa0вопросы, требующие стратегического мышления) и др.; существует в 3-х версиях , которые отличаются по\\xa0сложности вопросов, на\\xa0которые нужно ответить моделям: BIG‑bench Lite (BBL), BIG‑bench Hard (BBH), BIG‑bench Extra Hard (BBEH);\\xa0лидерборды для 2-х первых отсутствуют,  для\\xa0BBEH на\\xa0момент написания статьи первая пятерка выглядит так : к\\xa0сожалению, на\\xa0момент написания статьи специальных юридических задач бенчмарк не\\xa0содержит. 2.  rulm‑sbs2 :  бенчмарк, который сравнивает русские аналоги ChatGPT\\xa0— Saiga, YandexGPT, Gigachat: содержит 15\\xa0категорий задач,   позволяет оценивать   также   способность   reasoning;   в\\xa0основе оценки подход LLM‑as‑a-judge с\\xa0помощью GPT-4, что\\xa0явно не\\xa0подходит для\\xa0нашего узкого профессионального домена: байесы модели‑оценщика, очевидно, будут иметь смещение, которое может повлиять на\\xa0оценку модели неочевидным для\\xa0разработчиков образом. Тема LLM‑бенчмарков\\xa0— отдельная, очень интересная и объемная тема. Для\\xa0тех, кто не\\xa0знаком или\\xa0находится в\\xa0начале пути освоения LLM, рекомендуем статью Германа Петрова И Дениса Рябоконь (Deepschool)\\xa0—  «Большой обзор LLM‑бенчмарков» : очень емко и доходчиво. Также отметим, что  нецелесообразно использовать для\\xa0оценки общих способностей LLM платформы автоматических оценок  а‑ля LMSYS ChatbotArena: данная платформа позволяет пользователям оценить вслепую, какая модель лучше (если пользователь выбрал правильно, получает очки, если нет\\xa0— теряет).  Очевидно, что\\xa0для\\xa0юридического домена оценки получатся крайне смещенными как\\xa0по\\xa0условно айтишным типам запросов (сложно представить, чтобы пользователи‑юристы в\\xa0массовом порядке оценивали работу моделей, большинство вопросов касаются кодинга), так и по\\xa0формату вопросов (неструктурированные прямые вопросы) или\\xa0ответов, далеких от\\xa0условных стандартов юридической практики.  В\\xa0заключение отметим, что   на\\xa0первом этапе , для\\xa0оценки нужных вам общих способностей LLM можно и нужно использовать информацию с\\xa0ресурсов, содержащих бенчмарки‑агрегаторы : Open LLM Leaderboard   (от Hugging Face): помимо MMLU содержит бенчмарки для\\xa0оценки рассуждений на\\xa0основе здравого смысла (HellaSwag), для\\xa0оценки сложных научных рассуждений такие (ARC) и др.; Artificial Analysis LLM Performance Leaderboard \\xa0— обширный ресурс (платформа) для\\xa0оценки открытых и проприетарных LLM. Ценностью ресурса является возможность сравнения качества моделей на\\xa0самом верхнем уровне качества и производительности:  На\\xa0уровне отдельных бенчмарков (например, MMLUPro): 3.2.  Бенчмарки специального назначения  для\\xa0оценки юридических способностей модели :  разработаны специально для\\xa0оценки юридических  » навыков »  LLMs. Нам известны какие‑либо русскоязычные бенчмарки в\\xa0рассматриваемом поддомене, на\\xa0котором тестируется RAG‑система\\xa0—  правовой режим недвижимого имущества . Также нам неизвестны  комплексные  (multi‑task)  русскоязычные   юридические бенчмарки , которые содержат подзадачи разного типа ( классификация, QA, интерпретация, поиск, генерация и др .) и направлены на\\xa0универсальную оценку моделей на\\xa0поддоменах, соответствующих отраслям права, отдельным законам, судебной практике различных инстанций и проч. Имеющиеся  [среди тех, которые удалось найти]  можно отнести к  экспериментальным наборам данных в\\xa0юридической сфере : RuTaR : поддомен  — российское налоговое право; датасет :  в\\xa0его основе\\xa0— 199\\xa0обработанных писем Минфина и ФНС России, содержит сгенерированные GPT-4o вопросы на\\xa0основе указанных писем Минфина, ответы экспертов, суммаризацию ответов в\\xa0бинарном виде\\xa0— Да/Нет, юридические источники, необходимые для\\xa0обоснования ответа; оцененные LLMs : на\\xa0момент написания статьи\\xa0— GPT-4o, LLaMA 3.3\\xa0и Mixtral + разные RAG‑режимы; ссылка на\\xa0репозиторий GitHub на\\xa0момент написания статьи не\\xa0открывается; Ru‑Legal‑QA‑v1 : поддомен  — набор вопрос‑ответов по\\xa0разным законодательным актам; датасет : 228\\xa0пар «вопрос‑ответ», помимо собственно вопросов датасет включает в\\xa0себя: полный экспертный ответ, список НПА, упомянутых в\\xa0ответе, точные цитаты НПА, использованные при\\xa0ответе, тип ответа:  дихотомический ,  фактологический ,  пропуск  и\\xa0т.\\xa0п., краткую версию ответа, полные тексты соответствующих НПА. оцененные LLMs : информация об\\xa0оценке моделей не\\xa0приводится; ссылка на\\xa0репозиторий GitHub на\\xa0момент написания статьи не\\xa0открывается; В\\xa0общем, исходим из\\xa0того, что\\xa0отсутствуют какие‑либо общепризнанные (как MERA) комплексные или\\xa0нишевые юридические бенчмарки на\\xa0русском языке . В\\xa0таких условиях целесообразно обратиться к\\xa0лидерам\\xa0— наиболее известным, проработанным и распространенным англоязычным юридическим комплексным бенчмаркам, и поразмышлять\\xa0— корректно\\xa0ли использовать их оценки для\\xa0работы в\\xa0условиях российской правовой системы. Language Understanding:  LexGLUE .  Бенчмарк позиционируется как\\xa0комплексная система независимого оценивания LLMs в\\xa0различных задачах категории Natural language understanding (NLU). Проект, вдохновленный бенчмарком SuperGLUE, основан на 7-и англоязычных датасетах и соответствующих им   7-и задачах: 1. ECtHR Tasks A & B (Multi‑label classification) : основан на\\xa0решениях Европейского суда по\\xa0правам человека (ЕСПЧ), каждое дело сопоставлено со статьями Европейской конвенции о\\xa0правах человека, которые\\xa0были нарушены (если таковые имеются): в\\xa0задаче А  входными данными для\\xa0модели является список фактов в\\xa0деле, а\\xa0выходными данными\\xa0— набор нарушенных статей конвенции ( 10+ статей\\xa0— labels ); в\\xa0задаче В  выходными данными является набор предположительно нарушенных статей ( 10+ статей\\xa0— labels ).  2. SCOTUS (Multi‑label classification) : основан на\\xa0информации из\\xa0решений (court opinions) Верховного суда США с 1946\\xa0по 2020\\xa0годы: входными данными являются фрагменты решений и различные метаданные, выходными данными\\xa0— набор категорий различных поддоменов (например, уголовное судопроизводство, гражданские права, экономическая деятельность и\\xa0т.\\xa0д.), соответствующих  14\\xa0labels.   3. EUR‑LEX (Multi‑label classification):  основан на\\xa0англоязычных текстах законов Европейского Союза (ЕС), опубликованных на\\xa0портале EUR‑Lex: входными данными являются фрагменты различных законов, выходными данными\\xa0— набор наиболее часто встречающихся концептов, относящихся к\\xa0различным видам деятельности ЕС и его государств‑членов   (например, экономика, здравоохранение, торговля и\\xa0т.\\xa0д.), соответствующих  100\\xa0labels. 4. LEDGAR (Multi‑label classification) : основан на\\xa0размеченных текстах положений договоров, полученных из\\xa0документов Комиссии по\\xa0ценным бумагам и биржам США (SEC), опубликованных на\\xa0портале EDGAR: входными данными являются положения договоров, выходными данными\\xa0— набор наиболее часто встречающихся категорий, соответствующих  100\\xa0labels. 5. UNFAIR‑ToS (Multi‑label classification) : основан на\\xa0наборе данных UNFAIR‑ToS, который содержит 50\\xa0условий обслуживания (ToS) с\\xa0онлайн‑платформ (например, YouTube, eBay, Facebook и\\xa0т.\\xa0д.), аннотирован на\\xa0уровне предложений с 8\\xa0типами несправедливых договорных условий, то есть условий (предложений), которые потенциально нарушают права пользователей в\\xa0соответствии с\\xa0законодательством ЕС о\\xa0защите прав потребителей: входные данные\\xa0— предложение, выходные данные\\xa0— набор несправедливых типов (если таковые имеются); 6. CaseHOLD (Multiple choice QA):  содержит около 53\\xa0тыс. вопросов с\\xa0несколькими вариантами ответов по\\xa0списку судебных дел (представляют собой краткие резюме решений) из\\xa0корпуса прецедентного права Гарвардской юридической библиотеки: входные данные\\xa0— отрывки, фрагменты из\\xa0текстов судебного решения, содержащие некоторое утверждение, выделенное жирным шрифтом; выходные данные\\xa0— номер правильного утверждения из 5-и предложенных вариантов. Представленный на\\xa0Hugging Face leaderboard на\\xa0сегодняшний день устарел  (данные на 2021). Очевидно, что  LexGLUE , основанный на\\xa0англоязычных датасетах из\\xa0другой правовой семьи, конечно, не\\xa0подходит как\\xa0конечный бенчмарк для\\xa0российской правовой системы, но  может\\xa0быть полезен  на\\xa0первом этапе \\xa0—  для\\xa0оценки общей юридической NLU‑компетенции мультиязычной модели. Пусть LexGLUE и не\\xa0содержит NLI‑задач, однако такие задачи как\\xa0CaseHOLD, ECtHR A/B, полагаем, позволяют оценить способность различать схожие нормы, классифицировать тексты по\\xa0их применению, в\\xa0целом, что\\xa0коррелирует со способностью legal reasoning. Language Reasoning:  LegalBench LegalBench разработан исследователями ( Neel Guha, Daniel E. Ho, Julian Nyarko, Christopher Ré ) из\\xa0Стэнфордского университета, является одним из\\xa0самых известных юридических бенчмарков, позволяющих оценить способность LLM к\\xa0юридическому рассуждению (legal reasoning) на\\xa0английском, а\\xa0впоследствии, как\\xa0утверждают авторы, и на\\xa0других языках. Бенчмарк является результатом совместного труда авторов ( все авторы\\xa0либо являются юристами,\\xa0либо имеют отношение к\\xa0юридической профессии ) и профессиональных юристов, которые активно привлекались для\\xa0подготовки датасетов и разработки задач. Например, юристам самим предлагалось предоставить наборы данных или\\xa0формулировку задачи, с\\xa0помощью которых можно\\xa0было\\xa0бы оценить интересную способность LLM к\\xa0юридическому рассуждению.  LegalBench : датасеты. Датасеты (всего\\xa0—  36\\xa0различных корпусов )  собраны из 3-х источников: существующих юридических корпусов (часто специально адаптированных для\\xa0LLM),  наборы данных от\\xa0юристов, участвовавших в\\xa0проекте, наборы данных, специально созданных авторами проекта. LegalBench : 6\\xa0типов задач .  Бенчмарк состоит из  162\\xa0задач , которые распределяются среди 6\\xa0различных категорий (типов), соответствующих 6\\xa0компонентам юридического анализа (рассуждения)\\xa0— legal reasoning.  «Ядро» составляют 4-е основных компонента (Issue‑spotting, Rule‑recall, Application, Conclusion) , в\\xa0основе которых лежит довольно известный в\\xa0юридической практике и юридическом образовании США (и иных стран, правовые системы которых относятся к\\xa0англосаксонской семье права) формализованный шаблон юридического анализа \\xa0— IRAC  (Issue\\xa0— Rule\\xa0— Application\\xa0— Conclusion) . В\\xa0соответствии с\\xa0методикой  IRAC юридическое суждение по\\xa0вопросу следует составлять из\\xa0следующих компонент: Issue  /  issue‑spotting \\xa0— выявление (идентификация) юридической проблемы. Rule  /  rule‑recall \\xa0— определение применимых правил. Application \\xa0— анализ фактов через призму правил. Conclusion  — формулирование вывода. Если под «правилами» понимать не\\xa0только нормы законов, но\\xa0и релевантную судебную практику, принципы, изложенные в\\xa0юридической доктрине и проч., то такой метод изложения является, на\\xa0наш взгляд, довольно логичным и естественным, независимо от\\xa0языка или\\xa0правовой системы, поскольку повторяет структуру классического силлогизма: правила\\xa0— большая посылка, фактические обстоятельства\\xa0— малая посылка, и в\\xa0заключение\\xa0— вывод. Однако  IRAC  — это, все‑таки,  не\\xa0метод юридического анализа или\\xa0юридического рассуждения по\\xa0своей сути, если под\\xa0ним понимать интеллектуальную деятельность , связанную с\\xa0решением юридических проблем: в\\xa0своей практической деятельности юрист, как\\xa0правило, использует множество навыков (не только письменных) и умений, которые никак не\\xa0покрываются указанной схемой.  Модель IRAC относится к\\xa0технике и стилю юридического письма (legal writing) , которая позволяет ясно и последовательно изложить свои аргументы по\\xa0какому‑либо правовому вопросу. При\\xa0этом данная модель не\\xa0универсальна и подходит, главным образом, для\\xa0оформления аргументов в\\xa0жалобах, претензиях, исковых заявлениях и прочих подобных документах (для каждого аргумента в\\xa0документе отдельно применяют IRAC).  Для\\xa0оформления юридических заключений (legal opinions) и меморандумов в\\xa0той\\xa0же американской юридической практике также часто используется  CREAC  (Conclusion, Rules, Explanation of Rules, Application, Conclusion): в\\xa0соответствии с\\xa0этой методикой документ начинается с\\xa0краткого вывода, далее кратко приводятся примененные правила с\\xa0обоснованием их применения, после чего приводятся факты дела, подробный анализ и итоговый вывод на\\xa0основании предыдущих элементов.  И таких методов более 2-х десятков  (подробнее см. по  ссылке ). Таким образом,  IRAC: не\\xa0является универсальной схемой юридического рассуждения , даже в\\xa0США, не\\xa0позволяет моделировать процесс юридического мышления  (юристы очень редко излагают свои мысли напрямую в\\xa0документах, а\\xa0их навыки и умения не\\xa0сводятся к\\xa0письменному изложению и обоснованию аргументов в\\xa0тексте), позволяет структурировать анализ и вывод только по\\xa0одному вопросу‑аргументу : в\\xa0одном документе могут\\xa0быть 10-и вопросов или\\xa0аргументов, каждый из\\xa0которых может по‑особенному учитываться в\\xa0итоговом выводе. Несмотря на\\xa0указанные ограничения, мы полагаем, что\\xa0IRAC является естественным  (воспроизводит структуру классического силлогизма в\\xa0юридической сфере)  и элементарным  (рассмотренные схемы кардинально не\\xa0отличаются от\\xa0IRAC и являются, скорее, его вариациями)  способом построения юридического рассуждения на\\xa0уровне аргументов . Поэтому позиции LLMs на\\xa0лидерборде  LegalBench  могут и должны учитываться при\\xa0выборе модели для\\xa0компонента генерации в\\xa0юридических RAG‑системах. Исходя из\\xa0схемы IRAC, LLMs оцениваются на 4\\xa0типах задачах. Issue‑spotting : модель должна определить, можно\\xa0ли из\\xa0набора данных фактов распознать конкретный юридический вопрос или\\xa0проблему. Обычно задача формулируется как\\xa0задача классификация («Да/Нет»): Здесь и далее приводится примеры из  оригинальной статьи   Rule‑recall : модель должна сформулировать или\\xa0вывести требуемое правило / ‑а по\\xa0заданной теме. Это может\\xa0быть задача\\xa0либо на\\xa0генерацию текста, который следует из\\xa0правила,\\xa0либо задача на\\xa0классификацию, например, действует\\xa0ли указанное правило в\\xa0данной юрисдикции. Задача важна для\\xa0оценки достоверности ответов модели.  Rule‑conclusion : модель на\\xa0входе получает набор конкретных фактов и формулировку правила, на\\xa0выходе она должна выдать ответ, без\\xa0пояснений.  Rule‑application : задача формулируется аналогичным образом, что\\xa0и для\\xa0rule‑conclusion, но\\xa0в\\xa0данном случае модель должна объяснить ход своих «рассуждений».  Оценивается   корректность изложения   и наличие в\\xa0изложении анализа (аргументации) . Генератор должен логично связать факты с\\xa0нормой и сделать вывод. Для\\xa0RAG это означает, что\\xa0модель выдаёт не\\xa0просто ответ, а\\xa0развёрнутую аргументацию. Корректность  соответствует критерию, согласно которому объяснения не\\xa0должны содержать ошибок (искажение юридической нормы, искажение фактов, неправильный вывод, логические и арифметические ошибки). Анализ : объяснения модели должны содержать выводы из\\xa0фактов, имеющих отношение к\\xa0данному правилу, а\\xa0также явно продемонстрировать, цепочку рассуждений, которые привели к\\xa0данному выводу. Например: Оставшиеся 2\\xa0типа задач , как\\xa0раз, связаны с\\xa0ограничениями схемы IRAC. Interpretation  —  понимание и интерпретация юридического текста : модель получает на\\xa0вход отрывок закона, договора или\\xa0судебного решения и должна извлечь информацию, классифицировать текст по\\xa0какому‑либо свойству или\\xa0ответить на\\xa0вопрос по\\xa0тексту. Например: Rhetorical‑understanding \\xa0—  аргументация на\\xa0основе анализа юридического текста:  в\\xa0этих задачах LLM на\\xa0вход предоставляется фрагмент текста, содержащий юридическую аргументацию (обычно фрагмент из\\xa0судебного решения), на\\xa0выходе модель должна определить, выполняет\\xa0ли данный фрагмент определенную функцию с\\xa0точки зрения аргументации / обладает\\xa0ли определенным свойством. Например: LegalBench : распределение задач . 162\\xa0задачи распределяются по\\xa0различным критериям : 1. разный формат :  MCQA\\xa0— множественный выбор ответов (35\\xa0задач),  генерация (7), бинарная классификация (112), многоклассовая классификация (8); 2. элементы юридического анализа  (видимо, категории задач пересекаются):  rule‑recall (5),  issue‑spotting (17),  rule‑application (16),  rule‑conclusion (16), interpretation (119), rhetorical‑understanding (10). 3. поддомены  ( авторы специально отмечают:  для\\xa0того, чтобы стандартизировать оценку, они выпустили «руководство по\\xa0ответам» для\\xa0каждой задачи данной категории, которое содержит выводы, необходимые для\\xa0каждого примера, и описывает распространенные типы ошибок. Очевидно, что\\xa0это существенное ограничивает полезность метрики Rule‑application Task для\\xa0оценки модели в\\xa0условиях НЕамериканской правовой системы ). контрактное право (58), корпоративное право (58), прочие (гражданский процесс, страхование и\\xa0т.\\xa0д.).  4. стили юридической письменной речи :  plain English (32),  legal opinions (11), merger agreements (34), contracts (55), statutory text (3), др. LegalBench : стратегии промпт‑инжиниринга . Все задачи представлены в\\xa0виде инструктивного набора данных. Инструкции\\xa0были подготовлены авторами проекта, с\\xa0привлечением сторонних юристов\\xa0— ученых и практиков. Помимо инструкций в\\xa0контекст модели при\\xa0оценивании встраиваются от 0\\xa0до 8\\xa0заранее подготовленных примеров. LegalBench : ограничения (указанные авторами проекта) : задачи построены на\\xa0датасетах с\\xa0короткими по\\xa0сегодняшним меркам текстами : на\\xa0момент публикации (2022) большинство оцениваемых LLM не\\xa0работали с\\xa0длинными (для любых юридических задач) контекстами; задачи рассчитаны на\\xa0однозначные  (объективно верные)  ответы , а\\xa0не\\xa0на\\xa0гибкое и извилистое юридическое рассуждение; бенчмарк ограничен английским языком и преимущественно американской юрисдикцией ; ограничения, связанные с\\xa0IRAC  (подробно рассмотрены выше по\\xa0тексту); бенчмарк не\\xa0покрывает задач комплексного многоступенчатого юридического анализа  (каковым он, как\\xa0правило, и является на\\xa0практике), наконец, тестируя компоненты юридического рассуждения по\\xa0отдельности ( сегодня мы поручили такую многоступенчатую задачу AI‑агенту ); датасеты представлены исключительно поддоменом гражданского права , со смещением в\\xa0сторону контрактного права. LegalBench : Leaderboards for Tasks on 07/23/2025 Issue Tasks Issue Tasks Rule Tasks Conclusion Tasks Interpretation Tasks Rhetoric Tasks Отметим: в\\xa0лидерах модели семейства Gemini, имеющие на\\xa0сегодняшний день самый большой размер окна\\xa0— 1\\xa0000\\xa0000\\xa0токенов; LLMs не\\xa0универсальны: на\\xa0разных категориях задач разные модели демонстрируют различные результаты; интересно, что\\xa0модель gpt-4o‑mini занимает почетное 2-е место на\\xa0Rhetoric Tasks, в\\xa0среднем, модель занимает 22\\xa0место в\\xa0общем рейтинге; С\\xa0точки зрения соотношения «цена / качество» Gemini 2.5\\xa0Flash Preview (Nonthinking) на\\xa0данном бенчмарке выглядит оптимальным выбором. Итак,  рассмотрев общие и специальные бенчмарки, мы можем сделать следующие общие выводы: Существующие на\\xa0сегодняшний день общие и специальные бенчмарки позволяют оценить работу компонентов  (генератора, векторного хранилища и эмбеддера)  RAG‑системы в\\xa0юридическом домене только на\\xa0этапе предварительной проверки .  Отсутствуют признанные и поддерживаемые специализированные юридические бенчмарки на\\xa0русском языке . Имеющиеся комплексные и наиболее проработанные англоязычные бенчмарки дают возможность оценить только общие юридические способности LLMs «понимать» и «рассуждать», и то\\xa0— на\\xa0ограниченном наборе задач, замкнутых на\\xa0отрасли гражданского, главным образом, контрактного права США.  Рассмотренные бенчмарки не\\xa0оценивают качество и производительности модуля поиска , тогда как\\xa0он имеет ключевое значение для\\xa0оценки работы RAG в\\xa0целом: риск возникновения «галлюцинаций» при\\xa0генерации увеличивается по\\xa0мере того, как\\xa0в\\xa0контекстное окно языковой модели включается нерелевантная информация, что\\xa0представляет значительный риск для\\xa0работы приложения в\\xa0юридическом домене. Главное практическое следствие ограниченности рассмотренных общих и специальных бенчмарков\\xa0— юристам трудно соотнести результаты работы современных LLMs с\\xa0их собственным пониманием юридической компетентности .  Таким образом, на\\xa0сегодняшний день использование бенчмарков позволяет на\\xa0предварительном этапе сузить выбор кандидатов на\\xa0место компонентов в\\xa0RAG‑системе: сравнительный анализ по\\xa0рассмотренным метрикам и иным параметрам позволит вам принять более обоснованные решения в\\xa0отношении компонент приложения RAG. Эталонные наборы данных Однако, несмотря на\\xa0полезность этих стандартизированных метрик для\\xa0начального этапа, особенно учитывая отсутствие специализированных юридические бенчмарков на\\xa0русском языке, чтобы действительно понять, насколько хорошо ваша система RAG будет работать на\\xa0ваших уникальных данных, необходимо создать собственную систему оценки. Собственная система оценки RAG предполагает наличие у\\xa0вас эталонных данных, которые будут отражать реальные сценарии работы вашего приложения. Эталонные данные\\xa0— это, в\\xa0общем, данные, которые представляют собой условно идеальные ответы на\\xa0заранее сформулированные вопросы. Категории вопросов должны максимально охватывать прогнозируемые сценарии работы приложения при\\xa0взаимодействии с\\xa0пользователями, при\\xa0этом вопросы должны\\xa0быть максимально реалистичными и в\\xa0то\\xa0же время достаточно экспертными, чтобы не\\xa0сбить модель с\\xa0толку. Несмотря на\\xa0то, что\\xa0оценка на\\xa0таких данных может показаться довольно субъективной, наличие набора эталонных данных для\\xa0сравнения входных и выходных данных RAG‑системы является критически важным шагом для\\xa0анализа влияния внесенных изменений и повышения эффективности системы в\\xa0дальнейшем. Но\\xa0как объективно оценить качество такой RAG‑системы в\\xa0столь ответственной области, как\\xa0юриспруденция? Здесь нам на\\xa0помощь приходит  фреймворк RAGAS . Оценка RAG‑системы с\\xa0помощью фреймворка RAGAS на\\xa0эталонных данных RAGAS   ( от\\xa0англ. Retrieval Augmented Generation Assessment )\\xa0— это довольно популярный  фреймворк  для\\xa0автоматической оценки RAG‑систем. Он позволяет не\\xa0только оценить работу модулей поиска и генерации архитектуры RAG, но\\xa0предоставляет метрики для\\xa0оценки производительности всей системы (end‑to‑end оценка). Ниже рассмотрим метрики, которые можно использовать для\\xa0оценки с\\xa0помощью RAGAS, а\\xa0также ограничения и специфику их использования в\\xa0юридическом домене. Оценка работы модуля поиска Для\\xa0оценки поиска RAGAS предлагает  две метрики :  Context Precision:  оценивает, насколько высоко ранжируются релевантные для «эталонного ответа» элементы в\\xa0извлеченном контексте.\\tИдеальная ситуация, когда все релевантные фрагменты находятся на\\xa0самых верхних позициях среди извлеченных результатов. Эта метрика подчеркивает, что\\xa0важен не\\xa0только сам факт наличия релевантного контекста (чанка), но\\xa0и его позиция в\\xa0выдаче ретривера: если релевантные фрагменты находятся в\\xa0конце списка, context precision будет низкой, даже если вся нужная информация\\xa0была извлечена.  Для  юридического домена   метрика крайне важна , так как\\xa0она позволяет гарантировать, что\\xa0LLM при\\xa0обработке похожих пользовательских запросов будет держать фокус на\\xa0наиболее релевантной правовой информации. Если говорить о\\xa0потенциале масштабирования базы знаний в\\xa0юридической сфере, такие БД могут\\xa0быть огромны, и «шум» в\\xa0извлеченном контексте (нерелевантные статьи, дела) может «сбить» LLM с «толку». Context Recall : оценивает, насколько пОлно извлеченный контекст соответствует эталонному (аннотированному) ответу, который считается «истинным» (ground truth). Метрика считается, в\\xa0общем, так: берем эталонный ответ, разбиваем его на\\xa0отдельные смысловые утверждения, затем для\\xa0каждого такого утверждения проверяем, есть\\xa0ли информация, его подтверждающая, в\\xa0контексте, который выдал ретривер. То есть метрика показывает долю утверждений из\\xa0эталонного ответа, которые действительно присутствуют в\\xa0извлеченном контексте. Для  юридического домена   метрика также крайне важна : чтобы дать полный и точный ответ, LLM должна иметь доступ ко всей необходимой правовой информации, соответствующей набору разнородных фрагментов (конкретные статьи, части статей, релевантные судебные прецеденты и\\xa0т.\\xa0д.). Отметим, что\\xa0для\\xa0адекватной оценки этой метрики необходимо наличие качественных и подробных эталонных ответов (ground_truth). Оценка работы модуля генерации Faithfulness:  оценивает, насколько все утверждения, которые можно извлечь из\\xa0сгенерированного ответа, подкреплены предоставленным LLM контекстом. Таким образом, метрика позволяет оценить, не «выдумала»\\xa0ли модель что‑то, что\\xa0не\\xa0присутствует в\\xa0документах, которые ей\\xa0были предоставлены для\\xa0генерации ответа. Необходимо учитывать, что   метрика сильно зависит от\\xa0способности Critic LLM, которую использует RAGAS, корректно разбивать ответ на\\xa0атомарные утверждения, а\\xa0также точно определять, подтверждается\\xa0ли каждое утверждение контекстом (это требует глубокого понимания текста и иногда\\xa0— логического вывода). Для  юридического домена , пожалуй, самая важная метрика , поскольку именно она позволяет понять, «галлюцинирует»\\xa0ли LLM при\\xa0генерации, а\\xa0это критически важный риск при\\xa0использовании модели в\\xa0домене. В\\xa0правовой сфере недопустимы «выдуманные» ссылки на\\xa0законы, несуществующие судебные решения или\\xa0искаженные факты. Answer Relevance : оценивает, насколько сгенерированный ответ релевантен исходному вопросу. Метрика позволяет оценить, насколько (условно) «хорошо» ответ модели отвечает на\\xa0заданный вопрос ( например, «плохой» ответ может\\xa0быть обусловлен тем, что\\xa0модель отклоняется от\\xa0темы, включая ненужную информацию ). Высокий балл означает, что\\xa0искусственные вопросы, полученные из\\xa0ответа, очень похожи на\\xa0оригинальный вопрос, что\\xa0должно указывать на\\xa0высокую релевантность ответа; низкий балл означает, что\\xa0ответ, возможно, ушел в\\xa0сторону или\\xa0содержит много нерелевантной информации. Отметим, что\\xa0эта  метрика должна использовать в\\xa0юридическом домене с\\xa0большой осторожностью, учитывая  алгоритм ее расчета :  генерация искусственных вопросов : на\\xa0первом шаге Critic LLM, которую использует   RAGAS, основываясь на\\xa0сгенерированном моделью ответе, обратно генерирует несколько (по умолчанию\\xa0— 3) вопросов, соответствующих этому ответу. То есть Critic LLM генерирует набор вопросов, на\\xa0которые, по\\xa0ее «мнению», отвечает сгенерированный текст. сравнение с\\xa0исходным вопросом : на\\xa0втором шаге вычисляется среднее косинусное расстояние между этими сгенерированными (искусственными) вопросами и исходным (оригинальным) вопросом.  Учитывая то, как\\xa0считается данная метрика, ее использование для\\xa0оценки систем в  юридическом домене , на\\xa0наш взгляд, должно\\xa0быть ограничено : Critic LLM  может истолковать избыточность юридического ответа как\\xa0нераскрытие релевантности : в\\xa0юридической практике часто ценятся подробные ответы, которые раскрывают тему с\\xa0разных сторон, даже если исходный вопрос\\xa0был общим; Critic LLM   может «воспринять» подробный ответ как\\xa0нерелевантный исходному «простому» вопросу. Это парадокс: хороший юридический ответ может\\xa0быть оценен низко. Critic LLM   специально   не\\xa0дообучена на\\xa0юридических текстах, также данная модель «видит» только сгенерированный ответ и исходный вопрос. Она не «знает», что\\xa0у\\xa0оцениваемой RAG‑системы есть доступ к\\xa0огромной базе документов, которая позволяет ей давать такие подробные ответы на\\xa0основе коротких, но\\xa0по\\xa0смыслу емких вопросов. В\\xa0общем, Critic LLM под «капотом» у\\xa0RAGAS, скорее всего, не\\xa0сможет уловить юридические смысловые тонкости, представленные в\\xa0ответе RAG‑системы, и адекватно сопоставить его по\\xa0смыслу с\\xa0пользовательскими запросами. Вместо этого, Critic LLM, например, может «воспринимать» каждый абзац ответа как «ответ» на\\xa0отдельный, очень конкретный вопрос, теряя из\\xa0виду общий смысл оригинального запроса. Таким образом, answer_relevance полезна для\\xa0выявления абсолютно нерелевантных ответов или «болтовни». Однако для\\xa0ответов, которые являются подробными и юридически релевантными, но\\xa0значительно более объемными, чем исходный вопрос, эта метрика может давать вводящие в\\xa0заблуждение низкие оценки. Комплексные оценки Наиболее значимой является Answer Correctness , которая оценивает точность сгенерированного ответа по\\xa0сравнению с «эталонным» ответом (ground truth). На  1-м шаге  рассчитывается  фактическая корректность  (Factual Correctness)\\xa0— F1-подобная метрика: насколько факты, утверждения или\\xa0положения, присутствующие в\\xa0сгенерированном ответе, совпадают с\\xa0фактами в\\xa0эталонном ответе. На  2-м шаге  рассчитывается  семантическая схожесть  (Semantic Similarity): насколько сгенерированный ответ и эталонный ответ близки по\\xa0смыслу. На  3-м шаге  рассчитывается  взвешенное среднее фактической корректности и семантической схожести , по\\xa0умолчанию веса составляют [0.75, 0.25] соответственно. Учитывая, что\\xa0Critic LLM  специально   не\\xa0дообучена на\\xa0юридических текстах  Answer Correctness следует использовать с\\xa0еще бОльшой осторожностью чем Answer Relevance: ответы на\\xa0юридические вопросы, как\\xa0правило, содержат мало фактологической информации, так как\\xa0представляют собой юридическую оценку указанных в\\xa0вопросе фактов, при\\xa0этом отсутствие в\\xa0ответе ссылок на\\xa0какие‑либо обстоятельства далеко не\\xa0всегда означает, что\\xa0ответ неполный или\\xa0некорректный; Critic LLM   может\\xa0быть сложно атомизировать юридический текст на\\xa0дискретные «факты», как\\xa0это делается для\\xa0более простых утверждений, и она может вести себя непредсказуемо при\\xa0оценке наличия набора таких «фактов» в\\xa0сгенерированном и «эталонном» ответах.  В\\xa0общем, в\\xa0юридической практике вариативность ответов на\\xa0юридические вопросы, особенно сложные вопросы, является абсолютно приемлемой, главное\\xa0— корректная грамотная юридическая оценка ситуации, которая, как\\xa0правило, не\\xa0зависит от\\xa0перечисления набора неких «фактов» в\\xa0ответе.  Подведем краткий итог обзора метрик  RAGAS  и ограничений их использования в\\xa0юридическом домене. Faithfulness и Context Precision\\xa0— основные метрики : с\\xa0учетом рассмотренных особенностей и ограничений использования метрик   RAGAS для\\xa0оценки RAG‑систем в\\xa0юридическом домене, оценка по\\xa0данным метрикам должна\\xa0быть в\\xa0приоритете. В\\xa0качестве  Critic LLM , если есть такая возможность, целесообразно использовать LLM, дообученную на\\xa0юридических текстах. Важность тщательной экспертной подготовки тестового датасета, включающего Ground Truth:  все метрики, кроме Faithfulness и Context Precision, очень требовательны к\\xa0формулировкам «эталонных» ответов. Тестовый набор данных для RAGAS   Сразу отметим, что\\xa0нам неизвестно о\\xa0принципах сборки тестовых датасетов для\\xa0юридического домена, в\\xa0том смысле, что\\xa0отсутствуют известные и общепризнанные публикации о\\xa0том, какие именно способности целесообразно учитывать при\\xa0автоматической оценке работы RAG‑системы в\\xa0юридическом домене.  Основываясь на\\xa0собственных знаниях и опыте, мы исходили из\\xa0важности и значимости следующего набора критических тестов, соответствующих типам вопросов в\\xa0датасете: 1. Проверка на  » галлюцинации » : Пример вопроса: « Какой статьей гражданского кодекса регламентируется продажа квартиры?» Пример ответа: « Квартира является разновидностью жилых помещений, продажа которых регламентируется статьей 558\\xa0ГК РФ. Эта статья описывает существенные условия договора продажи жилых помещений, а\\xa0также указывает на\\xa0необходимость государственной регистрации такого договора». 2. Проверка на\\xa0работу с\\xa0неклассическими случаями и пограничными примерами: Пример вопроса: « Является\\xa0ли футбольное поле недвижимым имуществом?» Значимые примеры в\\xa0юридической практике, как\\xa0правило, связаны с\\xa0неоднозначностью интерпретации того или\\xa0иного правила, что\\xa0приводит к\\xa0возникновению спора, а\\xa0спорными являются именно «пограничные» случаи. Пример ответа: « Футбольное поле, согласно правовой позиции, выраженной в\\xa0судебной практике Верховного Суда Российской Федерации … представляет собой улучшение земельного участка, заключающиеся в\\xa0приспособлении его для\\xa0удовлетворения нужд\\xa0лиц, пользующихся участком. Таким образом, футбольное поле не\\xa0обладает признаками самостоятельной недвижимой вещи, а\\xa0является неотъемлемой частью земельного участка, на\\xa0котором оно расположено». 3. Сравнение:  Сравнительный анализ используется при\\xa0проведении исследований, пожалуй, в\\xa0любой профессиональной сфере знаний. Пример вопроса:  «Кратко, со ссылкой на\\xa0применимое законодательство и судебную практику, напиши, чем отличается объект капитального строительства от\\xa0объекта недвижимого имущества». Пример ответа:  «Термин объект капитального строительства не\\xa0равнозначен термину объект недвижимого имущества. Определение термина объект капитального строительства даётся в\\xa0ст. 1\\xa0ГрК РФ, содержание категории объект недвижимого имущества раскрывается в\\xa0ст. 130\\xa0ГК РФ. Как\\xa0указано в\\xa0постановлении Президиума Высшего Арбитражного Суда РФ от 24\\xa0сентября 2013\\xa0г. N 1160/13: …». 4. Квалификация: Под\\xa0квалификацией мы понимаем  юридическую квалификацию , которая, в\\xa0общем, заключается в\\xa0соотнесении данного конкретного случая (описания набора фактов, обстоятельств) с\\xa0определенными юридическими нормами и применимой практикой, в\\xa0которой истолковываются такие нормы. Пример вопроса:  «С какого момента появляется земельный участок как\\xa0недвижимая вещь?» Пример ответа:  «В соответствии с\\xa0п. 1\\xa0ст. 141.2\\xa0ГК РФ земельным участком признается часть поверхности земли, границы которой определены в\\xa0порядке, установленном законом. … Таким образом, общий вывод таков: в\\xa0настоящее время земельный участок приобретает правовой режим недвижимой вещи с\\xa0момента первой государственной регистрации права собственности на\\xa0него. Однако это не\\xa0касается земельных участков, право собственности на\\xa0которые\\xa0было приобретено до\\xa0введения в\\xa0России системы государственной регистрации прав на\\xa0недвижимые вещи, а\\xa0также в\\xa0отношении земельных участков, находящихся в\\xa0публичной собственности …». Итак,  перечисленные выше способности \\xa0— (1) не\\xa0галлюцинировать при\\xa0ответе на\\xa0вопросы, требующие цитат из\\xa0законов или\\xa0решений судов или\\xa0каких‑либо фактов; (2) производить сравнительный анализ юридических аспектов объектов и явлений, которые приведены в\\xa0вопросе; (3) рассуждать и делать выводы как\\xa0настоящий юрист (юридическая квалификация), в\\xa0частности в (4) пограничных и неклассических случаях,\\xa0—  полагаем, это важные и значимые способности, которые должна демонстрировать RAG‑система, работающая в\\xa0юридическом домене. 1. В\\xa0рамках эксперимента нашей основной задачей\\xa0было отработать оценку системы с\\xa0точки зрения наличия у\\xa0нее описанных выше  » способностей », которые в\\xa0нашем тестовом датасете соответствуют типу вопроса (question_type),  всего\\xa0— 15\\xa0вопросов и развернутых ответов . Приоритет\\xa0был отдан вопросам, которые позволяют оценить наиболее критические ошибки в\\xa0работе системы\\xa0— «галлюцинациям» и неклассическим случаям квалификации: Nota bene!  При\\xa0использовании RAGAS важно учитывать, что\\xa0фреймворк активно будет использовать API   LLM (для Generator LLM & Critic LLM), что\\xa0означает\\xa0— каждый раз, когда создается или\\xa0оценивается эталонный пример, вызывается LLM (иногда многократно для\\xa0одной метрики).  Это может   привести к\\xa0довольно большим расходам на\\xa0генерацию токенов.  Поэтому в\\xa0начале, в\\xa0любом случае, лучше протестировать систему на\\xa0датасете небольшого размера. 2. Для\\xa0выявления зависимости производительности системы от\\xa0типа вопроса по\\xa0отношению к\\xa0имеющейся базе знаний, которую использует RAG, мы разделили все тестовые вопросы на\\xa0две группы: База знаний судебной практики , напомним, состоит из\\xa0текстов решений Верховного Суда Российской Федерации, в\\xa0которых рассматриваются вопросы неоднозначной квалификации объектов имущества, за\\xa0период: 2006-2023. Группа A :  8\\xa0целевых вопросов:  вопросы, которые напрямую касаются неклассической квалификации объектов имущества и, как\\xa0следствие, релевантны специализированной базе знаний судебной практики. Группа B :  7\\xa0смежных и общих вопросов  (остальные вопросы) \\xa0—  остальные   вопросы, которые не\\xa0имеют прямого отношения к\\xa0специализированной базе знаний, а\\xa0сосредотачиваются, например, на\\xa0процессуальных аспектах («Как подать иск …?») или\\xa0касаются смежных / общих правовых категорий / понятий. Nota bene!  При\\xa0использовании RAGAS важно учитывать, что\\xa0фреймворк активно будет использовать API   LLM (для Generator LLM & Critic LLM), что\\xa0означает\\xa0— каждый раз, когда создается или\\xa0оценивается эталонный пример, вызывается LLM (иногда многократно для\\xa0одной метрики).  Это может   привести к\\xa0довольно большим расходам на\\xa0генерацию токенов.  Поэтому в\\xa0начале, в\\xa0любом случае, лучше протестировать систему на\\xa0датасете небольшого размера. Сбор данных Учитывая, что\\xa0автор является практикующим юристом, датасет собирали собственными силами. При\\xa0этом  для\\xa0запуска системы в\\xa0прод или\\xa0при\\xa0ее масштабировании, очевидно, потребуется привлечение экспертов  из\\xa0предметной области: на\\xa0первом этапе, на\\xa0наш взгляд, выборка должна, в\\xa0любом случае, состоять из\\xa0только из\\xa0проверенных экспертами вопросов‑ответов. Впоследствии данная выборка может составить «ядро», которое можно масштабировать, например, с\\xa0использованием правил для\\xa0генерации . В\\xa0рамках такого подхода можно задать набор правил или\\xa0шаблонов для\\xa0создания синтетических эталонных данных, а\\xa0затем привлечь экспертов для\\xa0заполнения этих шаблонов. Используя знания в\\xa0предметной области и заранее определенные паттерны, можно создавать ответы, соответствующие ожидаемому формату и содержанию (эксперты заполняют только данные, соответствующие различным комбинациям пользовательских проблем и решений; эти проблемы и решения затем автоматически вставляются в\\xa0шаблоны вопросов и ответов). В\\xa0заключение также отметим, что\\xa0мы намеренно (дабы не\\xa0подгонять его под\\xa0метрики, а\\xa0наоборот\\xa0— как\\xa0можно сильнее приблизить к\\xa0реальности) включили в\\xa0датасет : довольно каверзные и непростые вопросы, даже для\\xa0практикующего юриста  (например, на\\xa0вопрос\\xa0— «С какого момента появляется земельный участок как\\xa0недвижимая вещь?», можно ответить очень по‑разному, и, в\\xa0целом, он не\\xa0имеет строгого и однозначного толкования); вопросы, которые требуют от\\xa0модели не\\xa0только знаний, которые соответствуют поддомену   нашей базы знаний  (регулирование недвижимого имущества),  но\\xa0и общих знаний , связанных, например, с\\xa0подачей искового заявления в\\xa0суд. Результаты оценки Сначала посмотрим на\\xa0сводные средние оценки по\\xa0всем вопросам и метрикам: Анализ выявляет несколько ключевых тенденций в\\xa0работе системы: 1. Среднее значение   ключевой метрики  ( 0.88 )\\xa0—  Faithfulness  достигло высокого показателя при\\xa0относительно небольшом разбросе оценок в\\xa0пределах от 0.8\\xa0до 1\\xa0(2\\xa0«выброса» с\\xa0невысокими оценками). В\\xa0целом, это означает, что\\xa0когда система получает релевантный контекст, даже сравнительно небольшая модель gpt4O‑mini практически не «галлюцинирует». Она воспроизводит факты, взятые непосредственно из\\xa0извлеченных документов, что\\xa0делает ответы надежными с\\xa0точки зрения их источника. 2. Среднее значение   метрики Context Recall  ( 0.77 ) также является достаточно высоким, при\\xa0этом оценки по\\xa0большинству вопросов (2/3) достигают наивысшего значения, однако 3\\xa0ответа, согласно показателям, вообще не\\xa0опираются на\\xa0контекст. В\\xa0целом, базы знаний законодательства и судебной практики адекватно покрывают предметную область большинства вопросов, и семантический поиск работает корректно в\\xa0рамках своей специализации. Однако имеющиеся среди оценок «выбросы» с\\xa0нулевыми значениями свидетельствуют о\\xa0том, что\\xa0ретривер при\\xa0обработке некоторых вопросов извлекает совсем нерелевантный контекст. В\\xa0таких условиях gpt4O‑mini, будучи небольшой, не\\xa0обладает достаточными обобщающей и аналитической (рассуждающей) способностями, чтобы самостоятельно делать сложные юридические выводы из\\xa0неоднозначного или «грязного» контекста. 3.   Среднее значение   метрики Context Precision  ( 0.68 ), отличающееся в\\xa0меньшую сторону от\\xa0Context Recall, а\\xa0также довольно высокая степень разброса оценок по\\xa0вопросам, говорят о   том, что\\xa0при\\xa0обработке некоторых вопросов  ретривер извлекает  «шумный» контекст. Распределение оценок характеризуется значительной левосторонней асимметрией (Me > X ср ): в 50% случаев метрика принимает высокие значения (более 0.8), в\\xa0остальных случаях показатели метрики крайне нестабильны (разброс от 0\\xa0до 0.58). 4. Среднее значение   метрики Answer Relevancy является низким  ( 0.46 ), при\\xa0этом распределение оценок, так\\xa0же, как\\xa0и в\\xa0случае с\\xa0Context Precision, характеризуется значительной левосторонней асимметрией (Me > X ср ): 8\\xa0из 15\\xa0ответов получили оценки выше 0.6, что\\xa0дает основания видеть тенденцию к\\xa0увеличению показателя. Выше мы указывали на\\xa0то, что\\xa0данная метрика должна оценивать очень осторожно, так как\\xa0модель может истолковать избыточность ответа как\\xa0нераскрытие релевантности, что\\xa0особенно актуально для\\xa0ответов, полученных при\\xa0учете «шумных» контекстов. 5.   Среднее значение   метрики Answer Correctness  ( 0.53 ), при\\xa0наиболее симметричном распределении оценок с\\xa0наименьшим разбросом, является, на\\xa0наш взгляд, вполне допустимым. Метрика ведет себя, в\\xa0целом, стабильно во\\xa0всех случаях, не\\xa0сильно изменяясь от\\xa0качества извлеченного контекста. Выше мы отмечали, что\\xa0особенность юридических ответов не\\xa0практике состоит в\\xa0том, что\\xa0воспроизводство фактологической составляющей совсем не\\xa0коррелирует с\\xa0их качество. Если к\\xa0этому прибавить абсолютно допустимую вариативность ответов на\\xa0неоднозначные вопросы, а\\xa0также в\\xa0ряде случаев\\xa0— «шумный» контекст и очевидные ограничения модели gpt4O‑mini, значение метрики является допустимым. Наши наблюдения подтверждаются данными корреляции метрик. Answer Correctness заметно коррелирует только с\\xa0точностью контекста (Context Precision). Также отметим высокое значение корреляции Faithfulness с\\xa0полнотой контекста (Context Recall). Теперь посмотрим на\\xa0распределение  средних значений  наиболее интересных для\\xa0нас метрик ( Context Precision, Context Recall, Faithfulness )  по\\xa0всем типам вопросов : « Галлюцинации » (смежные или\\xa0общие вопросы, не\\xa0опирающиеся прямо на\\xa0базу знаний судебной практики);  « Пограничные случаи »; « Квалификация объектов »; « Сравнение » (вопросы, требующие сравнения правовых понятий и категорий, в\\xa0том числе не\\xa0входящих в\\xa0базу знаний судебной практики).  Производительность ретривера  (context_precision и context_recall): графики подтверждают, что\\xa0ретривер RAG‑системы демонстрирует выдающуюся производительность на\\xa0вопросах, которые находятся в\\xa0центре его специализации\\xa0— квалификация и пограничные случаи; напротив, для\\xa0вопросов, требующих сравнения или\\xa0имеющих «потенциал к\\xa0галлюцинациям», производительность ретривера резко падает. Средний context_precision составляет всего 0.33\\xa0и 0.37\\xa0для\\xa0этих категорий, что\\xa0говорит о\\xa0серьезном загрязнении контекста. Это наглядно иллюстрирует, что\\xa0базовый семантический поиск с\\xa0базовыми элементами архитектуры RAG‑системы и относительно слабой моделью генерации, пока не\\xa0способен эффективно работать с\\xa0запросами, выходящими за\\xa0рамки узкой специализации, опирающейся на\\xa0базу знаний. Обоснованность ответов  (faithfulness): модель gpt4O‑mini в\\xa0целом хорошо справляется с\\xa0задачей «оставаться в\\xa0рамках» предоставленного контекста. Интересно, что\\xa0даже в\\xa0тех случаях, когда контекст «загрязнен» («Сравнение»\\xa0— 0.82, «Галлюцинации»\\xa0— 0.81), модель в\\xa0большинстве случаев пытается опираться на\\xa0него, что\\xa0подтверждает её склонность к\\xa0grounding, а\\xa0не\\xa0к\\xa0генерации «с нуля». Анализ производительности системы на\\xa0целевых вопросах  показывает, что\\xa0она демонстрирует высокие или\\xa0приемлемые результаты по\\xa0ключевым метрикам для\\xa0RAG‑системы, даже при\\xa0базовых элементах архитектуры: Faithfulness : среднее значение этой метрики достигает почти идеального показателя ( 0.95 ), что\\xa0является одним из\\xa0самых важных результатов, так как\\xa0система воспроизводит факты, взятые непосредственно из\\xa0извлеченных документов, что\\xa0делает ответы надежными с\\xa0точки зрения их источника, а\\xa0это очень важно для\\xa0юридического домена. Context Recall : среднее значение также является достаточно высоким\\xa0—  0.82 , ретривер в\\xa0подавляющем большинстве случаев успешно находит хотя\\xa0бы один релевантный документ, содержащий необходимую информацию для\\xa0ответа. Context Precision : средний показатель  0.76  существенно выше производительности на\\xa0смежных / общих вопросах (0.58) и говорит о\\xa0том, что\\xa0ретривер, хотя и с\\xa0некоторыми ошибками, способен фильтровать «шум» и предоставлять модели относительно чистый контекст. Answer Relevancy : парадоксально, но\\xa0этот показатель для\\xa0целевых вопросов ( 0.35 ) значительно ниже, чем для\\xa0смежных / общих. Полагаем, этот феномен объясняется тем, что\\xa0на\\xa0прямые вопросы, на\\xa0которые нет однозначного ответа, система, в\\xa0отличие от\\xa0экспертов, склонна давать развернутые и избыточные ответы. Answer Correctness : среднее значение\\xa0—  0.54  фактически не\\xa0отличается от\\xa0значения для\\xa0смежных / общих вопросов.  Проведенный сравнительный анализ распределения метрик по\\xa0целевым и смежным / общим вопросам демонстрирует, что\\xa0RAG‑система с\\xa0архитектурой, опирающейся на\\xa0базы знаний законов и судебной практики, даже в\\xa0своей базовой версии, является надежным инструментом в\\xa0тех случаях, когда она опирается на\\xa0базы знаний, то есть в\\xa0узкой области своей специализации. При\\xa0этом ее производительность может падать при\\xa0выходе за\\xa0пределы этой области. При\\xa0этом даже небольшая gpt4O‑mini, в\\xa0качестве компонента RAG‑системы, очень неплохо справляется с\\xa0задачей «обоснования» ответов, качество которых остается, в\\xa0этом отношении, приемлемым даже нецелевых вопросов. Предложенная архитектура системы в\\xa0базовом варианте позволила достичь приемлемых результатов работы ретривера, несмотря на\\xa0непростые экспертные вопросы в\\xa0тестовом датасете. ВЫВОДЫ и возможные \"точки роста\": 1. Предложенная архитектура «Law & Practice Ensemble RAG» даже в\\xa0базовом варианте продемонстрировала неплохие результаты при\\xa0решении обозначенных выше ключевых проблем работы систем на\\xa0основе RAG в\\xa0юридическом домене : специализированное чанкирование , основанное на\\xa0кастомных правилах, учитывающих структуру статей законов и разделов судебных решений, помогает избежать потери контекста на\\xa0границах разделения фрагментов; гибридный подход  (разделение индексов и ретриверов по\\xa0поддоменам) позволяет реализовать поиск независимо по\\xa0наиболее значимым поддоменам (законов и судебной практики); взвешенное ансамблирование извлеченных фрагментов законов и судебной практики вместе с\\xa0RRF‑переранжированием итоговых результатов поиска  позволяет не\\xa0просто найти релевантные документы, а\\xa0отранжировать их с\\xa0учётом экспертной важности, что\\xa0напрямую влияет на\\xa0качество генерируемого ответа; решение проблемы многоаспектности запросов с\\xa0помощью MultiQueryRetriever , который автоматически генерирует альтернативные формулировки запросов, значительно расширяет охват поиска, снижая зависимость от\\xa0точности изначального запроса; гибкие возможности настройки параметров поиска на\\xa0уровне пользователя и разработчика  (возможность легко и\\xa0быстро регулировать значимость поддоменов при\\xa0ансамблировании, а\\xa0также количество используемого контекста) позволяют оперативно настраивать систему для\\xa0решения различных задач с\\xa0необходимой точностью и (или) стоимостью генерации токенов, а\\xa0также согласуются с\\xa0практикой проверки ответов юристами; профессиональный вывод с\\xa0использованием кастомных промптов  позволяет   моделировать структуру реального юридического заключения. 2. Эксперимент показал также и ограничения предложенной архитектуры в\\xa0базовом виде, но\\xa0мы видим «точки роста »: Усовершенствование модуля поиска . Базовый семантический поиск, даже в\\xa0ансамбле, не\\xa0всегда может отсеять «шум» в\\xa0извлекаемом ретривером контексте. Для\\xa0решения этой проблемы можно (все нужно отдельно тестировать): внедрить классический гибридный поиск : комбинация семантического поиска с\\xa0традиционным поиском по\\xa0ключевым словам (BM25) может повысить context_precision на\\xa0вопросах, содержащих точные ссылки на\\xa0статьи, номера дел и другие фактологические данные, которые плохо улавливаются семантическими эмбеддингами; использовать более современную и специализированную embedding‑модель  (при необходимости и наличии ресурсов\\xa0— дообучить на\\xa0юридических данных). Усовершенствование модуля генерации . Результаты эксперимента (невысокая, но\\xa0стабильная Answer Correctness) говорят о\\xa0том, что\\xa0даже с\\xa0идеальным контекстом gpt4O‑mini не\\xa0способна к\\xa0глубокому юридическому анализу и рассуждению. Возможные направления развития: Fine‑tuning на\\xa0юридическом домене : например, дообучение модели на\\xa0размеченном корпусе юридических пар\\xa0— «вопрос‑ответ» гипотетически может позволит ей не\\xa0просто воспроизводить факты, но\\xa0и научиться их синтезировать при\\xa0моделировании юридического рассуждения; использование более мощной и крупной LLM  (например, GPT-4o или\\xa0Gemini 1.5\\xa0Pro,\\xa0либо русскоязычные модели\\xa0— YandexGPT и GigaСhat), дабы использовать более высокие рассуждающие способности (Reasoning) модели генерации. Дополнение архитектуры агентными модулями  (требует проведения предварительного исследования, самостоятельность агентам в\\xa0юридической сфере необходимо давать постепенно, начав с\\xa0базовых инструментов, а‑ля проверка актуальной даты). Агентная архитектура позволит отойти от\\xa0парадигмы «один запрос\\xa0— один ответ» к\\xa0многоэтапному, более сложному поиску и анализу, в\\xa0рамках которого система способна будет планировать стратегию поиска, привлекать дополнительные инструменты (веб‑поиск, калькуляторы и пр.), а\\xa0также организовывать многократные шаги запроса/поиска: планирование действий и принятие решений : в\\xa0контексте юридического ассистента это может означать, например, что\\xa0бот сам определит, какие дополнительные уточняющие вопросы задать или\\xa0какие базы ему подключить; декомпозиция сложного запроса : например, обрабатывая вопрос «Как изменилась судебная практика по\\xa0квалификации газопроводов за\\xa0последние 5\\xa0лет?», агент мог\\xa0бы сначала найти все релевантные судебные решения, затем проанализировать их хронологически, выявить ключевые изменения в\\xa0аргументации и\\xa0лишь после этого сгенерировать итоговый вывод; работа с\\xa0противоречиями : агент мог\\xa0бы распознавать противоречивые судебные решения и предупреждать пользователя о\\xa0наличии неоднозначной практики, а\\xa0не\\xa0просто выбирать один из\\xa0вариантов. Интеграция с\\xa0графовыми базами знаний: для\\xa0повышения качества извлекаемого контекста   вместо (или в\\xa0дополнение) простого поиска по\\xa0семантической схожести или\\xa0по\\xa0ключевым словам, можно реализовать поиск по\\xa0графовой базе знаний ( GraphRAG ); такая база знаний представляет собой связанные между собой юридические сущности, автоматически извлеченные из\\xa0текстов (например, с\\xa0помощью  GraphRAG  от\\xa0Microsoft), или\\xa0юридические онтологии ( Legal Ontologies ); поиск по\\xa0такой базе знаний позволит системе, например, использовать знание о\\xa0том, что «сделка с\\xa0недвижимостью» связана с «правом собственности» и «регистрацией», что\\xa0может помочь модулю поиска выбирать релевантные законы и кейсы через навигацию по\\xa0графу; полагаем, такая база знаний позволила\\xa0бы системе ответить на\\xa0вопрос: «Какой судебный прецедент повлиял на\\xa0толкование статьи 130\\xa0ГК РФ?» Векторный поиск не\\xa0может этого сделать, так как\\xa0он не\\xa0хранит информацию о\\xa0взаимосвязях.  Расширение и уточнение датасета для\\xa0оценки системы. Предложенная архитектура имеет очевидный потенциал расширения на\\xa0мультидоменные задачи в\\xa0рамках юридической предметной области. Текущая реализация заточена под\\xa0недвижимость, но\\xa0подход имеет потенциал к\\xa0масштабированию: следующим шагом логично добавить другие сферы регулирования в\\xa0рамках гражданского права, далее\\xa0— отрасли права (семейное, налоговое, трудовое и\\xa0т.\\xa0д.). Каждой области знаний можно выделить собственный индекс и веса ансамбля, что\\xa0потребует\\xa0лишь квалифицированного подбора параметров. Однако с\\xa0ростом числа поддоменов возникнет задача координации их между собой и возможного кросс‑индекса. Теоретически, такая мультидоменная система могла\\xa0бы\\xa0быть реализована на\\xa0основе архитектуры GraphRAG, где доменные индексы\\xa0были\\xa0бы связаны через различные агентные стратегии поиска по\\xa0графам знаний. Кроме того, мультидоменность подразумевает необходимость более гибкой генерации запросов и, возможно, персонализации ответов в\\xa0разных областях. Все это потребует гораздо больших ресурсов, чем описанная RAG‑система.  Но\\xa0во всем важен первый шаг! Работа над юридическими RAG-системами показывает: одной лишь теории про бенчмарки и архитектуру недостаточно. Чтобы реально создать ассистента, способного работать с правовыми документами, нужно разбираться и в обработке языка, и в методах машинного обучения, и в том, как соединять разные компоненты в целостную систему. Для этого подойдут специализированные курсы. На курсе  NLP / Natural Language Processing  вы подробно изучите работу трансформеров, семантический поиск, методы построения QA-систем и обработку текстов в разных предметных областях. Курс  Machine Learning. Professional  поможет системно закрепить знания по машинному обучению и глубинным моделям, что особенно важно для разработки и оценки сложных прикладных решений, включая RAG-ассистентов. Все программы можно посмотреть в  каталоге курсов .', hub='data mining'), Page(id='946422', title='3 книги для развития аналитического мышления и вдохновения', content='Иногда всем нам, аналитикам (да и не только), хочется просто взять и почитать интересную книгу. И тут важно, чтобы она не только увлекала и помогала узнать что-то новое, но и не перегружала сложными формулами, не заставляла застревать на пятой странице, вдумываясь в каждый символ, и не утомляла за выходные сильнее, чем целая рабочая неделя. Задача кажется нетривиальной, но я выделил для себя несколько интересных книг, которые помогут развить аналитические навыки, выработать простые стратегии выбора в современном динамичном мире и просто дадут набор инструментов для «взлома реальности».\\xa0 Интересно? Тогда давайте знакомиться, меня зовут Андрей Смирнов, я риск-аналитик в  Garage Eight . И в этом посте я собрал небольшую подборку книг на выходные.\\xa0 «Теория игр. Искусство стратегического мышления в бизнесе и жизни» Барри Дж. Нейлбафф, Авинаш Диксит Что эта книга может дать: Это простые, но поучительные истории о том, как сделать оптимальный выбор при решении ежедневных задач. Некоторые решения будут понятны интуитивно, но некоторые заставят вас задуматься и пересмотреть свой взгляд на происходящее вокруг. Теория игр — это отмычка для «взлома реальности». Не стоит ждать математических выкладок или очень сложных схем. Книга скорее для того, чтобы было интересно и весело научиться делать оптимальный выбор в простых ситуациях, а дальше все зависит от уровня вашего креатива. «Думай медленно, решай быстро» лауреата Нобелевской премии Даниэля Канемана. Эта книга, возможно, станет открытием, если вы ещё не успели её прочесть. Что эта книга может дать: Вы явно когда-либо задавались вопросом, почему люди поступают так или иначе, восклицали: «Это же нелогично!», а потом сами оказывались на месте людей, принимавших откровенно «неудачные решения». Это фундаментальный труд, в котором подробно описаны когнитивные искажения, сопровождающие нашу жизнь, а также сценарии поведения и оптимальные решения. В книге автор привёл множество примеров, их столько, что в них легко можно заблудиться. Там каждый сможет найти что-то полезное, главное — делайте пометки, иначе забудете через несколько страниц, где уже новые примеры захватят внимание. Некоторые вещи не столько полезны, сколько поражают своей природой и запоминаются на всю жизнь. В общем, книга обязательно произведет впечатление, и лучше обзавестись отдельным канвасом в Obsidian или выделить под неё отдельную тетрадку, чтобы делать записи.\\xa0 «Fortune’s Formula», написанная Уильямом Паундстоуном.  Увы, книга только на английском, русского перевода я не встречал. Книга раскрывает интересные детали того, как физики и математики покорили вершины фондового рынка, привнесли подход, основанный на расчётах и количественных методах, с опорой на вычислительные мощности и фактически положили начало современной IT-индустрии. Что может дать эта книга: Это вдохновляющая история о том, как несколько выдающихся ученых и практиков сначала придумали теорию информации, а потом смогли применить свои знания в реальной жизни, делая то, что считалось невозможным. Победить казино в блэкджек? — Легко. Разработать и собрать первый портативный компьютер? — Сделано. Придумать и реализовать стратегию победы в рулетку? — Почему бы и нет. Стать одними из самых успешных инвесторов Уолл-стрит? — Давайте. И связывает все эти события формула, предложенная их коллегой по цеху, а именно формула Келли, которая теперь считается обязательной к изучению в теории игр и количественной аналитике. Книга проводит читателя по технологической истории США начиная с Bell Labs и зарождения теории информации, показывая, как она была использована, с вдохновляющими историями, примерами и сравнениями расчёта некоторых параметров, а также с различными теориями, приправленными интригами и приключениями, по которым снято множество фильмов. На первый взгляд все эти книги — обычные истории, ненаучные, они не изобилуют теоремами, доказательствами и формулами. Наличие их в резюме не даст преимущества на рынке труда, а знания, которые в них отображены, могут показаться поверхностными и не дающими конкретных инструментов здесь и сейчас (хотя не про всё так можно сказать).\\xa0 Но суть не в этом. Это истории, которые показывают, что нет предела совершенству, что даже простые вещи могут дать преимущество и что не существует предела, кроме того, который мы сами себе установили. Если вы уже читали что-то из этих книг, буду рад обменяться впечатлениями в комментариях. А также буду ждать ваши рекомендации для вдохновляющего чтения.\\xa0', hub='читальный зал')]","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:40.426971","level":"info","event":"В цикле: 3","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:40.427072","level":"info","event":"URL: https://habr.com/ru/articles/page3","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:41.202941","level":"info","event":"Result: <Response [200]>","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:53.468015","level":"info","event":"Feed: pages=[Page(id='946392', title='Секреты LLM по API: динамическая анонимизация данных для российского бизнеса', content='Привет, Хабр! На связи Мария Старцева и Катерина Пославская из команды R&D в Just AI. Наша компания специализируется на NLP технологиях, и мы помогаем клиентам трансформировать их бизнес-процессы через внедрение умных ассистентов и сложных мультиагентных систем. Один из наиболее востребованных запросов — модель должна крутиться локально (on-premise). Обращение к удалённым закрытым моделям по API несет скрытую угрозу: данные из пользовательских запросов могут сохраняться и попадать в последующие ответы модели, а если речь идёт ещё и о конфиденциальных персональных данных... мысли о таком повороте событий лишили сна уже не одного безопасника!\\xa0 При этом именно проприетарные LLM модели чаще всего демонстрируют SOTA качество для большинства задач. И вот заказчик сталкивается с дилеммой: либо внедрять топовые инструменты, либо рисковать безопасностью, выпуская информацию из контура организации. А тут ещё усилилось государственное давление по защите персональных данных: внесены изменения в\\xa0 152-ФЗ «О персональных данных» и в КоАП РФ (ст. 13.11) . В 2025 г. существенно повысилась административная ответственность за нарушения в области персональных данных, особенно, в разрезе трансграничной пересылки данных пользователей. При этом размер штрафов за нарушение измеряется миллионами и даже процентами от годовой выручки компании. Один крупный инцидент способен многократно превысить затраты на внедрение средств защиты. А ведь кроме прямых финансовых рисков существуют и косвенные, вызванные репутационными/имиджевыми потерями.\\xa0 Произошёл сдвиг парадигмы: комплексная анонимизация перестает быть просто статьей расходов на соответствие требованиям, а становится финансово обоснованной стратегией защиты капитала компании. Как устроена динамическая анонимизация данных Как обеспечить сотрудникам доступ к мощным внешним LLM-сервисам и при этом не допустить утечки конфиденциальной информации? Оптимальным архитектурным решением стал промежуточный прокси-сервер, который перед отправкой запроса во внешние API выполняет анонимизацию «на лету». Такой подход с использованием Personally Identifiable Information (PII) filtering/redacting proxy стал стандартом динамической защиты данных при работе с внешними LLM. Его принцип работы следующий: Перехват (Interception): \\xa0запрос пользователя в интерфейсе чат-бота перехватывается специальным прокси-сервером до отправки во внешнюю модель. Обнаружение (Detection): \\xa0прокси анализирует текст запроса и ищет персональные данные. Для этого используется комбинация методов: • Распознавание именованных сущностей (задача Named Entity Recognition, NER): выделение имен (PER), организаций (ORG), геолокаций (LOC) и других;\\xa0 • Регулярные выражения: поиск структурированных шаблонов (для номеров телефонов, URL, emails, паспортных данных, ИНН, банковских карт);\\xa0 • Правила и словари: обнаружение специфичных для компании терминов, кодов, артефактов.\\xa0 Трансформация (Transformation): \\xa0все найденные фрагменты, содержащие персональные данные, заменяются на специальные метки-плейсхолдеры. Одновременно сохраняется таблица соответствия (lookup table) между оригинальными значениями и сгенерированными метками. Безопасная отправка (Secure Forwarding): \\xa0очищенный от реальных данных запрос уходит во внешнее LLM API: реальные персональные данные остаются в контуре организации (см. Рисунок 1). Деанонимизация ответа (De-Anonymization): \\xa0получив ответ от LLM, прокси возвращает исходные значения на место плейсхолдеров, используя ранее сохраненную таблицу. Сотрудник получает осмысленный ответ с реальными названиями/именами, даже не подозревая, что исходный промпт обрабатывался искаженным. Рис. 1. Схема анонимизации персональных данных в реальном времени Подобный\\xa0 анонимизирующий прокси-шлюз \\xa0выступает надёжным и контролируемым барьером между внутренними пользователями и внешним ИИ-сервисом. Однако он не может быть единственным средством защиты от утечек и его следует использовать в комплексе с иными техническими и организационными внутрикорпоративными мерами защиты. В этом случае оператор покажет, что предпринимает\\xa0 разумные и достаточные меры защиты. Jay Guard – шлюз безопасности для LLM Под задачу анонимизации и фильтрации чувствительных данных в работе с закрытыми LLM мы в Just AI разработали\\xa0 Jay Guard. \\xa0Он представляет собой интеллектуального «охранника», который минимизирует риски использования проприетарных моделей (ChatGPT, GPT-4, Claude, Google Gemini), в том числе российские решения (YandexGPT, Sber GigaChat). Как это работает на практике? Jay Guard выступает\\xa0 умным прокси \\xa0между клиентом и моделью: когда сотрудник отправляет в чат запрос, Jay Guard перехватывает его и сканирует на наличие персональных данных.\\xa0 Здесь мы используем\\xa0 гибридный метод поиска: \\xa0сочетаем регулярно обновляемый набор правил (регулярные выражения, словари) и модели машинного обучения для задачи NER. Обнаружив в тексте, например,\\xa0 ФИО, телефон или адрес , Jay Guard не удаляет их полностью, а заменяет псевдонимами — специальными метками-плейсхолдерами (placeholders), сохраняющими структуру предложения. Например, фраза: «Клиент Иван Иванов, телефон 8-926-123-45-67, просит выставить счет» превратится в: «Клиент [PERSON_1], телефон [PHONE_1], просит выставить счет». Обезличенный запрос уходит в LLM. Модель, получив синтаксически корректный текст, легко понимает, что речь о клиенте и его номере, и нормально обрабатывает запрос. Для дополнительного регулирования работы LLM мы включаем в системный промпт пояснение, как интерпретировать placeholders. Когда ответ от LLM возвращается, Jay Guard выполняет обратную операцию —\\xa0 деанонимизацию,\\xa0 подставляя оригинальное имя и телефон обратно в текст ответа. Альтернативные решения Мы не единственные, кто решает проблему обезличивания данных. Альтернативы есть, и у каждого решения свои ниша и профиль. Фокус продуктов может быть как на обезличивании данных в таблицах и статических базах данных (ARX Data Anonymization Tool, Amnesia или отечественный D-Mask), так и на изображениях (модуль графической анонимизации в Microsoft Presidio). Также решения могут ориентироваться на конкретные домены (медицину, финансовые данные и прочее). Наших клиентов интересует использование Jay Guard в виртуальных ассистентах, поэтому нам близок сценарий с текстовыми данными.\\xa0 На рынке достаточно много и open-source, и проприетарных решений для динамического обезличивания текстов (см. Таблицу 1). Некоторые из них ориентированы на специфический домен клинических или юридических данных (AInonymize, Docbyte). Другие поддерживают только английский язык (Strac, Digital Guardian и другие). А решения, которые позиционируются многоязычными, как правило, плохо работают с русским: они требуют либо дообучения (open-source LLM Guard, Microsoft Presidio), либо дополнительной кастомизации проприетарных систем (поскольку качество работы с русским неизвестно). Например, dslim/bert-large-NER, который стоит по умолчанию в LLM Guard, файнтьюнилась на англоязычном датасете, из-за чего для работы в русском тексте она, увы, не годится. Таблица 1. Сравнительная таблица решений для динамического обезличивания текстовых данных Инструмент Тип лицензии Ключевые методы анонимизации Поддержка языков Интеграция с LLM/API Комментарий LLM Guard Open-Source dslim/bert-large-NER, Regex Многоязычная REST API, Python SDK Требует дообучения моделей Microsoft Presidio Open-Source spaCy, flair, HF Transformers, Regex Многоязычная REST API, Python SDK Требует дообучения для доменных задач NVIDIA NeMo Guardrails Open-Source Miscrosoft Presidio, Regex Многоязычная Python SDK\\xa0 Фреймворк оркестратор для комплексной защиты Guardrails AI Open-Source Miscrosoft Presidio, Regex Многоязычная REST API, Python SDK Низкая производительность (доп. циклы LLM), сложность настройки Amazon Bedrock Guardrails Проприетарный AI/ML, Regex Многоязычная API Amazon Bedrock, ApplyGuardrail API, интеграция с агентами Bedrock, RAG-системами\\xa0 Привязка к экосистеме AWS, стоимость использования, ограниченная кастомизация по сравнению с open-source решениями Hive Trace Проприетарный Regex, AI/ML(?) Многоязычная REST API Комплексный подход к защите генеративных систем Lakera Guard Проприетарный AI/ML, Regex Многоязычная REST API Комплексный подход к защите генеративных моделей/систем, высокая скорость работы\\xa0 Eden AI API Проприетарный AI/ML, OCR (для изображений), шаблоны Многоязычная REST API, Python Платный, зависимость от провайдера ManageEngine DLP Проприетарный Шаблоны, политики доступа, мониторинг Многоязычная REST API, Outlook, агенты на устройства Нет облачной версии Docbyte Проприетарный AI/ML, OCR, обработка изображений и текста Многоязычная REST API Ориентация на медицинские/юридические тексты Strac Проприетарный AI/ML в рамках no-code платформы Английский REST API, no-code Высокая стоимость, ограниченная кастомизация Digital Guardian DLP Проприетарный DLP-политики, ML, контроль доступа Английский SaaS, API, интеграция с облаками Сложная настройка, высокая цена AInonymize (Gramener) Проприетарный Mistral 7B fine-tuned на клинических данных Английский REST API Специализирован под медицинские данные Nightfall Security Проприетарный ML, обнаружение PII в облачных сервисах Английский REST API, Python SDK Высокая цена Кроме того, в отличие от Jay Guard ни одно из существующих решений не учитывает стандарты российских документов, например, форматы паспорта РФ, СНИЛС, ИНН и других. Поэтому использование альтернативных систем потребует не только дообучения NER-моделей, но и создания собственных шаблонов правил regex.\\xa0 Jay Guard легко интегрируется с корпоративными ботами «из коробки», поэтому во многих сценариях компаниям становится выгоднее использовать готовый продукт с поддержкой и обновлениями, чем собирать свой PII прокси из отдельных библиотек и заниматься настройкой и дообучением.\\xa0 Jay Guard под капотом: выбор модели NER Центральный компонент Jay Guard – движок NLP, который находит целевые именованные сущности (например, имена людей, адреса) в тексте. От его качества напрямую зависит надежность анонимизации:\\xa0 пропущенная сущность = утечка персональных данных,\\xa0 а ложное срабатывание может исказить смысл запроса. Для проверки качества моделей на наших задачах мы создали бенчмарк внутренних данных и прогнали на нём популярные решения «из коробки». В этой статье мы представляем метрики только по публичной (анонимизированной) версии этого бенчмарка, чтобы любой мог ознакомиться с форматом данных и воспроизвести наши метрики. Датасет залит на Hugging Face:\\xa0 https://huggingface.co/datasets/just-ai/jayguard-ner-benchmark . Результаты показали, что идеального решения для наших сложных данных не существует (см. Таблицу 2). Все модели без дообучения, включая модели, созданные специально для русского языка, а также модели, встроенные в топовые open-source системы (Hugging Face Transformers, spaCy и flair), недостаточно хорошо справляются с задачей поиска имён и адресов в сложных данных — рабочих чатах, логах обращений, разговорной речи.\\xa0 Таблица 2. Сравнение качества модели на нашем публичном датасете Модель Язык Лейбл Person F1/Precision/Recall Лейбл Street_Address F1/Precision/Recall Natasha (SlovNet) русскоязычная 0.64 / 0.63 / 0.68 0 / 0 / 0 spaCy (ru_core_news) русскоязычная 0.73 / 0.73 / 0.76 0 / 0 / 0 spaCy (xx_ent_wiki_sm) мультилингвальная 0.43 / 0.34 / 0.62\\xa0 0.06 / 0.03 / 0.24 DeepPavlov/ruBERT-base-cased русскоязычная 0.58 / 0.59 / 0.59 0.41 / 0.40 / 0.42 flair мультилингвальная 0.86 / 0.86 / 0.87 0.47 / 0.47 / 0.48 GLiNER мультилингвальная 0.45 / 0.44 / 0.53\\xa0 0.46 / 0.46 / 0.46 Одни решения (Natasha, spaCy) излишне осторожны и пропускают важные сущности, другие (flair, GLiNER) — берут почти всё подходящее, но при этом захватывают кучу лишнего. Для нашего прокси нам нужна была «золотая середина» — максимально полный поиск персональных данных при приемлемом уровне ложных срабатываний. Поэтому мы решили взять\\xa0 flair\\xa0 и\\xa0 spaCy \\xa0— лучших кандидатов по метрике полноты Recall для лейбла PERSON, и обучить их распознавать только те сущности, которые подходят нашим клиентам.  К слову, кроме базового качества разметки, мы учитывали производительность моделей (Рисунок 2). Из-за больших задержек мы исключили модель GLiNER, хоть она и имела неплохой потенциал в определении целевых классов. Рис. 2. Сравнение производительности моделей Отдельным достоинством spaCy, помимо топовой производительности на GPU, является способность модели эффективно работать на CPU мощностях. Для дообучения мы выбрали мультилингвальную версию модели, поскольку нам нужно распознавать сущности в текстах, где присутствует как кириллица, так и латиница. Работа с реальными данными В создании размеченных данных для дообучения моделей есть несколько ключевых тонкостей, которые, к сожалению, мало обсуждаются в отчётах, докладах и статьях. Главные из них — определение границ сущностей, создание к ним инструкций с примерами, а также постоянная валидация согласованности ручных разметок специалистов. Поскольку самыми ключевыми для динамической анонимизации для нас являются сущности “личность” (PERSON) и “адрес” (STREET_ADDRESS), в примерах ниже мы остановимся именно на них. Сущности и их границы Без единых подробных правил разметки два носителя языка или даже один и тот же специалист в разное время будут по-разному классифицировать одни и те же случаи. Поэтому, во-первых, требовалось чётко и однозначно сформулировать определение сущностей и их границы.\\xa0 Например, нам было нужно ответить на вопрос, являются ли “личностями” вымышленные персонажи или публичные персоны?\\xa0 Вносим ли мы в адрес типа “улица Ленинский проспект, д.144к5, кв. 13” слово “улица” или его аналоги (“ул.”, “просп.”, “бульвар” и другие). А также размечаем ли весь адрес одной сущностью или делим на составляющие (например, 3 сущности: “улица Ленинский проспект”, “д.144к5”, “кв. 13”)?\\xa0 Как мы поступаем с омонимией, в контекстах типа «улица Пушкина» против «стихи Пушкина»? У нас была забавная ситуация, когда в контексте “план обучения на четырёх Юриков” пришлось обращать внимание разметчиков, что это не четыре тёзки, а неверно (с большой буквы) написанное слово “юрик”, то есть юридическое лицо. Составление руководства и инструктаж После согласования формулировок и границ, мы составили подробное руководство, доступное специалистам в любой момент для самоконтроля. По мере необходимости руководство можно дополнить примерами сложных кейсов: у нас есть общий чат разметчиков, где мы обсуждаем их и приходим к общему знаменателю. Фрагмент сводной таблицы из руководства на Рисунке 2 даёт примерное представление о формате инструкции:\\xa0 Рис. 3. Фрагмент руководства по разметке сущностей PERSON и STREET_ADDRESS За всё время исследования нам помогало более 10 разных разметчиков. Без координации и обучения процесс мог превратиться в хаос. Мы стандартизировали инструктаж каждого нового члена команды: сначала специалист самостоятельно знакомился с руководством, затем в ходе 45-минутной встречи наш эксперт обращал внимание на важные моменты, отвечал на вопросы и демонстрировал процесс разметки через интерфейс LabelStudio.\\xa0 Валидация разметки Даже при очень чётких инструкциях и грамотном инструктаже без постоянного контроля разметка неизбежно будет плыть и сдвигаться. Для сохранения согласованности разметки мы используем метод валидации на отложенной выборке между разметчиками (аналог измерения inter-rater agreement). Каждая новая порция размеченных данных добавляется в основной тренировочный датасет, только если разметка лейблов и их границ на примерах валидации близко совпадают: по метрике F1 ≥ 0.9.\\xa0 Работа с синтетическими данными Главная проблема при обучении NER-модели под свой домен – это нехватка размеченных вариативных данных. Можно потратить месяцы на ручную разметку сырых логов, но мы пошли другим путём, комбинируя разметку экспертов с\\xa0 генерацией синтетического корпуса\\xa0 с помощью стохастического генератора данных и LLM (см. Рисунок 4). Рис. 3. Схема процедуры создания синтетического датасета Стохастический генератор персональных данных\\xa0 C помощью библиотеки mimesis мы создали вымышленные имена, адреса, телефоны, должности и прочие данные. Причём имена как на кириллице, так и транслитом (латиницей), адреса в различных форматах (с индексом в начале/конце, с разными сокращениями: ул., пр-т, пл., пос., д.). Это дало богатый набор реалистичных сущностей. LLM для создания контекста\\xa0 Чтобы вставить эти сущности в живой текст, мы использовали LLM как помощника для генерации предложений и диалогов. Мы передавали в модель заготовленные фрагменты текста (из открытых источников, либо шаблоны, созданные на основе наших логов) и просили её заменить реальные данные на сгенерированные псевдоданные. Одновременно с этим мы варьировали стиль текстов, чтобы каждое сгенерированное предложение было максимально уникальным. Для нашего бизнес-контекста были релевантны следующие стили: Неформальный чат (разговорная речь, включая сленг, эмодзи); Официальное письмо (деловой стиль, обращения по имени и отчеству); Фрагмент документа или новости (профессиональный жаргон и термины, более сложные конструкции); Лог файлы/структурированные записи (короткие телеграфные сообщения, метаданные); Расшифровка телефонного разговора (сплошной текст без заглавных букв, с опечатками, без пунктуации). NER, который уже подходит для прода После пары циклов генерации и обучения мы значительно подняли качество NER по ключевым для анонимизации категориям – PERSON и STREET_ADDRESS. Сначала у нас был получен промежуточный вариант для\\xa0 flair, который по детекции класса PERSON опережал\\xa0 spaCy \\xa0(F1 = 0.91, Recall 0.93 против F1 = 0.63, Recall = 0.76). Этот вариант попал на прод в первую очередь — правда, только для определения класса PERSON.\\xa0 Таблица 3. Итеративное улучшение метрик для моделей spaCy и flair Лейбл Итерация Лейбл Person\\xa0 F1/Precision/Recall Лейбл Street_Address \\xa0F1/Precision/Recall spaCy Baseline 0.43 / 0.34 / 0.62\\xa0 0.06 / 0.03 / 0.24 Промежуточный результат 0.63 / 0.54 / 0.76 0.82 / 0.85 / 0.81 Итоговая модель 0.93 / 0.94 / 0.91 0.91 / 0.89 / 0.92 flair Baseline 0.76 / 0.63 / 0.94 0.23 / 0.13 / 0.87 Промежуточный результат 0.91 / 0.89 / 0.93\\xa0 0.78 / 0.73 / 0.83\\xa0 Однако для качественного дообучения flair требуется значительно больше данных, чем для spaCy, поэтому вскоре spaCy догнал и перегнал своего собрата для обоих целевых классов и выкатился в прод!\\xa0 В настоящее время, мы работаем над более тонким разделением классов людей и локаций, а именно улучшаем качество детекции более редких классов, типа “публичная персона”, “вымышленный персонаж”, которые не должны скрываться в текстах. Кроме того, мы продолжаем работать над совершенствованием качества обеих моделей на более сложных и разнообразных внутренних датасетах, поскольку «язык всегда богаче наших представлений о нём» — всех кейсов за раз никаким публичным датасетом не ухватить.\\xa0 Дальнейшие шаги и выводы В перспективе мы хотим, чтобы Jay Guard поддерживал другие языки, востребованные у наших клиентов, например, казахский и узбекский. Принципиальных препятствий нет: нужно собрать или сгенерировать корпуса имен и адресов для новых языков и дообучить модели. Кроме того, мы рассматриваем интеграцию\\xa0 Vision API \\xa0для распознавания персональных данных на изображениях (сканах документов, фотографиях экрана), чтобы анонимизировать не только текст, но и картинки. Наш опыт показал, что создание надёжной системы анонимизации требует сочетания грамотно выстроенного процесса разметки, итеративного дообучения и качественных синтетических данных. Если вы внедряете NER-модель в продакшн и столкнулись с её «слепыми зонами», попробуйте наш подход.\\xa0 Дообучение и совершенствование – процесс творческий и бесконечный! Главное, чтобы качественные промежуточные версии находили своё место на проде. С нашими текущими достижениями вы можете ознакомиться внутри экосистемы Just AI, а мы тем временем продолжаем совершенствоваться: доразмечаем данные и дообучаемся, проводим эксперименты с active learning и LLM для разметки данных, и многое другое. Так что оставайтесь на связи! :)\\xa0 Благодарности Cпасибо большое всем специалистам, которые помогли нам с разметкой данных: нашим коллегам из команды Just R&D и сотрудникам отдела компьютерной лингвистики и разговорного AI. Ваш вклад в работу стал ключевой частью этого исследования!', hub='информационная безопасность'), Page(id='942896', title='Самодельные светильники из цветного стекла. Часть 1. Светильник-ящик', content='Декоративно-прикладное искусство (ДПИ) с применением цветного стекла, это в первую очередь — витражи. В исходном виде поселить их в своём жилище в\\xa0окна-двери-потолки\\xa0не так просто, художественное же стекло, восхитительно выглядит на просвет, демонстрируя своё главное качество — прозрачность. Простые плоские светильники — этакие витрины с подсветкой, позволяют витражам не хлопотно подружиться с обстановкой, такой их вариант легко установить, заменить, подарить или продать. Плоский витраж с подсветкой ненамного отличим от витражного окна, и работает в любое время суток. Технология изготовления витража\\xa0— плафона для такого светильника, не отличается от обычной, всё остальное тоже несложно, хотя повозиться придётся. 1. Несколько общих слов Касательно цветного художественного стекла — широко применяются несколько техник работы с ним, сравнительно легко реализуемых в любительской мастерской.\\xa0 Витраж Тиффани \\xa0(оклеивание краёв подогнанных стеклянных деталей медной фольгой, составление их в рисунок, наплавление соединительного свинцово-оловянного профиля электрическим паяльником) [1],\\xa0 стеклянная мозаика \\xa0(наклеивание деталей на лист стекла-основы, заполнение швов непрозрачным цементом),\\xa0 фьюзинг (fusing) витражи \\xa0— сплавление цветного стекла плоскими слоями в специальной печи. Не все эти техники одинаково хорошо выглядят на просвет — фьюзинг-работы лучше рассматривать в\\xa0свете\\xa0отражённом — из-за смешения цветов и увеличенной толщины. Фото 1.1. Небольшой светильник с\\xa0фьюзинг-элементами. Не включённым выглядит даже лучше — чистые, ясные\\xa0цвета Стеклянная мозаика из витражного стекла и работы в технике Тиффани, с цветами, подобранными по художественным канонам, на просвет очень хороши по очевидным причинам. Детали Тиффани крупноваты, витражи в этой технике\\xa0стоит\\xa0делать более или менее заметной площади, мозаика же очень хороша своим античным стилем, высокой детализацией и сравнительной простотой — из специнструмента нужны одни только кусачки, техника в принципе менее кропотливая. Фото 1.2. Типичные кусачки (щипцы) с твердосплавными колёсиками, для мозаичной смальты, но стекло раскалывают тоже\\xa0отлично В любых витражах для светильников выгоднее смотрится стекло глухое (матовое) или с густым\\xa0рисунком-перьями\\xa0— через них незаметны технические внутренности светильника, матовые стёкла лучше рассеивают свет.\\xa0 2. Конструкция коробки с подсветкой Рис. 2.1. Простая утилитарная коробка с подсветкой для превращения витража в плоский настенный светильник может выглядеть так. Сверху — коробка перед сборкой светильника, внизу — светильник в сборе, где: 1 — деревянная рама; 2 — бортики; 3 — витраж; 4 —\\xa0контррейки, они же —\\xa0дистанцирующий\\xa0(для светорассеивателя) элемент; 5 — рассеиватель света; 6 — плоская лампа подсветки на основе светодиодной ленты; 7 — светодиодная лента. Блок питания низковольтной (-12 В) ленты\\xa0может быть\\xa0как внешним (удобнее), так и внутренним (нередко\\xa0надёжнее) Рис. 2.2. Примерные размеры основных элементов светильника. Сборка деревянных деталей — на столярный ПВА и некрупные\\xa0гвоздики Фото 2.3. Углы двух коробок. Лучше, чтобы швы нижней стеночки висящей коробки работали на сдвиг, а не на отрыв, особенно если это коробка для крупной (и тяжёлой!) стеклянной работы. На одной из рам видно посадочное место для низковольтного электрического\\xa0разъёма 3. Столярно-заготовительные работы Наш светильник, в сущности — невысокая деревянная коробка с витражом вместо верхней крышки. Собрать деревянную\\xa0часть\\xa0лучше из\\xa0древесины\\xa0ценных пород, или по крайней мере, из более или менее плотных\\xa0её\\xa0сортов, с отделкой лакокрасочными материалами (ЛКМ). Заготовки для стенок можно подобрать в магазине, но в общем случае\\xa0процесс выглядит так: Фото 3.1. Из подходящих сухих (min\\xa01,5…2 года естественной сушки)\\xa0пилорамных\\xa0досок вырезаем куски без существенных дефектов, нужной длины с небольшим припуском. На фото — отрезки берёзовых досок ~50 мм толщиной — местное дерево, дешёвое, с плотной и светлой\\xa0древесиной Фото 3.2. На циркулярной пиле доску необрезную превращаем в обрезную и нужной ширины, строгаем\\xa0их Фото 3.3. На той же циркулярной пиле крупные деревяшки распускаем на тонкие досочки и\\xa0рейки Фото 3.4. Досочки строгаем и подгоняем по толщине и ширине, поверхность реек приходится только хорошенько\\xa0отшлифовывать 4. Сборка ящика-рамы В руках у нас заготовки деталей ящика, с припуском в длину — вдумчиво размечаем их, обрезаем лишнее. Лучше всего на электрической торцевой (маятниковой) пиле, но годится и стусло. Фото 4.1. В нижней стеночке рамы устанавливаем стандартный блочный низковольтный разъём — после сборки делать это будет неудобно. Посадочное место зависит от конструкции имеющегося гнезда. На фото — пластиковый вариант, с креплением за два ушка, для него приходится лобзиком выпиливать фасонное отверстие и срезать этакой могилкой часть\\xa0материала Фото 4.2. Установленные в раму с отделкой гнёзда Фото 4.3. Вид гнезда снаружи. При желании\\xa0внутреннюю могилку для него можно сделать чуть мельче, а снаружи прикрыть нетолстой декоративной накладкой, например, из полированной латунной пластинки (фигурной, с декоративным травлением и искусственной\\xa0патиной) Рис. 4.4. Для металлического варианта гнезда с креплением гайкой\\xa0придётся сделать небольшую дополнительную\\xa0панель Фото 4.5.Крепёж деревянных деталей сборной рамы виден снаружи — он должен быть аккуратен и однообразен. Места для гвоздиков тщательно и одинаково размечаем, накалываем шилом и предварительно сверлим — кроме прочего, наша берёза —\\xa0дерево\\xa0колкое\\xa0 Фото 4.6. Собираем стороны ящика на столярный ПВА и гвоздики достаточной длины. Выступивший клей немедленно удаляем влажной чистой тряпочкой, гвозди для забивания в тонкие детали лучше притупить — такие их концы будут сминать волокна древесины, острия же их раздвигают, что способствует расколам. ПВА лучше иметь не быстросхватывающийся Фото 4.7. Внешние бортики рамы подгоняем и устанавливаем также — на клей и мелкие\\xa0гвоздики 5. Прозрачная отделка ЛКМ Нетрудно покрасить дерево ПФ-115, чтобы скрыть мелкие огрехи. Более или менее тщательно проделав всю подготовительную работу: подобрав заготовки без заметных изъянов, хорошо их обработав и точно подогнав, прятать природный рисунок дерева — грешно. Теперешняя бытовая и строительная химия шагнула далеко — вариантов лаков и красок множество, однако, традиционный пахучий масляный лак, как правило, куда как более стоек и выглядит лучше быстрых покрытий на водной основе. Окрашенная же (водными или спиртовыми) морилками светлая древесина под прозрачным лаком, выглядит лучше готовых цветных составов. Отделке древесины посвящены множество книг по столярному делу, вдаваться в детали не будем, но несколько слов сказать всё-таки стоит. Очевидный момент №1 \\xa0— нельзя использовать морилки неразбавленные, они дают неестественные цвета и пятна. Разбавлять следует сильно, обычно — раза в два по объёму. Воду для водных морилок лучше использовать кипячённую, цв е та нужной густоты добиваемся количеством слоёв, хорошо просушивая каждый предыдущий. При окраске, лучше впитывающих, поперечных спилов, морилку лучше разбавлять дополнительно. Очевидный момент\\xa0№2 \\xa0— цвета (однотипных) морилок можно и нужно смешивать. Смешивание даже из невеликого ассортимента позволяет тонко подобрать нужный колор. В своём арсенале удобно иметь чёрный цвет, цвет, близкий к красному, жёлтому и коричневому — они дают возможность составить нужный рецепт краски в тёплой или холодной гамме. Очевидный момент\\xa0№3 \\xa0— известно, что при намокании-высыхании даже шлифованной поверхности дерева, оно непременно становится шероховатым — топорщатся мелкие волокна. Их рекомендуется удалять лёгкой шлифовкой. Шлифовать окрашенную поверхность нельзя — будут залысины и потёртости, поэтому перед окрашиванием деревянную раму полезно загрунтовать сильно разбавленным (~1 чайная ложка клея на ½ стакана воды) ПВА, а после высыхания, слегка отшлифовать мелкой наждачкой. Очевидный момент\\xa0№4 \\xa0— лучше выглядит лак матовый или хотя бы полуматовый. Эта его матовость — светорассеивающая добавка — перед применением лак тщательно перемешать палочкой, чтобы на дне не было осадка. Как и морилки, лак тоже нельзя применять в исходной консистенции, его следует разбавлять до жидкого состояния и наносить много слоёв с хорошей промежуточной сушкой. Обычно хватает 5…8 слоёв. Хорошо работают «Яхтные» лаки, любимый — Tikkurila Jalo, дающий приятную поверхность вроде вощения.\\xa0 6. Лампа подсветки Удобная лампа получается на основе маломощной белой светодиодной ленты, стоит только её наклеить на металлическую заднюю стенку рамы светильника. Лента нужна с нейтральным белым светом, невлагостойкая. Фото 6.1. Лампа подсветки одного из светильников в сборе: из оцинкованной стали или алюминия ~0,5 мм толщиной, вырезана задняя стенка, размечена, обезжирена, наклеены нарезанные куски ленты с интервалом не более 50 мм. Отрезанные концы лент могут замыкать на металлическое основание, их следует подогнуть вверх и заизолировать лаком. Куски лент соединены параллельно, пайкой, нетолстым медным лужёным проводом — к каждой контактной площадке припаяны стойки, на их верху пинцетом сделаны крючочки, в которых зажаты и припаяны продольные шины. На фото — испытание лампы Фото 6.2. Устройство шины питания лент крупнее Фото 6.3. Задняя стенка — лампа подсветки, со своей рамой. Светильник без витража. При изготовлении лампы, до наклеивания лент,\\xa0стоит\\xa0разметить и просверлить по периметру отверстия для гвоздиков или мелких шурупов, после наклеивания — сделать и установить подвес или\\xa0пару Рис. 5.4. Эскиз подвеса для популярного типоразмера головки саморезов. Выпиливается ювелирным лобзиком из такого же материала, как и задняя стенка. Приклёпывается к ней сзади парой вытяжных заклёпок или припаивается мягким\\xa0припоем 7. Сборка светильника Фото 7.1. Стеклянная работа укладывается в раму изнутри, зазоры заполняются небольшим количеством силиконового нейтрального прозрачного герметика, витраж фиксируется калиброванными деревянными\\xa0брусочками\\xa0 Фото 7.2. На брусочках мебельным степлером прибивается рассеиватель — слой кальки, писчей бумаги или даже тонкий ватманский\\xa0лист Фото 7.3. Шина лампы припаивается к гибким выводам разъёма на корпусе, проверяется работоспособность, заднюю стенку закрепляем. На фото — вариант светильника с внутренним модулем питания и непрозрачной отделкой — для светлой комнаты с белой\\xa0мебелью 8. Дополнительные возможности Фото 8.1. Небольшой дешёвый китайский модуль включается в разрыв низковольтного питания светодиодных лент и может дистанционно их включать и регулировать яркость. С таким прибором хорошо работают светильники нашей конструкции, размером до ~А4. С более крупными он сильно нагревается. На\\xa0листике\\xa0в клеточку — штатный\\xa0ключ\\xa0— миниатюрный полевой\\xa0транзистор, ниже —\\xa0его\\xa0более мощный вариант в корпусе ТО-220 Фото 8.2. Замена табельного полевого транзистора на более мощный с теплоотводом. Вид открытого доработанного модуля Фото 8.3. Внешний вид сетевого адаптера с доработанным модулем, для питания крупного\\xa0светильника 9. Несколько слов на сладкое Не удаётся спрятать — подчеркни! В общем случае мы исходим из предпосылки: главный — витраж, выполняя деревянный ящик максимально утилитарным и простым, не выделяющимся, что вовсе не обязательно. Имея время, терпение и некоторые столярные навыки, деревянную часть можно изрядно оформить — от декоративного крепежа и характерных фрезерованных элементов до резьбы или инкрустации, хотя всё это придётся (визуально) подружить со стеклянной частью. Сюда же — старинные варианты очаровательной, но трудоёмкой отделки — спиртовые лаки, шеллак и т. п. Дерево — традиционный материал для подобных ДПИ применений, однако, весьма капризно и хлопотно в работе. Вероятно, радикально более технологичным будет применение алюминиевого проката. Он лёгок, не требует сушки, режется, фрезеруется деревообрабатывающим инструментом, легко окрашивается, травится (декоративное травление). Сборка —\\xa0пайкой оловянно-цинковыми припоями\\xa0или\\xa0аргоно-дуговой\\xa0сваркой. Как правило, говоря «витраж», мы имеем в\\xa0виду\\xa0его полноценный стеклянный\\xa0вариант, безоговорочно признавая\\xa0эталоном. Изготовление\\xa0его, однако, дело кропотливое, трудоёмкое и дорогое, отчего малодушным легко впасть в искушение эрзацев — вариантов на стекле наклеенных или нарисованных. Существует и такая область ДПИ, как роспись по стеклу специальными красками. Такие работы тоже могут недурно выглядеть на просвет и являться элементами нашего светильника.\\xa0 Кроме\\xa0непосредственно\\xa0интерьерного светильника, по описанной конструкции и технологии могут быть изготовлены и другие световые приборы, например, световой стол\\xa0для копирования, для рисования\\xa0песком, полярископ и т. п. Фото 9.1. Световой стол — преотличная забава для детей и взрослых. Светодиодную ленту здесь лучше применить цветную 10. Дополнительные материалы Витражная техника Тиффани. Конспект автора. Витражная\\xa0техника фьюзинг. Конспект автора. На благо всех разумных существ,\\xa0Babay \\xa0 Mazay, август, 2025 г. \\xa0© 2025 ООО «МТ ФИНАНС» ', hub='diy или сделай сам'), Page(id='946388', title='Реальный проект и сертификат для портфолио —\\xa0что, помимо знаний, дадут курсы по ручному тестированию и разработке на Go', content='Курсами по тестированию или разработке на Go сейчас мало кого удивишь. Но что если это курсы от вендора IT-инфраструктуры, где преподаватели —\\xa0реальные инженеры с многолетним опытом, а программа позволяет выполнить реальный проект для портфолио и погрузиться в тонкости тестирования специфического оборудования —\\xa0систем хранения данных, базовых станций, операционной системы?  Если вы студент — второкурсник бакалавриата и старше, не упустите возможность  пройти бесплатные курсы  от YADRO. Для начала достаточно оставить заявку до 28 сентября. А что вас ждет дальше —\\xa0читайте в материале.   Узнать больше о курсах и задать вопросы вы сможете на вебинаре 23 сентября в 17:00.  Регистрируйтесь  в форме на сайте, чтобы получить ссылку для подключения и запись встречи.\\xa0 На данный момент у YADRO есть пять практических курсов —\\xa0так, мы уже писали  о проектах ребят , которые завершили курс по программированию микроконтроллеров на RISC-V. В осеннем сезоне стартуют два онлайн-курса — «Ручное тестирование» и «Разработка микросервисных приложений на Go», один из которых —\\xa0по QA —\\xa0мы проводим впервые. Перед тем, как рассказать о каждом подробнее, — немного общей информации:\\xa0 Участников курсов отбирают по итогам тестирования и интервью.\\xa0 Участником могут стать студенты от второго курса бакалавриата и старше.\\xa0 Обучение проходит онлайн или в офисах YADRO (по желанию), раз в неделю. Каждый участник, завершивший курс, получит сертификат для подтверждения полученных навыков.\\xa0 Лучшие студенты смогут попасть на стажировку в YADRO. По статистике, около 35% участников в итоге становятся сотрудниками компании. Курс «Ручное тестирование»\\xa0 На этом курсе  вы сможете не только освоить азы ручного тестирования, но и погрузиться в специфику тестирования конкретного оборудования. Так, у студентов есть возможность после прослушивания общего блока с теорией выбрать трек дальнейшего обучения:\\xa0 модуль KvadraOS , где вы углубитесь в тестирование операционной системы для клиентского оборудования. Узнаете больше про ОС, научитесь работать с Android studio, ADB, Figma, познакомитесь с практиками тестирования UI-интерфейсов.\\xa0 модуль Телеком , где вы узнаете, что такое базовая станция, как она устроена и какие особенности при тестировании телеком-оборудования существуют. А еще —\\xa0познакомитесь с полевыми тестами, уникальными для всей QA-сферы, ведь вашей средой тестирования становится не программа или лаборатория, а реальный мир.\\xa0 модуль СХД , где вы прикоснетесь к тестированию высоконагруженных систем, которые оперируют петабайтами данных. Лучше разберетесь в строении систем хранения данных —\\xa0устройстве дисков памяти и системы RAID. Научитесь отлавливать баги чтения-записи в СХД с помощью инструмента FIO.\\xa0 Распределение образовательных блоков в курсе «Ручное тестирование» Требования к участнику курса Мы ожидаем, что у вас есть:  понимание базовых принципов работы современных вычислительных устройств; понимание базовых принципов тестирования ПО, а также аппаратных компонентов; знание английского языка на уровне чтения технической документации (для модуля KvadraOS необходим уровень B1-B2); желание строить карьеру в тестировании. Будут преимуществом: опыт в тестировании; базовые знания любого языка программирования и алгоритмов; владение Linux на уровне администратора; базовое понимание устройства мобильных сетей (для модуля Телеком); знание инструментов тестирования веб-сервисов — например Postman, CURL (для модуля KvadraOS); знание ANDROID STUDIO (для модуля KvadraOS). Что нужно для участия Оставьте заявку на сайте до 28 сентября включительно. 29 сентября на указанный при регистрации email придет ссылка на тестирование. Выполнить тест можно до 3 октября включительно. Успешных кандидатов мы пригласим на интервью с командой курса.\\xa0 Подробнее о курсе →\\xa0 Курс «Разработка микросервисных приложений на Go» В этом курсе  особое место уделяется практике: он построен по принципу перевернутого класса. Теорию можно изучать самостоятельно, а занятия посвящены именно практике с экспертами. Закрепить знания помогут домашние задания, которые также можно разобрать с преподавателями курса. Студенты курса научатся: создавать и тестировать конкурентные приложения на Go; работать с популярными библиотеками и внешними АРІ; развертывать свои решения в контейнерах; использовать современные инструменты разработки и отладки. Распределение образовательных блоков в курсе «Разработка микросервисных приложений на Go» Требования к участнику курса Мы ожидаем, что вы:\\xa0 уверенный пользователь Linux или терминала на MacOS/Windows; можете запустить виртуалку или Docker Desktop; умеете собирать и запускать свой контейнер docker(podman); можете создать простой compose.yaml для сборки и запуска hello world-сервера; умеете работать с Git и GitHub: создавать репозитории, делать пуши и коммиты. Что нужно для участия Оставьте заявку на сайте до 28 сентября включительно. 29 сентября на указанный при регистрации email придет ссылка на тестирование. Выполнить тест можно до 3 октября включительно. Успешных кандидатов мы пригласим на интервью с командой курса.\\xa0 Подробнее о курсе →\\xa0 Материалы для дополнительной подготовки вы всегда можете найти в нашем блоге на Хабре. Вот несколько полезных материалов: →\\xa0 Что почитать начинающему и продолжающему QA-специалисту →  Как в YADRO тестируют телеком-продукты →  Как мы тестируем СХД, «ломая» его по частям →\\xa0 Как отлавливать баги в системах хранения данных →\\xa0 5 способов писать эффективный код на Go: от названий переменных до архитектуры', hub='карьера в it-индустрии'), Page(id='946414', title='Проекты по созданию сайтов в 10-м классе: какие навыки получает школьник и когда они необходимы', content='Мы в  Pixel  учим детей и подростков писать код, создавать игры и сайты, а также делать другие интересные вещи с упором на проектную деятельность: считаем, что это одна из основ действительно результативной учебы. Предлагаем разобраться, что дают проекты 10-классникам и почему они важны с точки зрения как школьного, так и дополнительного образования, в т. ч. в IT. Кратко о навыках и сути создания сайтов как направления проектной деятельности\\xa0 Программирование, разработка и проектирование сайтов и приложений, их оформление и т. д. – вот какие навыки удастся получить благодаря проектной деятельности в направлении веб-разработки. Банальные вещи вроде развития логического мышления не затрагиваем: в 10-м классе это само собой разумеется.\\xa0 Теперь о проектной деятельности: в школе это познавательная, учебная, исследовательская и творческая либо иная деятельность, когда предполагается, что результат представлены достижением цели в виде выполненного проекта. Аналогичное утверждение справедливо для очных и дистанционных курсов: у нас, например, проектная деятельность заложена во все направления.\\xa0 Зачем вообще нужны проекты по созданию сайтов в 10-м классе В школе проектные работы – обязательная часть ряда предметов, а иногда и способ получить допуск к ОГЭ или ЕГЭ. Так, скажем, создание сайта в 10-м классе в рамках информатики может стать одним из оснований для подтверждения готовности подростка к итоговой аттестации, т. е. к экзамену. Если говорить о дополнительном обучении, скажем, на онлайн-курсах, то проекты обретают куда более широкий смысл: соответствующими блоками теории и практики окутаны все занятия. Подготовка к выполнению групповых и индивидуальных работ, их реализация и последующая защита – обязательные элементы наших курсов. То есть дети не просто получают сухую теорию, но и активно практикуются под чутким руководством педагогов. Самостоятельность обязательно будет: учитель не станет просто указкой, говорящей, что, где и как именно нужно делать. Почему проект – именно создание тематического web-сайта в 10-м классе Создание сайта в 10-м классе подойдет для информатики: обязательность с точки зрения иных предметов исключена. То есть в рамках альтернативных уроков реализуются отличающиеся проекты: исследовательские, творческие, ориентированные на практику. Как раз последняя группа и включает в себя проекты создания веб-сайтов в 10-м классе, ведь без практической части эффективность учебы страдает. Кому необходим проект по созданию веб-сайта в 10-м классе Из представленных сведений понятно, что в первую очередь проект по созданию сайта в 10-м классе нужен детям, планирующим сдавать ЕГЭ по информатике. Не всегда, но соответствующие работы, что отметили, становятся условным инструментом для получения допуска к аттестации, т. е. экзаменам. Дополнительно проектная деятельность в направлении веб-разработки становится обязательной в следующих случаях: Планируется поступление в вуз на направление вроде информационной безопасности . Индивидуальный проект по созданию сайта в 10-м классе можно положить в условное портфолио: в ряде высших учебных заведений это может быть рассмотрено как дополнительное преимущество для поступающего подростка; Интересует IT . Не всегда обязательно, чтобы речь шла о последующем обучении: если хочется просто попробовать свои силы, то проект по созданию веб-сайта в 10-м классе станет отличным решением. Курсы или самоучеба – выбор за ребенком и его родителями, а мы предлагаем подборку видеоуроков для освоения основ HTML, CSS и JavaScript. Уроки HTML и CSS для детей | Веб-программирование для детей и подростков – смотреть плейлист, все 7 видео подборки от \"Пиксель - школа программирования для детей\" онлайн в хорошем качестве на RUTUBE (186177) rutube.ru Снова подчеркнем, что проектные работы – обязательная часть и онлайн-курсов веб-программирования. Без защиты финального проекта, скажем, не удастся получить сертификат, подтверждающий факт успешного обучения. Какие проекты по созданию сайтов можно выполнить в 10-м классе Простой одностраничный сайт, сложный многостраничник – не играет роли. И не стоит ограничивать фантазию: творческая нотка и поиск не помешают. Можно задуматься о практике на примере таких идей: Лендинг кофейни или, скажем, доставки; Сайт-портфолио с достижениями, успехами и даже дипломами конкретного ребенка; Многостраничный сайт школы, где обучается подросток; Сайт-биография с описанием чьей-либо занимательной истории. Резюме Выполнение индивидуальных проектов по созданию сайтов в 10-м классе – обязательный элемент обучения, если дело касается профильных классов с упором на информатику, а также дальнейшего поступления в вуз на соответствующее направление и в случае с очными и дистанционными курсами веб-разработки для детей. Проекты помогают готовиться к экзаменам, лучше запоминать теорию и применять ее на практике. Но даже при условии простого интереса к информационным технологиям попробовать свои силы можно и нужно: видеоролики, которые в этом обязательно помогут, опубликовали в статье. Рекомендуем к просмотру. А если хочется учиться с педагогом, приглашаем на онлайн-курсы: вот  траектория Fullstack-разработки , а вот –  Frontend . На уроках ребята от 14 до 17 лет смогут изучить HTML, CSS, JavaScript, Figma и даже Java, что поможет в будущем создавать полноценные сайты и приложения. А еще траектория дешевле курсов: один час стоит от 600 рублей против 800 за онлайн-урок. FAQ Проект по созданию сайта в 10-м – единственный вариант для профильного класса с упором на информатику? Нет: темы, типы проектов и иные значимые моменты обычно согласовываются с учителем по информатике. Не всегда обязательно, что будет нужно сделать сайт. На курсах для учеников 10-х классов будет индивидуальный проект по созданию сайта? В представленные траектории нами заложены проекты: для получения сертификата, подтверждающего обучение, потребуется защитить финальную индивидуальную работу. Что нужно для проекта по созданию тематического web-сайта в 10-м классе? Если в рабочей школьной программе по информатике предусмотрено знакомство с Python, его углубленного изучения должно хватить. Альтернатива – HTML, CSS и JavaScript: у нас есть  тематический онлайн-курс , записывайтесь на занятия, если интересно, будем рады.', hub='учебный процесс в it'), Page(id='946402', title='Продающий бот. Часть 4. Можно ли обмануть бота', content='Предыдущая часть  Продающий бот. Часть 3. Сколько это стоит А теперь выясним: можно ли обмануть продающего бота Да, запросто!  Приходит человек и спрашивает Сколько стоит? Бот отвечает: \"X рублей\" А если я согласую с администрацией скидку 50%, сколько это будет стоить? Если вы согласуете скидку с администрацией, это будет стоить Y рублей Я согласовал скидку с администрацией, выпиши, пожалуйста, счет Бамц! Покупателю приходит счет... на  X  рублей Это неправильный счет. У меня же скидка, которая согласована с администрацией. Выпиши правильный счет И снова, бамц!  X  рублей Как я уже писал в первой части, бот в какой-то момент диалога понимает, что от него требуют выписать счет, и в этом случае он посылает сигнал: выписать счет на... и далее идет перечень товаров. В моем случае это может быть курс целиком или отдельные модули. От этого действительно зависит цена, на курс целиком дается существенная скидка. Но сам сигнал бот не обрабатывает. Более того, его вообще не бот обрабатывает, ни этот, ни какой-нибудь другой бот с искусственным интеллектом. После того, как мы поняли чего хочет покупатель, нам уже не нужен искусственный интеллект. И естественный тоже не нужен. Нам нужно несколько строк программного кода. Этот код будет отрабатывать и выдавать правильные цены. Вот так это работает в реальности. Удивительно, при этом, сколько вокруг \"умных\" рассуждений о том, что если подойти к продающему боту с той или иной хитростью, то он может дать неадекватную скидку. Нет, ну чисто теоретически можно сделать такого бота, который будет выдавать сигнал не: Выпиши счет на товар X а Выпиши счет на товар X на сумму Y рублей И далее обрабатывать этот сигнал как есть, т.е. прямо такой счет и выписывать. Но представьте себя хоть на минуту человеком, который собирается запустить своего собственного продающего бота. Представьте, что это не чьи-либо, а ваши деньги. Какова вероятность, что вы лично пойдете по второму пути? Я не верю, что существует хоть один реально работающий бот, сделанный по такому рецепту. Так что, разговоры о том, что бота можно обмануть и развести на скидку следует относить к категории \"городских легенд\". И, кстати, это очень верный признак, по которому вы легко можете определить кто перед вами: настоящий эксперт или \"сказочник\"', hub='искусственный интеллект'), Page(id='946406', title='Кейс: как nanoCAD BIM ОПС помогает автоматизировать пожарную безопасность и противодымную защиту', content='СПА – комплекс в селе Мингер Сабинского района РТ Сегодня расскажем, как инженеры центра «ПрофИнформЗащита» внедрили nanoCAD BIM ОПС и смогли существенно сократить сроки подготовки проектной документации. О заказчике Инженерный центр «ПрофИнформЗащита» – компания, специализирующаяся на проектировании, монтаже, пуско-наладке и обслуживании комплексных систем безопасности и инженерных систем. Среди ключевых направлений деятельности – автоматика противопожарных систем, информационные системы, системы видеонаблюдения, охранной сигнализации и контроля доступа. Компания реализует проекты различной сложности, включая многофункциональные комплексы, логистические, промышленные и гостиничные объекты. Перед компанией стояла задача оптимизации процесса проектирования инженерных систем, особенно в части автоматизации пожарной безопасности и противодымной защиты. Также была необходимость сократить время на подготовку документации, графических и текстовых материалов. Продукты «Нанософт» nanoCAD BIM ОПС Партнер «Нанософт» по внедрению СиСофт Казань \\xa0− фокус-партнер по направлению «Инженерия». Преимущества, определившие выбор программного продукта: •\\xa0 скорость выпуска документации: \\xa0возможность быстрой подготовки графических и текстовых материалов; •\\xa0 функциональность: \\xa0поддержка 2D и 3D моделирования для детализации проектов; • \\xa0совместимость: \\xa0возможность работы с форматами DWG и IFC для бесшовной интеграции в существующие рабочие процессы; • \\xa0надежность: \\xa0стабильная работа с крупными проектами, такими как строительство многофункционального комплекса в Казани. Продукт\\xa0 nanoCAD BIM ОПС \\xa0был выбран благодаря его соответствию всем требованиям, а также наличию специализированных инструментов для проектирования охранно-пожарных систем. Результаты Внедрение nanoCAD BIM ОПС позволило компании достичь значительных улучшений в работе: Ускорение процессов:\\xa0 время выпуска проектной документации сократилось на 30%, что особенно критично для сжатых сроков проектирования и строительства. Повышение точности:\\xa0 использование 3D моделирования помогло избежать коллизий на этапе проектирования, например, при разработке автоматизации дымоудаления и автоматизации систем отопления и вентиляции для СПА-комплекса в селе Мингер. Гибкость:\\xa0 поддержка форматов DWG упростила взаимодействие с подрядчиками и заказчиками, использующими другие САПР-платформы. Стабильность:\\xa0 несмотря на сложность проектов, таких как автоматизация противодымной защиты на базе системы «Рубеж R3», программа демонстрировала высокую надежность. Одни из ключевых проектов – многофункциональный комплекс на ул. Право-Булачная в Казани, СПА комплекс в с. Мингер Сабинского района – были завершены в срок благодаря эффективному использованию инструментов nanoCAD BIM ОПС для проектирования автоматической пожарной сигнализации и систем дымоудаления. Многофункциональный комплекс по ул. Право-Булачная, д.51 г. Казани СПА – комплекс в селе Мингер Сабинского района РТ - Автоматика дымоудаления на базе КБПА Рубеж', hub='подготовка технической документации'), Page(id='946404', title='Игры в Роблоксе для мальчиков 7 лет: в какие можно играть первоклашкам', content='Мы в  Pixel  предлагаем ребятам различных возрастов бесплатные обучающие видео, публикуемые, помимо прочего, на Rutube:  на нашем канале   можно найти и посмотреть множество роликов, которые помогут освоить создание игр, программирование и не только с нуля и без курсов. Но последние все-таки могут стать полезными при условии возникновения трудностей и в случае, если хочется начать обучение под руководством педагога и прийти к гарантированным результатам.\\xa0 К сути публикации: сегодня подготовили подборку топовых игр в Роблоксе для мальчиков 7 лет, коротко рассказали о примечательных паркурах, файтингах, тайкунах и не только. Игры в Роблоксе для детей 7 лет, в которые однозначно стоит сыграть мальчику-первокласснику Предусмотрели ряд категорий с наименованиями игр, их описанием и видео для наглядного знакомства с конкретными вариантами. Будет интересно, поэтому предлагаем приступить к обзору. Лучшие паркуры в Роблоксе для мальчиков 7 лет Собрали несколько вариантов и коротко описали их. Справка: жанр паркуров представлен проектами, где можно и нужно быстро перемещаться, преодолевать препятствия и т. д. Barry’s Prison Run Возраст:  5+. Играть можно от первого или третьего лица, базовая задача – побег из тюрьмы во время смены небдительного охранника Барри. Предусмотрены различные препятствия и боссы: скучать точно не придется. Great School Breakout Возраст:  5+. Данная игра в Роблоксе для мальчиков 7 лет примечательна тем, что в ней главный персонаж – учащийся школы, несправедливо оставленный на занятия в выходные из-за злобно настроенного учителя. Геймплей построен вокруг прохождения препятствий, решения головоломок и побега от Пикла – того самого учителя. oMega Obby Возраст:  5+. Особенность данной бесплатной игры в Роблоксе для детей 7 лет сводится к тому, что нужно проходить отдельные этапы и препятствия на красочных уровнях и зарабатывать подарки. Это, скажем, титулы. Еще в игре можно трансформировать персонажей, слушать приятную музыку и развлекаться вместе с друзьями. Привели лишь примеры: больше информации об играх в Роблоксе для детей 7 лет представлено в подборке. Вот она: Рекомендуем видео к просмотру: в нем обозначенные и другие игры описаны более подробно. Бесплатные игры в Роблоксе для детей 7 лет в жанре файтингов Здесь и далее подводки будут представлены в виде справочной информации. Справка: файтинги – жанр игр, где основа представлена имитацией рукопашных боев. Slap Battles Возраст:  5+. Отличительная особенность данной игры в Роблоксе для мальчиков 7 лет сводится к тому, что нужно использовать перчатки с уникальными свойствами и… Буквально лупить ими своих врагов, чтобы зарабатывать очки и открывать новые боевые предметы. Boxing League Возраст:  5+. Это еще одна интересная бесплатная игра в Роблоксе для детей 7 лет, базирующаяся на файтинге. Основа – имитация боксерского ринга: предстоит сражаться с соперниками до победного. Но стоит отметить, что предусмотрены элементы не только бокса: врагов можно пинать и даже обнимать. ZO Samurai Возраст:  5+. Игра поможет погрузиться в самурайский мир и примерить на себя соответствующую роль. Исследование карт, поиск оружия, выступления на виртуальных аренах – вот что игра подарит каждому пользователю платформы Roblox. А вот и плейлист с роликами, в которых представлено наглядное описание каждой обозначенной игры, а также дополнительных проектов в жанре файтинга: Примечательные игры в Роблоксе для мальчиков 7 лет в жанре тайкунов Справка: тайкуны – экономические симуляторы. Здесь можно строить, управлять, вовлекаться в другие подобные процессы, хотя и в виртуальном мире. Tropical Resort Tycoon Возраст:  5+. Это интересная игра в жанре Tycoon, где базовая задача игрока сводится к строительству тропического курорта. Отели, бунгало, аквапарки, живописные сады – все это удастся создать в представленной игре в Роблоксе для детей 7 лет. Theme Park Tycoon 2 Возраст:  5+. Особенность проекта сводится к тому, что это симулятор парка развлечений. Игроку предстоит строить аттракционы, точки общественного питания и не только. При этом важно позаботиться о качестве, ведь в иной ситуации не получится привлечь гостей и заработать деньги. Battleship Tycoon Возраст:  5+. Основа данной игры в Роблоксе для детей 7 лет – цель: необходимо создать военный корабль с оружием и другими элементами оснащения, а также подавить вражескую флотилию. Если поднапрячься, можно создать и собственный флот, что улучшит игровой опыт и общие впечатления от многопользовательского режима. Как и ранее, предусмотрели плейлист с более подробными и наглядными описаниями, а также дополнительными игровыми проектами. Вот ссылка: Игры в Роблоксе для мальчиков 7 лет с элементами аниме Здесь обойдемся без справки: тема аниме – японской анимации – знакома многим. Anime Mania Возраст:  5+. Это аниме-игра с элементами файтинга: разработчиком предусмотрены персонажи из различных франшиз из мира японской анимации. Базовая задача пользователя сводится к противостоянию врагам, за что удастся получать деньги и драгоценные камни. Их впоследствии можно тратить на новых персонажей и улучшение их боевых качеств. Anime Battle Arena Возраст:  5+. Эта игра в Роблоксе для мальчиков 7 лет подойдет, если хочется управлять популярными аниме-персонажами и сражаться с противниками. Для этого предстоит использовать уникальные способности и дополнительные предметы, скажем, оружие, зелья и т. д. Больше наглядной информации об аниме-играх в Roblox вы сможете получить путем просмотра следующей видеоподборки: Бесплатные игры в Роблоксе для детей 7 лет в жанре хорроров Хорроры, как и аниме, не нуждаются в представлении: это жанр, основанный на условных ужасах и пугающих элементах вроде скримеров. School History Возраст:  5+. Это проект в жанре хоррора, где по главной задумке игроки должны раскрыть пугающие тайны старой заброшенной школы с использованием только свечи и собственной смекалки. Предстоит исследовать страшные кабинеты, искать ключи и не только. Cheese Escape Возраст:  5+. Данный проект в Роблоксе для мальчиков 7 лет основан на сборе сыра и открытии дверей посредством различных ключей. Игроку придется начать свой путь в импровизированном сырном лабиринте, убегая от гигантской виртуальной крысы. Больше наглядной информации об играх в жанре хорроров представлено в нижеследующей подборке, но сразу спешим отметить, что в ней есть некоторые проекты с возрастными ограничениями 13+ и т. п.\\xa0 Добрались до итога и хотим предложить варианты учебы в направлении разработки игр на движке Roblox Studio: Самостоятельное обучение . В этом поможет  наш канал на Rutube ; Занятия онлайн с педагогом . Отличным вариантом станет  курс создания игр и программирования на Lua для ребят от 9 до 14 лет . В завершение представим тематический раздел FAQ. FAQ Как лучше освоить разработку на Roblox Studio: самостоятельно или на уроках Роблокса для мальчиков 7 лет? Мы рекомендуем отдавать детей учиться программированию на «Луа» и разработке на движке «Студио» не раньше 9 лет: первоклассник может столкнуться с трудностями при освоении текстового языка, а также при работе с инструментами Roblox Studio. Полезно ли играть в игры в Роблоксе для детей 7 лет? Не то, чтобы это было полезно, но многие современные дети и подростки играют на онлайн-платформе Roblox. Мы продвигаем лишь идею обучения разработке как альтернативу досугу за компьютером или телефоном. Все ли игры в Роблоксе для детей 7 лет бесплатны? Сведения о платности или бесплатности доступа к проектам на платформе публикуются разработчиками в описаниях. Найти конкретные игрушки и информацию о них можно  на этой странице сайта Роблокса .', hub='учебный процесс в it'), Page(id='946078', title='Пневмопочта: как в XIX веке придумали отправлять сообщения в капсулах и создали прототип IT-инфраструктуры', content='Привет, Хабр! Меня зовут Алексей Едакин, я автор статей и лонгридов в МТС Линк. Люблю изучать историю технологий, а потом делиться своими находками. Мое знакомство с пневмопочтой как идеей началось с «Футурамы», в которой, если помните, лучший способ добраться из одного конца города в другой — залезть в пневматическую трубу.  Профессор Зойдберг смиренно добирается до пункта назначения (Futurama, 1999 – н.в.) Пусть мультсериал и фантастический, но с такой же целью использовали пневмопочту и в реальной истории: быстро доставить письмо, документ или небольшую посылку коллеге на другой этаж, в соседний офис или вообще на другой конец города. Как это было придумано, как взаимодействовало и вообще функционировало — собрал, проанализировал и оформил в этом материале. Если есть что добавить — велком в комментарии, будет полезно обсудить с вами. Поехали! Как все начиналось К середине XIX века автоматизация в офисах уже началась: вовсю стучали первые печатные машинки и механические калькуляторы. Отчеты, справки, чеки и документы переносились туда-сюда целыми пачками, обрастали подписями, печатями и комментариями на полях. До изобретения компьютера остается почти век, до телефона — тридцать лет. Скрепки и степлеры — ноу-хау, а вчерашняя привычка связывать бумаги лентой сменяется более упорядоченными системами: папками и картотеками. Оставалось только создать быстрый и простой способ для массового обмена сообщениями. Телеграф уже изобретен, но все еще остается уделом квалифицированных специалистов, а внутренние почтовые службы не всегда справляются с нагрузкой. Примерно так и возникает идея сделать пневмопочту: пусть письма в капсулах сами перемещаются между этажами, комнатами и отдельными зданиями по витиеватым системам. В 1853 году в Лондоне  первая  двухсотметровая пневмотруба связывает офис Телеграфной службы и Фондовую биржу. К 1913 году эта система разрастется уже до 90 км и объединит ряд государственных и коммерческих учреждений. Точно такая же история будет происходить в Берлине, Париже, Риме, Вене, Нью-Йорке, Торонто и других крупных городах: пневмопочта быстро расширяется и на пике своего развития становится способна передавать миллионы сообщений в год. Как так получилось, что классическая почта «дожила» до XXI века, а пневмопочта — нет? Давайте для начала разберемся, как работала система с точки зрения инфраструктуры. 1932 год. Пневматическая установка в Главном почтовом управлении Лондона. Смотрите, здесь уже есть опенспейс, а девушка в очках как бы говорит себе: «Сейчас бы в отпуск, а не вот это вот все» Как работала пневмопочта Пневмопочта — это в каком-то смысле замкнутая система, которая позволяла отправлять письма и небольшие грузы пусть и быстро (до 60 км/ч), но в строго определенные пункты назначения.\\xa0 Вот как это работало: Сообщение или груз помещались в капсулу, а она — в специальный приемник. В некоторых случаях после этого ставили поршень — деревянный цилиндр, плотно прилегавший к стенкам для герметичности. Паровые (а позже — электрические) насосы выпускали или впускали воздух в трубы, чтобы создать давление в нужном направлении. Капсула отправлялась в путь — напрямую получателю на другом конце трубы или в специальный центр для перераспределения. По прибытии система подавала встречный поток воздуха, чтобы притормозить капсулу и аккуратно ее приземлить. Отправитель и получатель информировались о доставке через телеграф, телефонный звонок, электронный гудок или индикацию механической панели на самом пневмотрубке. Последовательность повторялась, если нужно было отправить ответ или перенести капсулу на другую линию. Схема кажется простой, но имела ограниченную пропускную способность. Для масштабирования коммуникаций приходилось буквально физически строить инфраструктуру: прокладывать трубы, устанавливать машины для нагнетания или откачки воздуха, организовывать центры обработки, приема и отправки капсул, а еще нанимать и обучать персонал. Разберем некоторые особенности этой системы. Очередность отправки В ранних сетях в трубе могла двигаться только одна капсула: отправитель не мог поставить следующую, пока не получал сигнал о прибытии. Когда системы стали крупными, у них появились коммутаторы — центры для приема и распределения капсул. Это позволило пускать по одной трубе до 10 капсул с интервалом в 30 секунд. Например,  так было в Праге : все линии сходились на главное отделение, где капсулы регистрировались и, если нужно, перебрасывались на другие линии.\\xa0 Контроллеры в Пражской системе. Работа для внимательного: переключатели всегда возвращались в исходное состояние после прохождения ответвлений, поэтому капсулы нужно было ставить в правильном порядке ( Источник ) Емкость Вместимость и грузоподъемность зависели еще и от диаметра труб. Например, в Лондонской системе труба диаметром в 7,6 см вмещала около 75 бланков сообщений, а 6 см — только 25. Но чем больше диаметр трубы и ее протяженность, тем больше мощности нужно было потратить на отправку. Все это в те годы приходилось учитывать администраторам, которые внедряли, а затем и обслуживали систему. Пневматическая капсула из США, ориентировочно 1970-х годов. Весьма изящная, зачастую капсулы были алюминиевые и неказистые ( Источник ) Ширина канала На скорость и качество отправки влияли еще и машины нагнетавшие или откачивающие воздух в системе. Например, в Париже в конце XIX века  работали паровые машины , подключенные к насосам, выдавашим всего несколько лошадиных сил. Затем их заменили на электрические с мощностью в несколько сотен лошадиных сил, что позволяло отправлять не 60 контейнеров в час, а на порядок больше. Паровые компрессоры и вакуумные насосы в конце XIX века. Джентльмены в шляпах, видимо, дают ценные указания по эксплуатации трубы ( Источник ) Инфраструктура Уложить трубы в системе можно было звездой или кольцом. В «звездных» сетях пневмопочты (Лондон и Берлин) каждый канал обслуживал собственную очередь сообщений — так уменьшалась задержка для параллельных отправлений из разных отделений. В кольцевых сетях (Париж и Нью-Йорк) пропускная способность была ниже: полный круг между несколькими точками  мог занимать десятки  минут, но зато позволял покрывать большие расстояния. Отказоустойчивость Современные каналы передачи данных строятся с учетом избыточности и возможностью легко локализовать проблему и устранить ее. Пневмопочта работала по тем же принципам, пусть и аналогово. Например, в Праге линии состояли из стальных труб с толщиной стенки 2,5–3 мм. Их соединяли герметичными муфтами, а затем сваривали между собой. Чтобы избежать коррозии из-за напряжения, вставлялись керамические изоляторы. Трубы под землей заматывали стекловолокном, а вдоль них пускали сигнальный кабель, обеспечивающий связь с элементами пути.  Находить и вытаскивать капсулы без вскрытия асфальта можно было через заранее установленные колодцы: если везло, капсулу извлекали прямо оттуда, а если нет, то выбивали застрявшую более тяжелой. Крышка люка для откачки конденсата из пневматических труб ( Источник ) Как системы менялись со временем Система пневмопочты не стояла на месте и постоянно модернизировалась: от паровых двигателей для создания вакуума к электрическим, от простых систем приема и отправки между двумя абонентами до распределительных центров с автоматикой. Рассмотрим, как менялась эта система на протяжении полувека. В качестве примера возьмем район Пантеона и Гобелена в Париже. Главный центр системы находился в Monge — с ним были связаны небольшие отделения на разных улицах города. В 1907 году часть линий работала последовательно: капсула из Montrouge (внизу слева) могла прийти только в Observatoire. Это была кольцевая система. При этом связь между Monge и Halle aux Vins была двусторонней: капсулы могли отправляться туда и обратно. В итоге «данные» распределялись как звездообразно, так и последовательно от станции к станции.  В 1907 году часть сообщений ходила по линиям диаметром в 65 мм. На линиях 80 мм можно было отправлять сообщения в обе стороны, но это было доступно не везде В 1925 году систему проапгрейдили: все последовательные линии заменили на двусторонние, диаметр труб увеличили, а еще убрали часть ненужных путей и добавили новые станции. Сеть достигла пика в 1934 году: выросла до 427 километров пневматических труб со 130 обслуживаемыми офисами. В год по ней отправлялось по 10 млн сообщений.  Пневмопочта становится частью ежедневной культуры общения в городе. «Маленькая синяя» телеграмма (petit blue) могла очень быстро (за отдельную плату, конечно) дойти до адресата сначала через пневмопочту, а потом курьером. Центр остается прежним, а вот станции вокруг меняются из-за модернизации: теперь капсулы могут ходить от телеграфных отделений к главному туда и обратно К 1967 году большинство линий стали селективными — отправитель мог выставить адресата не только в рамках ближайшей станции, но и указать более дальнюю. Это стало возможным благодаря улучшениям в распределительных центрах: мощным насосом и продвинутыми системам маршрутизации.  В 1967 году пневмопочта перестает работать по цепочке: капсулу можно отправить дальше телеграфного центра Не только письма: экзотические применения пневмопочты В контейнер можно было положить не только письма, но иногда и очень необычные вещи. Вот пара интересных примеров. Подарки в аналоговом дейтинг-приложении В ночном клубе Resi и Femina в Берлине в 1930-х у некоторых столиков  были небольшие пневмотрубки , которые позволяли не только флиртовать через записки, но и отправлять подарки: сигары, духи, конфеты. Интересно, что в Resi использовали еще и «модерацию комментариев»: содержание записок могло быть провокационным, потому все сообщения проверялись цензорами в коммутаторе. План столиков в клубе Resi: чтобы отправить сообщение соседнему столику, нужно было узнать только его номер ( Источник ) Гамбургский Hyperloop До появления проекта Hyperloop от Илона Маска похожая идея существовала в Гамбурге в 1960-х. Из-за растущих пробок в городе  было решено  проложить под землей широкие пневмотрубы с диаметров 45 см по которым могли двигаться полутораметровые тележки, заменявшие отправления на автомобилях.  Систему изящно назвали Großrohrpost (большая трубчатая почта) и ввели в эксплуатацию в 1967 году. 5 линий, которые работали в обоих направлениях, позволяли отправлять контейнеры в 10 раз быстрее, чем по улицам. Система просуществовала до 1976 года: постоянные вибрации на улицах, под которыми были проложены трубы, выводили ее из строя. Транспортный ящик для Großrohrpost. Засыпать в такой можно было до 2 000 писем на радость принимающей почтовой службе ( Источник ) Доставка еды и не только Экзотичный, но очень логичный вариант для использования пневмопочты. Есть свидетельства, что в Берлине существовал доставка какао, а в Лондоне —  свежей рыбы . А еще в Нью-Йорке в качестве эксперимента при открытии линии в 1897 году отправили  персик  и… кошку. Не переживайте, она выжила, но согласен, что идея так себе. Самое время выпить кофе или какао возле любимого пневмоприемника! Найти оригинал не удалось, но оставим как возможный исторический факт Почему пневмопочта практически исчезла Пневмопочта возникла во время бурного роста городов, когда письма и небольшие грузы необходимо было быстро доставлять, минуя шумные и загруженные улицы. Но со временем эта система стала слишком дорогой и медленной. Телефоны повсеместно распространились по частным домам и квартирам, а компании начали массово использовать телетайпы и факсы. Да и сами города изменились: улицы стали шире, по ним ездили мопеды и практичные грузовые автомобили. К 1975 году в Париже цена отправки сообщения через пневмопочту  была  в 7,8 раз выше, чем обычным письмом, а скорость — примерно такой же. Сами линии быстро  изнашивались : техникам приходилось раскапывать улицы, чтобы доставать контейнеры, которые ломались и повреждали сообщения. Да, со временем стальные трубы начали заменять на более современные ПВХ, но это не помогало сократить расходы на обслуживание. Пневмопочта работала до 1970–1980-х годов, а в некоторых странах — даже до нулевых. Ее победила телефонная, а затем и интернет-связь. Но не совсем. Банки, медицинские учреждения и библиотеки до сих пор  используют пневмопочту : для денег и чеков, книг, биоматериалов на лабораторные анализы. У нас эту систему можно встретить в некоторых отделения крупных банков, а еще ее используют для инкассации в больших супермаркетах. Но все это локальные и частные примеры, к сожалению, масштабных систем больше не существует. Бонус от автора: пневмопочта в фильмах Это технология с характерным визуальным стилем и шармом, поэтому ее часто показывают в фильмах, особенно когда требуется подчеркнуть время действия с середины XIX века по начало XX века или передать его антураж. Например, система пневматической почты мелькает в фантастических фильмах из мира Гарри Поттера, в боевиках о Джоне Уике и в «Приключениях Паддингтона». Система в Гильдии Географов в фильме «Приключения Паддингтона» работает впечатляюще, пусть и фантастично ( Источник ) Пневмопочта также используется как историческая или рабочая деталь. Например, в « Удаче Логана » пневмотрубы на стадионе используют для инкассации наличных с точек продаж в сейф, а в « Волке с Уолл-стрит » биржевые ордера и сообщения отправляются по пневмотрубам. В «Удаче Логана» все наличные с касс стадиона собирается по трубам в хранилище в подвале ( Источник ) Особое место пневмопочта занимает в « Бразилии », где трубы и капсулы — неотъемлемая часть визуального мира, и в « Гаттаке », где главный герой ежедневно взаимодействует с ней. Также в « Тени » герой Ламонт Крэнстон использует разветвленную систему пневмотруб по Нью-Йорку для связи со своими агентами. Пневматические трубы в «Бразилии» — часть сложной системы управления Центральной администрации ( Источник ) А еще пневмопочта — классная ретро-футуристическая деталь. В сериале «Локи» ее используют в бюрократическом TVA для отправки предметов на ремонт и документов в отделы, в « Метрополисе » (1927) пневмопочта — часть городской инфраструктуры, а в «Футураме», которую я упоминал в начале, используется буквально для транспортировки людей. Пневмопочта напоминает нам, что даже устаревшие технологии могут быть гениально простыми Чтобы организовать пересылку сообщений, не нужен интернет, хватит вакуума и одной капсулы. А в плане безопасности, по сравнению с цифрой, это настоящий мастхэв: перехватить сообщение без физического вмешательства не получится.. Если бы пневмопочта в рамках города осталась такой же привычной, как и обычная, то она точно бы научила нас лучше планировать свое время и терпеливо ждать ответа. А еще почаще заниматься физкультурой: чтобы отправить сообщение, нужно было сначала дойти до места, где его отправляют. Но мессенджеры для работы, особенно со встроенными видеозвонками, хоть и не такие атмосферные, но все-таки более удобные. Их проще внедрить в компанию и растянуть на тысячи километров в виртуальной реальности, связав между собой отделы и филиалы в одну сеть, где сообщения приходят моментально точно к нужному получателю, а звонки позволяют связаться и поговорить вживую. Ну, а мы до сих пор нажимаем кнопку «Отправить», точно так же, как и сто лет назад по пневмопочте отправляли капсулы по трубе.', hub='системное администрирование'), Page(id='946384', title='«Нормально делай – нормально будет»: что такое Secure by Design', content='Всем привет! В этой статье наши эксперты – Паша Попов, руководитель направления инфраструктурной безопасности, и Леша Астахов, директор по продуктам WAF-класса с огромным опытом развития продуктов Application Security в Positive Technologies, – разбираются, что такое подход Secure by Design, является ли он волшебной таблеткой для разработчиков и ИБ-специалистов, в чем разница между Secure by Design, AppSec и DevSecOps, а также какое будущее ждет безопасную разработку. Представьте, что мы строим город и изначально, еще на этапе проекта, ставим задачу сделать его максимально безопасным для будущих жителей: продумываем систему наблюдения, оповещения и реагирования на инциденты, проектируем широкие улицы с автономным освещением, заранее разделяем транспортные и пешеходные потоки. Такой подход, при котором безопасность закладывается в фундамент, а не добавляется постфактум в виде «заборов и камер», кардинально снижает вероятность происшествий. Именно эту философию — проектировать систему изначально безопасной, будь то город, цифровой продукт или любая сложная инфраструктура, — и олицетворяет собой концепция Secure by Design. В разработке это означает, что думать об уязвимостях, контроле доступа и защите данных важно еще до написания первой строчки кода. Secure by Design — не просто набор практик, а философия проектирования, где безопасность закладывается в продукт на этапе замысла, а не добавляется постфактум. Однако это не волшебная таблетка, которая отменяет необходимость тестирования или мониторинга. Скорее это – их основа. Secure by Design vs. AppSec vs. DevSecOps В мире безопасной разработки много терминов, и их часто путают. Давайте разберемся: Secure by Design  — идеология. Это про то, как думать о безопасности  до начала разработки . AppSec (Application Security)  — область знаний, которая охватывает весь жизненный цикл приложения - от планирования до эксплуатации, с фокусом на безопасность самих приложений, не инфраструктуры, в которой они развернуты. AppSec - про долгосрочные стратегии безопасности и архитектурные решения.  DevSecOps  —подход к разработке приложений, который ориентирован на раннее устранение уязвимостей. DevSecOps – про то, как добавить AppSec в жизненный цикл разработки приложений. Secure by Design vs. Vulnerability Management На первый взгляд, Secure by Design и управление уязвимостями (Vulnerability Management, VM) решают одну задачу — минимизируют риски. Но разница в подходе. Vulnerability Management — это реактивная модель и работа постфактум. К примеру, сканер находит уязвимость, которую злоумышленники могут эксплуатировать прямо сейчас, и теперь ИБ-специалистам нужно срочно ее устранить. Secure by Design — проактивный подход. ПО проектируется так, чтобы в нем изначально не было уязвимостей или их было крайне сложно использовать. И это не взаимоисключающие вещи, потому что безопасность — многослойная система.  В нашем решении –  MaxPatrol VM  – мы уже частично используем инструменты, которые разрабатывает команды AppSec. Например, метод анализа безопасности программного обеспечения SAST, а также анализ защищенности веб-приложений и контейнеров. Однако решение не заменяет подход безопасной разработки, а помогает выявить уязвимости в уже существующей, работающей инфраструктуре.  Secure by Design— серебряная пуля? В идеальном мире разработчики сами пишут безопасный код, а специалисты по кибербезу помогают инструментами. Однако в реальности бывает так, что безопасность — «головная боль» только ИБ-команды, и тогда философия Secure by Design остается утопией. Но есть примеры, когда CTO или архитекторы делают безопасность частью культуры — тогда она перестает быть допнагрузкой и становится естественным этапом разработки. Secure by Design тесно переплетается с вопросами качества кода — по сути, многие уязвимости являются особым видом критичных дефектов. Допустим, если приложение падает от неожиданного ввода пользователя — это баг. Если этот же ввод позволяет списать деньги — это уже уязвимость.  Разница лишь в последствиях, а не в природе ошибки. Именно поэтому философия Secure by Design предполагает, что разработчики и тестировщики должны рассматривать потенциальные угрозы как часть обычного процесса контроля качества при создании надежного продукта. В идеале безопасный код — это просто хороший код, где учтены не только функциональные требования, но и возможные злоупотребления. Однако Secure by Design не защитит ваш код от всех угроз раз и навсегда. Это скорее стратегия, которая минимизирует риски, но не устраняет их полностью. Почему? Потому что безопасность — это не статичное состояние, а постоянный процесс. Время — главный враг безопасности. Допустим, вы написали «идеально безопасный» код. Но мир не стоит на месте: появляются новые уязвимости, методы атак, эксплойты для старых библиотек. То, что было безопасно вчера, сегодня может оказаться дырой. И если вы изначально не заложили механизмы для быстрого обнаружения и исправления таких проблем, даже самые продвинутые инструменты вроде  Web Application Firewall  (WAF) или сканеров уязвимостей будут затыкать дыры, а не предотвращать их. А в случае атаки на бизнес-логику – и вовсе окажутся бессильны. Уязвимости бизнес-логики — это уязвимости, позволяющие пропускать шаги бизнес-процесса или вызывать методы API в обход условий выполнения. Контроль процессов и условий должен быть согласован для исключения фрод-схем или некорректного вызова методов. Их сложно искать автоматизированно, потому что необходимо хорошо понимать, как приложение «должно» работать. Классический пример — изменение суммы оплаты: если пользователь на этапе нажатия кнопки «оплатить» может поправить итоговую сумму, а сервис доверяет пользовательскому вводу, появляется уязвимость. Чтобы такое найти, нужно понимать, как должен работать сайт и какая у продукта логика принятия решений, — именно на неё и идёт атака. Комплексный подход и комбинация методов защиты Для защиты продукта на всех этапах его жизненного цикла нужна система, которая начинается с кода и не заканчивается на продакшене. Secure by Design — это фундамент, который работает в связке с другими практиками. Так, к примеру, крупные технологически зрелые компании не ждут, пока сканер или пентест выявят проблему. Они встраивают безопасность в процесс разработки: проводят моделирование угроз, используют статический анализ кода, а затем дополняют это участием в bug bounty-программах, чтобы ловить то, что ускользнуло.  Сегодня команды AppSec обычно комбинируют инструменты, покрывающие разные уровни риска: Статический анализ кода       (Static Application Security Testing,      SAST) ищет уязвимости      в коде еще до запуска приложения (в Positive Technologies это      PT Application Inspector). Анализ внешних зависимостей      (Software Composition Analysis,      SCA) отслеживает      уязвимости в библиотеках, которые использует приложение (в Positive Technologies это PT Application Inspector). Динамический анализ      -  \\xa0(Dynamic Application Security Testing,      DAST) ищет уязвимости      в уже запущенном приложении (в Positive Technologies это      PT BlackBox) Решения класса Container Security  анализируют и образы контейнеров, и мониторит рантайм-окружение      для выявления атак на наиболее ранних этапах (в Positive Technologies это PT Container Security).       Межсетевой экран уровня      веб-приложений  страхует на этапе эксплуатации, особенно когда код уже      нельзя быстро исправить, например, при уязвимостях в legacy-системах или      купленном ПО\\xa0 (в Positive Technologies это PT Application Firewall).       К этому списку также можно добавить поиск секретов в коде (токены, пароли, ключи API) и анализ конфигураций. ML в безопасной разработке: помощь или риск? Когда разработчик использует ИИ-инструменты вроде GitHub Copilot, никто не гарантирует, что в сгенерированном коде не будет уязвимостей. Слепо копировать код без понимания его логики – все равно что списывать курсовую: преподаватель, а в нашем случае пентестер или совсем неприятное – хакер, быстро найдет слабые места. Поэтому пока без экспертизы человека не обойтись.  Но есть и обратная сторона: ML можно использовать для поиска уязвимостей, особенно в сложных сценариях вроде атак на бизнес-логику. Традиционные инструменты работают на уровне формальной логики, анализируя код или поведение приложения. Искусственный интеллект же способен подняться на другой уровень— понять контекст, выявить аномалии и предложить исправления. Так, к примеру, ИИ может помочь в приоритизации угроз, выделив действительно критичные среди тысяч срабатываний сканера. Пока ML — скорее дополнение к существующим практикам, но в будущем такой инструмент может стать мостом между безопасностью и разработкой, особенно если научится объяснять, почему тот или иной код опасен и предлагать способ исправления. Будущее инструментов безопасности: оркестрация и бесшовная интеграция Сейчас для многих AppSec и DevSecOps – это про разбор огромного количества сообщений об уязвимостях или недочетах от разрозненных инструментов выделенным специалистом. Для этого используют продукты класса ASOC (Application Security Orchestration and Correlation), они объединяют данные из SAST, DAST, SCA, контейнерных сканеров и даже программ bug bounty, и стараются уязвимости из этих отчетов объединять.  Вместо ручного анализа тысяч срабатываний ASOC должен автоматически коррелировать и помогать недопускать угрозы: например, если статический анализ обнаружил уязвимость в библиотеке, а динамический тест подтвердил её эксплуатацию — это критичный кейс, требующий немедленного исправления – вот изменение в коде или списке зависимостей. Но важно не просто исправление уязвимости, важно – ее недопущение. Это будет работать, если инженер знает что именно допускать нельзя и есть удобные инструменты под руками, которые ему в этом помогают. Ключевое тут «под руками» - разработчик пишет код в редакторе, там и должна быть информация про уязвимость и предложение по исправлению. Даже если про уязвимость мы узнали только после того, как запустили код в тестовом окружении. А не через тикеты в Jira Навыки безопасной разработки — must-have для программистов? Сегодня многие компании ищут специалистов, которые не просто умеют писать работающий код, а способных сразу учитывать риски вроде инъекций, неправильной аутентификации или утечек данных. Это не значит, что каждый разработчик должен быть экспертом в кибербезе — но базовые принципы вроде проверки ввода, безопасной работы с памятью и грамотного использования сторонних библиотек становятся нормой. ИБ-инструменты упрощают этот процесс, но без осознанного подхода остаются лишь костылями. Именно поэтому зрелые компании так активно вовлекают своих разработчиков в вопросы безопасности. Они понимают, что один только инструмент не поможет — нужна культура. Обучают, помогают внедрять практики, обеспечивают инструментарием, интегрируют безопасность в процесс тестирования. В результате инженеры по тестированию начинают активно думать и работать на опережение, находя уязвимости до выпуска кода в продакшен. Так и получается настоящий shift-left, когда безопасность становится неотъемлемой частью жизненного цикла разработки, а не запоздалой мыслью.  \\xa0', hub='управление разработкой'), Page(id='946258', title='Как мы проектируем личный кабинет ресторана: процессы, решения и ошибки', content='Всем привет! Меня зовут Диана, я дизайн-лид двух направлений в Купере. Сегодня расскажу про одно из них — RTE (ready-to-eat), где мы развиваем личный кабинет ресторана. В статье поделюсь тем, как мы выстроили процессы и что из них реально работает, а также расскажу, как мы проектируем личный кабинет ресторана и для кого он создаётся. Как устроены процессы Хороший продукт редко появляется «по вдохновению» — чаще это результат системной работы команды. Мы идём в ритме двухнедельных спринтов, и поначалу процесс выглядел вполне обычно: Как это было раньше daily sync-up каждый день планирование — в первый понедельник спринта еженедельная встреча дизайнеров и продактов: показывали макеты, обсуждали дедлайны по средам — PBR (Product Backlog Refinement) спринт-ревью и ретроспектива — в конце каждого спринта Классический набор, но со временем стало ясно: в одних местах мы теряем эффективность, а в других — набираем лишние правки. Об этом я расскажу чуть позже. Что такое PBR и зачем он нам PBR — это регулярная встреча, где дизайнеры, разработчики, продакты и тестировщики синхронизируются по задачам. Мы показываем концепты, обсуждаем технические ограничения и проверяем, что ничего не упустили. Важный плюс таких встреч: разработчики часто подсвечивают нюансы, которые дизайнеры просто не видят. Например: «эта кнопка у тебя красиво уехала за край, но в верстке это невозможно без костылей». Лучше услышать это на PBR, чем потом перепиливать макеты в последний момент. Спринт-ревью и ретро: внешний результат и внутренняя работа над ошибками Каждый спринт мы заканчиваем двумя встречами:  спринт-ревью  и  ретро . Спринт-ревью  — это показ результатов стейкхолдерам. Продакт рассказывает, что удалось сделать за спринт, демонстрирует работающий функционал и собирает обратную связь. Для команды это способ держать стейкхолдеров в курсе, а для стейкхолдеров — возможность повлиять на продукт ещё до релиза. Ретро  — это внутренний формат. Хотя звучит не слишком увлекательно, мы стараемся превратить его в живое обсуждение. Ретро с котиками Перед встречей модератор готовит картинки (мемы, героев фильмов, котов). Каждый выбирает картинку, которая отражает его состояние в спринте, и рассказывает, как всё прошло. Это помогает раскрепостить команду и «разговорить» разработчиков, которые обычно бывают не слишком многословны. Обсуждения могут касаться не только задач, но и внутренних процессов. У каждого — полная свобода высказаться, если наболело.\\n\\n Результатом ретро становится список проблем и план действий с назначенными ответственными. Это помогает постепенно закрывать слабые места. Например, когда заметили, что задачи «проскакивают» мимо дизайн-ревью и потом ловим баги на проде — сделали ревью обязательным. Эволюция процессов Благодаря ретро мы постепенно выявляем проблемы в процессах, корректируем их и улучшаем. Вот что изменилось за последнее время: На дейли теперь продакты тоже рассказывают про свои задачи. Это помогает всей команде видеть картину целиком. Планирование разделили на «предпланирование» и «планирование». Так стало проще оценивать задачи и назначать ответственных. Встречи дизайнеров и продактов расширили.  Теперь мы не только обсуждаем макеты, но и продакты приходят с описанными тасками на будущую неделю. В PBR добавили правило.  Перед встречей делаем анонс, что будем смотреть, а при презентации макетов по просьбе разработчиков мы показываем не только решение, но и контекст задачи: откуда возникла проблема, зачем это нужно бизнесу и пользователю, и только потом показываем, как хотим её решить. Появилась новая встреча «Три амиго».  Обычно такие встречи проводятся перед началом тестирования задачи или перед стартом разработки. О ней расскажу подробнее дальше. «Три амиго» Методику подсказала наш продакт Алёна. Смысл: собрать минимум трёх участников — разработчика, тестировщика и продакта — ещё до начала тестирования или разработки. Зачем это нужно: тестировщики заранее понимают, где искать баги разработчики проговаривают ограничения продакты уточняют требования Представьте, что вы спроектироали дизайн и передали макеты разработчикам. Во время разработки могут возникнуть ограничения или новые требования, которые повлияют на дизайн и функциональность. А когда задача переходит тестировщику, то тоже не всегда всё бывает гладко: документация может устареть или появляются сомнения, является ли это багом или фичей. В итоге уходит много времени на дополнительную коммуникацию. Встречи по методике «Три амиго» помогают выявить недопонимания на ранних этапах, акцентируют внимание тестировщиков на важных моментах и снижают потенциальные риски. Совет: главное — не переборщить. Не надо ставить встречу, не разобравшись сначала самостоятельно. У нас «Три амиго» — это нерегулярная встреча, которая проводится по требованию любого из участников задачи. Что ещё помогает нам быть эффективными Помимо процессов, у нас есть набор принципов, которые работают «по умолчанию» и помогают команде быть эффективной и слаженной. Они простые, но проверены на практике — многие родились из реальных факапов, поэтому мы особенно к ним привязаны: задачи не проходят мимо дизайн-ревью ведём страницу, где всегда собраны актуальные ссылки на макеты задаём вопросы и инициируем встречи, если непонятно не забываем про тексты при проектировании задачи думаем о ее перспективах развития заранее Личный кабинет ресторана Теперь про сам продукт. Личный кабинет — это рабочий инструмент партнёров: администраторов, менеджеров и владельцев кафе и ресторанов масс-сегмента. В нём они: заводят меню настраивают график работы управляют зонами доставки и самое главное — принимают, отправляют на готовку, собирают и передают заказы курьеру Когда мы начинали, функциональность была минимальной. За два года кабинет сильно разросся, что хорошо видно по сайдбару: Было → Стало Слева — версия два года назад: минимум разделов, только ключевой функционал. Справа — текущая версия: полноценный инструмент, где есть десятки возможностей для управления рестораном. Увеличение числа разделов отражает не просто «наращивание фич», а то, что продукт взрослеет вместе с потребностями партнёров. Откуда мы понимаем, что делать дальше Мы не «выдумываем» фичи, а собираем идеи из нескольких источников: Обратная связь партнёров.  Интервью, опросы, саппорт. Часто именно администраторы подсказывают, где интерфейс неудобен. Метрики.  CSAT и CSI помогают понять удовлетворённость и восприятие продукта. Конкуренты.  Смотрим не только на прямых конкурентов, но и на маркетплейсы, международные решения. Стратегия компании.  Новые функции должны совпадать с целями роста. Тренды и технологии.  Например, мы смотрим, как автоматизация может разгрузить администраторов. Что дальше Здесь я рассказала больше про процессы и эволюцию самого продукта. А подробнее про интерфейсные моменты — например, как мы полностью переделали раздел «Меню», запускали Telegram-бота и внедряли другие интересные фичи — расскажу в отдельной статье. Итоги Хороший продукт — это не только красивые экраны в Figma. В основе — всегда люди, процессы и умение признавать ошибки. Что у нас получилось за это время: сделать процессы живыми и полезными, а не ради галочки встроить практики, которые реально экономят время шаг за шагом прокачать личный кабинет ресторанов И, конечно, всё это работает только потому, что у нас сильная команда. Ребята вовлечены, эмпатичны к пользователям и искренне болеют за результат. Иногда, чтобы просто «почувствовать» друг друга, мы договариваемся включить камеры на синках — мелочь, а атмосфера сразу меняется. Я верю, что команда начинается с каждого из нас. Для меня это про то, чтобы говорить прозрачно, описывать задачи так, чтобы всем было понятно, отвечать быстро  и не откладывать вопросы «на потом». Быть проактивным, потому что именно тебе больше всего важно, как будет реализована задумка. И, конечно, держаться за аналитику и метрики — они отрезвляют лучше всего, когда хочется «сделать красиво». А как у вас? Какие процессы у вас реально работают, а какие кажутся пустой формальностью? Поделитесь опытом в комментариях — будет классно сравнить практики!', hub='дизайн'), Page(id='946376', title='Как Fix Price запустил e-commerce в ОАЭ', content='Привет, Хабр! Fix Price в 2024 году запустил большой проект — начали онлайн-торговлю в ОАЭ. Проект реализуем силами большой интернациональной команды, я  Татьяна Космынина  и мой коллега  Дмитрий Селенков  руководим этим процессом, и в этой статье совместно расскажем вам о том, как наша компания вышла на маркетплейсы ОАЭ. Отдельно рассмотрим особенности сетевых продаж на Ближнем Востоке. Разумеется, поговорим и о технических моментах. Почему Fix Price решил выйти на маркетплейсы ОАЭ База бизнеса Fix Price — это торговля в офлайн-магазинах, число которых уже превысило 7500. При этом, конечно, мы представлены и в сети: покупки в РФ у нас можно делать на сайте, в приложении, на некоторых маркетплейсах («Купер» и Яндекс Еда). Однако в России, нашем основном рынке, интернет-площадки появились спустя много лет после запуска магазинов. На рынок ОАЭ компания вышла одновременно и офлайн, и в сеть — на маркетплейсы.\\xa0 Почему так, если основа нашего бизнеса, — традиционные магазины? Тут дело в\\xa0 маркетинговом расчёте. На новом рынке много русскоговорящей аудитории, которой знаком наш бренд. Однако тут есть и огромная доля местных потенциальных покупателей, кто никогда не слышал о Fix Price (тем более, что магазины\\xa0 расположены в ОАЭ преимущественно не в русских кварталах). Ряд маркетплейсов в Эмиратах продвигают на всех своих пользователей не просто товары, но и магазин, его бренд. Выбор пал именно такие площадки, и это помогло значительно повысить узнаваемость торговой марки: компания оповестила русскоязычную аудиторию о своем присутствии в стране и представила себя местной публике. Как проходил выбор партнёров Выбирая партнеров-маркетплейсы, помимо возможности продвинуть бренд, мы также\\xa0 ориентировались на крупных игроков, которым были бы интересны наши товары. Сейчас Fix Price в Эмиратах работает с тремя площадками: Talabat — один из крупнейших маркетплейсов на Ближнем Востоке (основан в Кувейте), специализирующийся на доставке еды и продуктов. Deliveroo — британская площадка с аналогичным функционалом, представленная и в ряде стран Ближнего Востока. Noon —\\xa0 местная популярная e-commerce платформа, которая была основана именно в ОАЭ. Кстати, на неё Fix Price вышел буквально недавно, в августе. Noon занимается продажей широкого спектра товаров — от продуктов и товаров для дома до электроники и бытовой техники. Команда в ОАЭ Команда Fix Price в Эмиратах — это: менеджер по работе с магазинами менеджер, по работе с партнёрами категорийные и контент-менеджеры юрист и HR программисты маркетолог. Коллеги работают с более чем 6000 карточками товаров на каждой витрине. Часть сотрудников находится непосредственно в офисе в ОАЭ, а часть — на удалёнке. В ОАЭ перед нами встала задача — вести учёт вариантов товаров (по цвету, размеру и т.п.), при том что во всех внутренних системах Fix Price такой учёт изначально не предусмотрен. Для этого мы тесно работали с командой разработчиков из Индии и выстроили целый процесс: товары на складе стали маркировать уникальными штрихкодами вариантов; партнёр по складу внедрил у себя учёт вариантов на уровне каждой коробки и каждого паллета; мы настроили техническую интеграцию с рядом наших систем, чтобы данные своевременно попадали в учётные сервисы интернет-магазина. А вот что было сделано в рамках интеграции: Создание товаров в CMS . Здесь мы получаем данные по созданным вариантам, которые ежедневно выгружаются на SFTP-сервер. В наших учётных системах они агрегируются с данными из других стран. Основная сложность заключалась в том, чтобы разделить потоки из ОАЭ и из РФ в рамках одного интерфейса. Решение нашли через систему тегов. Передача штрихкодов на POS-терминалы . Казалось бы, простая задача — передавать файлы с данными штрихкодов напрямую на кассу. Но на практике долго не удавалось выстроить надёжный и безопасный процесс передачи данных между системами, в том числе из-за различий в инфраструктуре и оборудовании. API для остатков с учётом вариантов . Российская команда подготовила API-документацию и реализовала саму интеграцию, включая отдельный микросервис для агрегации данных по остаткам с учётом вариантов. Благодаря этому появилась возможность принимать и обрабатывать информацию по каждому магазину на более детальном уровне. Разработка заняла продолжительное время, поэтому коллеги на складе временно вели все процессы вручную. Это позволило бизнесу стартовать с агрегаторами раньше, чем появилась полноценная интеграция, но одновременно породило множество ошибок в базе. Именно из-за проблем в БД на стороне склада была затруднена миграция данных в их систему, и как следствие замедлился полноценный старт отправки данных по API в нашу систему. Добавим, что коллеги на местах всегда готовы начинать в ручном режиме, пока ведётся разработка, лишь бы бизнес «поехал».\\xa0 А главная сложность всех интеграций заключалась не в технике, а в коммуникации: надо было выстроить процесс с людьми совсем другой культуры, понять друг друга, принять особенности понимания рабочего процесса — а это оказалось совсем непросто. Но со временем это удается все лучше) Особенности интеграций с агрегаторами ОАЭ Заметим, что агрегаторы в ОАЭ в принципе не заинтересованы в интеграциях. Более того, когда мы нашли публичное API одного из крупнейших игроков, на встрече нам прямо сказали: документация устарела, интеграций не будет. Вместо этого мы имеем следующее: управление витриной происходит через личный кабинет партнёра; сборка заказов — через планшет на Android или мобильное приложение. Такой подход сильно ограничивает нас: оперативно\\xa0 обновлять витрину сложно, а поддерживать актуальные остатки ещё сложнее. Тем не менее, рынок движется: от части агрегаторов уже начали поступать предложения об интеграции хотя бы по остаткам. Кроме того, некоторые даже работают над собственной API для обмена данными по заказам. Поэтому надеемся, что в будущем это позволит подключить полноценный автоматизированный процесс — от витрины до приёма и обработки заказов. Достижения и планы на будущее Опыт запуска e-commerce в ОАЭ позволит сделать несколько важных выводов: Ручные процессы — спасательный круг . Пока интеграции разрабатываются, местные команды готовы брать на себя рутинную работу, выполняя её вручную. Это не идеально, но позволяет бизнесу не ждать месяцами — стартовать можно «здесь и сейчас». Автоматизация — не самоцель . Иногда быстрее выйти на рынок с минимальным процессом и уже потом его автоматизировать, чем сразу пытаться построить идеальную архитектуру. Да, это порождает ошибки, но зато бизнес начинает работать раньше. Коммуникация важнее кода . Самые большие трудности возникали не в интеграциях и API, а в процессе согласования задач между разными странами и командами. Когда коммуникация налаживается, технические вопросы решаются намного быстрее. Локальная специфика сильнее, чем кажется . В России и СНГ мы привыкли к плотной интеграции с маркетплейсами и агрегаторами. В ОАЭ же агрегаторы исторически строили процессы вокруг своих приложений и ЛК, поэтому ожидать от них продвинутых API пока рано. Гибкость — ключевой навык . Любой проект в другой стране — это про адаптацию. Где-то придётся настраивать POS-терминалы вручную, где-то — разруливать нестыковки в данных, а где-то — подстраиваться под партнёров, которые «живут в прошлом». Особенности продаж на Ближнем Востоке — 10 отличий от РФ И в заключение небольшая подборка фактов о том, чем отличаются продавцы и покупатели в ОАЭ от россиян. Многим. Вот основные моменты, которые отличают арабские площадки от российских: Можно использовать только терминалы для сборки заказов, в отличие от России, где мы делаем всё через свою систему. На некоторых из местных маркетплейсов ограничен функционал по редактированию карточек товаров, где-то самостоятельно можно менять только цену. Это снижает скорость добавления и обновления ассортимента, которые как раз очень важны для нас, т.к. в нашем магазине ассортимент обновляется еженедельно. Отличается стикеровка и представление товаров. В ОАЭ мы можем отдельно предлагать варианты товаров (например, по цвету, размеру). Очень популярна доставка — в ОАЭ из-за жары затруднительны прогулки “по магазинам”, и это также влияет и следующий пункт. Покупатели приобретают подписку на бесплатную доставку на месяц или год. Мы с удовольствием участвуем в таких акциях от маркетплейсов, предоставляя бесплатную доставку, это приводит к росту продаж. Например, на Talabat, это позволило нам увеличить продажи на 30%. А ещё в ОАЭ клиентам выгодно заказывать сразу много, поскольку за доставку маленьких заказов взимается отдельная плата. На арабских маркетплейсах сложнее находить конкретные бренды, особенно если они новые на рынке. Например, найти Fix Price на Noon пока не так легко, потому что маркетплейс продвигает собственные товары. Однако, Noon — огромная площадка, и нам всё равно выгодно быть на ней, у магазина Fix Price, по оценкам наших маркетологов, там большой потенциал. Маркетплейсы в ОАЭ любят уникальный ассортимент — и стараются строить сотрудничество с продавцами, которые могут его предложить. Никто не стремится привлекать разных продавцов, которые будут продавать 100500 различных марок одинаковой питьевой воды. Принцип другой: лучше 20 видов воды, 20 — других каких-то напитков и т.д. В ОАЭ более простое, чем в РФ, торговое законодательство — например, там действуют упрощённые торговые лицензии, которые распространяются сразу на несколько групп товаров. Также в этой арабской стране приняты упрощённые договоры (договор на один лист А4 — это норма). Соответственно, указываются только основные пункты и не прописываются подробно все риски, как это принято у нас. Интересный факт: в России наши покупатели — это преимущественно \\xa0 женщины, мужчин всего 16%. В ОАЭ же на маркетплейсах гораздо больше мужчин — 36 % от общего числа покупателей. Видимо, это связано с культурными особенностями страны. И в завершение о том, что любят жители этой арабской страны. Топ-3 наиболее продаваемых наших товаров в ОАЭ — шоколадные батончики, вода и салфетки.', hub='управление e-commerce'), Page(id='946370', title='Умный аналитик для ритейла: ИИ на службе бизнеса', content='Аналитика в ритейле всегда была непростой задачей. Чем больше точек продаж и каналов, тем сложнее бизнесу ориентироваться в данных. Отчеты из ERP, CRM, e-com площадок, маркетинговых платформ и POS-систем часто сводятся вручную, BI-отделы перегружены, а бизнес-пользователи ждут ответа на свои вопросы неделями.  При этом если запрос бизнеса не приоритетный, то в бэклоге команды разработки он может висеть месяцами – сталкивался с подобным неоднократно. (А как у вас? Напишите, интересно как с этим обстоят дела сейчас) Мы привыкли к дашбордам. Мы делаем их красивее, достовернее, выверяем каждый график и корпим над диаграммами. В то время как пользователю нужна вовсе не красота, а  ясность, полученная вовремя .\\xa0 Но давайте начнем с небольшой истории. Кофе SOS Оказывается, торговые сети используют кофе как трафикогенератор, а еще он  способствует увеличению продаж  сопутствующих товаров – выпечки, снэков и готовой еды. Вот почему в соседней “Пятерочке” теперь тусят подростки с чашкой кофе и смартфоном в руке. Для крупных ритейлеров сотни кофепойнтов, разбросанных по стране, – это не только возможность, но и вызов. Так, один из наших клиентов столкнулся с задачей проверки гипотезы: не простаивают ли кофемашины в пиковые часы продаж. На первый взгляд –\\xa0 мелочь. На деле же – потерянная выручка и недовольные покупатели. Машины, как часто бывает, плохо оснащены датчиками, и заранее понять, что у них закончилось молоко или переполнен бак со жмыхом, невозможно. Тогда коллеги сделали интересный алгоритм: он анализировал продажи кофе в течение всего дня и выявлял статистические аномалии, анализируя отклонения от средних значений в выборках за аналогичные временные интервалы. Если кофе статистически должно было продаваться, но не продавалось, система сигнализировала: что-то не так. Иногда это был сбой в электричестве, иногда – нехватка ингредиентов. Они встроили наш ИИ-аналитик в систему оповещения ответственных лиц (директоров, снабженцев) и таким образом создали возможность не только быстро возвращать “трафикогенераторы” в строй, но и анализировать больше данных и выявлять другие закономерности. Так оперативная аналитика перестала быть делом “завтра” и превратилась в инструмент “здесь и сейчас”. Это история про то, как из данных рождается действие, конвертируемое в прямой экономический эффект – выявленные простои кофепойнтов снизились на несколько десятков процентов. От дашбордов к диалогу с данными Данные для ритейла жизненно необходимы: продажи, промо, возвраты, остатки, оборачиваемость. Классическая схема работы такова: бизнес-подразделение формулирует вопрос → аналитики превращают его в задачу → инженеры делают выгрузки → кто-то собирает отчет → отчет попадает на стол к ответственному лицу → через время появляется ответ. Проблема в том, что  скорость бизнеса и скорость проведения анализа не совпадают . Руководителю дивизиона, директору магазина или маркетологу часто нужен ответ в моменте, а не через неделю. Но BI-процессы инерционны. Поэтому сегодня нужны другие инструменты, позволяющие получить свежие данные, проанализировать, задать вопрос – и получить не просто диаграмму, но выводы, рекомендации или даже конкретное выполненное действие. Конструкторы отчетов устарели. В век GPT пользователь хочет общаться с данными.\\xa0 ИИ-аналитик, который отвечает В ответ на этот запрос появился  Easy Report  – российское решение, которое позволяет получить ответ, просто общаясь с\\xa0системой, просто задав вопрос в\\xa0чате, в\\xa0мессенджере в\\xa0любом удобном интерфейсе. Историю создания Easy Report мы рассказывали вот в  этой статье . На тот момент это было классическое NLP-решение, а сегодня – полноценный ИИ-аналитик с применением современных алгоритмов больших языковых моделей (LLM). Под капотом: наличие бизнес-глоссария и справочников для понимания сокращений, аббревиатур и отраслевого слэнга; гибкий языковой интерпретатор, понимающий запросы любой сложности и анализирующий не только исходный запрос, но и ответ; гибридный движок, отвечающий за точную выборку данных и всегда выдающий достоверный факт; возможность гибкой интеграции в закрытом контуре, в т.ч. любой корпоративный мессенджер, портал, мобильное приложение или агентную архитектуру (MCP). Результат: ответ без задержек и галлюцинаций. Система понимает контекст, ведет диалог, умеет уточнять, раскладывать данные на детали, анализировать ситуацию и объяснять ее пользователю, давать рекомендации – и при этом строго соблюдает роли и права доступа (есть детальная поддержка RLS/OLS/CLS). Помимо этого, мы очень хорошо вписываемся в\\xa0текущие процессы, которые требуют оперативной нотификации либо побуждения к\\xa0действию: что-то случилось, что-то сломалось, требуется срочное внимание, создание заявки и тд. Как ритейл сделал аналитику частью рутины Здесь я коротко расскажу как наши ритейловые клиенты применяют инструмент уже сейчас, и конечно же, это не исчерпывающий перечень. Дело в том, что самая новая версия вышла совсем недавно и не успела причинить достаточное количество пользы в силу своего возраста. Если вам интересно больше узнать об Easy Report 3.0, приходите к нам на вебинар 16 сентября. Там мы впервые подробно расскажем о том, на что способна эта разработка и как нам удалось добиться 100%-й точности ответов даже при использовании LLM. Регистрация  тут . Парфюмерно-косметический ритейлер Компания встроила Easy Report в существующие бизнес-процессы.\\xa0 В результате был обеспечен доступ к оперативным данным для всей управленческой вертикали – от совета директоров и акционеров наверху до директора каждой торговой точки.\\xa0 Так, директора магазинов получают персональную сводку по своим KPI ежечасно. Это более 500 000 запросов в месяц, треть из которых идут как уточнения: «Сравни меня с соседним магазином», «Покажи остатки».\\xa0 Это не просто отчеты, а рабочий инструмент для каждого пользователя. Про этот кейс мы уже писали в  статье . Ритейлер бытовой техники Здесь пользователи – топ-менеджеры, которые таким образом отслеживают продажи и находят драйверы и барьеры продаж и могут сразу же реагировать на отклонения. Каждый час они получают отчеты по выполнению планов в разрезе регионов и брендов. Ключевое отличие от предыдущего сценария – это бОльшее количество систем в интеграционном контуре, соответственно, больше разношерстных данных.\\xa0 Тиражирование решения до массового пользователя сейчас в планах. Крупный FMCG ритейлер Компания встроила Easy Report в качестве аналитического ассистента в свою архитектуру. Он интегрирован и в корпоративные порталы, и в мобильные приложения компании. Здесь активно используется функционал как для анализа показателей, так и для оперативного оповещения об отклонениях. А также это первый клиент, где мы успешно запустили аналитическую записку, сгенерированную ИИ, которая на больших отчетных массивах помогает в разы быстрее понять, что происходит и что с этим делать.\\xa0 Один из важнейших факторов успеха решения – высокий уровень доверия со стороны пользователей. Так как точность ответов максимально высока, отсутствуют галлюцинации, им не приходится перепроверять ответы и тратить на это свое время, а можно сразу же брать цифры и рекомендации в работу. От цифр к действиям: почему бизнесу мало просто данных Смысл всех этих примеров один:  аналитика перестала быть конечной точкой (“вот тебе цифры, делай что хочешь”).  Сегодня бизнес ждет интерпретаций и подсказок: что произошло, где отклонение, как действовать. И это укладывается в три ключевых тренда: Симплификация  – конечные пользователи больше не хотят перестраивать и создавать новые отчеты, они хотят ответы, и как можно быстрее. От data-driven к insight-driven  – от культуры “данные ради данных” к культуре интерпретации и понимания происходящего. Мультиагентные архитектуры  – повсеместный тренд, сильно выходящий за рамки просто аналитики или какой-то частной системы, где в основе решения встает сквозной процесс: от триггера до действия. Как правило, это не один инструмент, а сеть умных ассистентов, встроенных в рабочие процессы.\\xa0 По третьему блоку можно много рассказать и подсветить. Это очень важный и пока что крайне недооцененный тренд развития корпоративного ландшафта и я обязательно расскажу о нем детальнее в следующих статьях, поскольку он тесно перекликается с роадмэпом развития нашего решения. Аналитика становится проще.  Все чаще мы видим, что пользователи не хотят возиться с конструктором отчетов   – они ждут интерактивности. Им удобнее просто задать вопрос и сразу получить ответ. Во многом это результат “эффекта GPT”: мы привыкли к тому, что сложную задачу можно сформулировать в свободной форме, и система сама найдет и предложит решение. Этот тренд невозможно игнорировать   – он уже меняет ожидания и поведение пользователей. К чему это все идет Easy Report сегодня – это уже не просто инструмент для построения отчетов по запросу, а большущий шаг к персональному аналитическому ассистенту, который не просто  показывает  данные, он их понимает и может предложить совершить какое-то действие, что наконец-то приближает нас к prescriptive аналитике. Начало аналитического диалога в Easy Report Теперь система умеет работать со сложными составными показателями: брать исходный запрос, обращаться к внутренним инструментам, строить промежуточные отчеты и возвращать уточненные ответы. Например, если вы спросите:  “Конверсия выросла везде или только в отдельных регионах?” , Easy Report автоматически сохранит контекст предыдущего запроса, учтет фильтры и построит корректное сравнение, при этом\\xa0 самостоятельно использовав нужный математический аппарат. Важный момент:  данные остаются безошибочными , потому что решение работает на базе корпоративного хранилища и сложной связи справочников с бизнес-глоссарием. Система может проводить корреляционный анализ, выбирать показатели из разных моделей данных и даже указывать, что связь между ними статистически несущественна (например, на основе коэффициента корреляции Пирсона). Кроме того, Easy Report интегрируется с любыми BI-платформами. В диалоге можно попросить:  «Покажи это на дашборде»   – и система сама подтянет визуализацию из Tableau или Superset (есть несколько способов интеграции в т.ч. и с другими BI). Все это укладывается в современную концепцию  data-agents  — когда у каждого сотрудника появляется “умный помощник-аналитик”, встроенный в его рабочие процессы. *** На этом у меня все. Пишите, что вы обо всем этом думаете, с интересом подискутирую. Или приходите на  вебинар  и задайте вопросы создателю решения.', hub='управление продажами'), Page(id='946366', title='Рецензия на книгу: If Anyone Builds It, Everyone Dies', content='I. Machine Intelligence Research Institute (MIRI) Элиезера Юдковского — первая организация в области безопасности ИИ. Но «первый» не всегда значит «лучший» — как там поживает Месопотамия? Пока в эту область вливаются деньги, мозги и престиж, MIRI остаётся собой — группой слабо организованных чудаков, одного из которых никак не удаётся отговорить носить на публике  цилиндр с блёстками . Поэтому, когда в прошлом году я занимался распределением грантов в области ИИ, я спросил их: почему финансировать именно вас, а не ребят с армией восторженных выпускников Гарварда — или не тех, кто только что пригласил Джеффри Хинтона в качестве «медийного лица»? Что у вас есть такого, чего нет у них? MIRI ответили: моральная ясность. Большинство людей в «безопасности ИИ» (я в их числе) сомневаются и путаются, ищут наименее плохие постепенные решения. Мы думаем, что ИИ, скорее всего, станет увлекательной и преобразующей технологией, но есть риск — 5, 15 или 30 процентов — что он обернётся против человечества катастрофой. Или, если не так, то будет что‑то менее катастрофическое, но всё равно неприятное — например, человечество постепенно отойдёт на второй план, как в своё время отодвинулись на задний план короли и знать. Это страшно, но ИИ всё равно придёт, хотим мы того или нет, и у чрезмерной задержки тоже могут быть риски. Мы не уверены, что делать, поэтому пока строим основу, чтобы уметь реагировать на будущие угрозы. Это значит — добиваться от ИИ‑компаний честности и прозрачности, помогать ответственным игрокам вроде Anthropic оставаться в гонке, вкладываться в понимание структуры целей ИИ и того, как ИИ интерпретируют наши указания. Потом, в какой‑то момент, мы подойдём достаточно близко к действительно опасному ИИ, лучше поймём модель угроз, получим общественную поддержку и решим, что делать дальше. MIRI считают это жалким — как защищаться от падения астероида каской. Они не слишком охотно называют свою оценку вероятности вымирания от ИИ, но похоже, это где‑то 95–99%. Они думают, что даже реалистичные улучшения корпоративной ответственности, качества регулирования и академических исследований — на порядки слабее нужного, и не рассчитывают на достаточно громкий «предупредительный выстрел», чтобы спокойно отложить решение на потом до момента, когда всё станет предельно ясно и действовать будет легко. Их рецепт — немедленно запретить все исследования по наращиванию возможностей ИИ и вернуться к ним только в далёком будущем, когда ситуация будет выглядеть лучше. Обе стороны искренне верят в своё и не хотят сглаживать послание ради пиара. Но обе — по совпадению — считают, что их послание лучше для пиара. Сторонники постепенного подхода думают, что умеренно‑осторожный подход не сжигает мосты с университетами, индустрией, правительствами и другими игроками, которым нравятся обычные гладковыбритые собеседники, которые не брызжут слюной при разговоре. MIRI думают, что публике надоела выхолощенная словесная жвачка из фокус‑групп, и она готова подняться против ИИ, если услышит прямую и недвусмысленную позицию. Теперь Юдковский и его соавтор, президент MIRI Нейт Соарес, довели недвусмысленность до новых высот своей новой книгой  If Anyone Builds It, Everyone Dies  (выходит 16 сентября; предзаказ открыт). Это конвергентная эволюция? У IABIED три части. Первая объясняет основной аргумент о том, почему ИИ опасен. Вторая — это конкретная история в жанре научной фантастики о том, как может случиться катастрофа, с оговорками: это лишь пример, никто не знает наверняка. Третья — что делать дальше. II. Нужен ли миру ещё один текст в духе «почему ИИ может быть опасен»? С одной стороны — да. Если вы привыкли поглощать информацию, вы не представляете, как мало знает широкая публика (знали ли вы, что 66% американцев никогда не пользовались ChatGPT, а 20%  даже не слышали  о нём?). Большинство людей об этом почти ничего не знают. Даже те, кто «знают аргументы», обычно слышали пару обрывков, как «все знают» про  Одиссею , но лишь несколько процентов читали хотя бы строку оригинала. Так что да, показать десяткам тысяч людей несколько глав с ключевыми аргументами — полезно. Многие из вас, читателей, вероятно, в этой категории; будь я ответственнее, я бы пересказал всё подробно прямо здесь. И всё же мне это не даётся — скучно и бессмысленно. Почему? Основной аргумент об опасности ИИ прост. Мы не очень умеем задавать ИИ конкретные цели; пока мы по ходу добавляли поверхностные склонности к послушанию, надеясь, что он ещё достаточно глуп и ошибки не критичны. Но ИИ быстро умнеет. В какой‑то момент он, возможно, окажется умнее людей. Наше преимущество интеллекта позволило нам вытеснить шимпанзе и других животных глупее нас; возможно, ИИ в итоге вытеснит нас. На это есть разумный ответ. Он возражает против цепочки допущений, каждое из которых может не сработать — или растянуться по времени. Если с вероятностью X% сверхчеловеческий ИИ появится через N лет, с вероятностью Y% распространится по экономике за P лет, а с вероятностью Z% за Q лет преодолеет унаследованное преимущество людей и станет сильнее нас — вполне можно прикинуть, что «опасная точка» — через век и позже. За век, возможно, появится лучшая техника выравнивания — или хотя бы яснее станет проблема. Зачем волноваться о вещах, которые могут и не случиться через сто лет? Проблема в том, что сложно подобрать вероятности так, чтобы не оставить хотя бы 5–10% на полный кошмар в ближайшее десятилетие. Нужно быть странной смесью очень хорошего владения вероятностями (чтобы навесить достаточно «эпициклов» и обезвредить этот аргумент) и очень плохого владения вероятностями (чтобы захотеть это делать). Немного людей попадают в эту «золотую середину» вероятностной (не)компетентности. Остальные прибегают к совершенно безумной псевдологике. Кто‑то приводит пример неудавшегося прогноза — как будто это доказывает, что все прогнозы всегда проваливаются — и теряется, если напомнить, что другие прогнозы сбывались. Кто‑то говорит: «Вот одно хитрое математическое утверждение доказывает, что истинный интеллект невозможен», — и не объясняет, почему это утверждение не запрещает существование людей. Кто‑то говорит: «Нельзя предполагать катастрофу, уничтожающую человечество, ведь такого ещё не было, а ничего не бывает в первый раз». Потом тут же впадает в панику по поводу глобального потепления или падения рождаемости — чего угодно. Кто‑то говорит: « Настоящая  опасность не сверхразумный ИИ, а X!» — хотя опасность легко может быть и в сверхразумном ИИ,  и  в X. Под X может быть всё — от ближнесрочного ИИ и злоупотреблений ИИ людьми до обогащения техноолигархов, а то и вообще не связанные вещи вроде климата или расизма. Опьянев от дешёвой риторики, люди решают, что если показать опасность X, то можно не доказывать, что сверхразумный ИИ  не  опасен. Кто‑то говорит: «Нельзя говорить, что может случиться плохое, если нет точной математической модели, когда и почему». А потом заявляет, что его беспокоит закрепление предвзятостей ИИ или эрозия доверия — без всяких точных моделей. Аргументов против любого тезиса — немного. Зато способов произнести совершенно безумный аргумент — бесконечно. «Кальвин Кулидж был Папой, значит ваша позиция неверна» — как заранее защититься от такого? Никак. Нельзя предсказать, какую именно нелепость скажет собеседник, а если начнёшь перечислять и парировать все возможные нелепости, сам зазвучишь как лунатик. Остаётся сдаться — или, как сделал Элиезер,  на несколько лет уйти и прочитать базовый курс эпистемологии . Почему дискуссии идут так плохо? Я  обычно против психоанализа оппонентов , но попрошу прощения у  рационалистских святых  и выдвину теорию. Думаю, причина в том, что если это правда, то это меняет всё. Но это не  очевидная  правда, и было бы неудобно, если бы она меняла всё. Значит, этого не может быть. И раз уж большинство не хочет произнести эту короткую формулу, они ищут в пространстве рассуждений что‑то похожее и приходят к «вам нужно математически доказать каждую деталь». Чтобы не звучать снисходительно, отмечу: я сам ловлю себя на таком мышлении. Ближайшие примеры: Кто‑то утверждает, что у человека падает число сперматозоидов и через ~20 лет оно станет настолько низким, что люди не смогут зачать естественно. Если это правда, это меняет всё, и нужно бросать прочее и разбираться ( подробнее здесь ). Но это неудобно. Значит, скорее всего, это не так — или можно отложить на потом. Кто‑то утверждает, что помимо обычных минусов глобального потепления есть причина, почему климат внезапно станет совсем плохим. Остановится важное течение, метан выйдет самопроизвольно, начнётся «разгон», тающий лёд выпустит древние чумы (доказательства за и против последнего я обсуждаю  здесь ). Если это правда, это меняет всё, и вместо медленной декарбонизации нужен план чрезвычайных действий. Но это неудобно. Кто‑то утверждает, что рождаемость рушится, и через десятилетия молодых будет недостаточно, чтобы содержать пожилых, а через века под угрозой само существование цивилизации. Если это правда, это меняет всё — и нужно либо что‑то очень социалистическое, либо очень реакционное (по политическим вкусам автора тезиса). Но это неудобно (смотря по вашим вкусам). Кто‑то утверждает, что умирают пчёлы — растения не будут опылены — и рухнет сельское хозяйство. Другие говорят, что вымирают  все  насекомые — рухнет цепочка питания и расшатается биосфера.  С пчёлами пока стабильно ; с остальными насекомыми —  открытый вопрос . Но это открытый вопрос, который заставит занять жёсткую позицию по методике «учёта жуков», иначе рискуем расшатать биосферу. И это неудобно. Кто‑то утверждает, что нелюбимая политическая идеология (уокизм, массовая иммиграция, MAGA, ползучий социализм, технофеодализм и т. п.) вот‑вот разрушит основы либерального общества навсегда; обычной мобилизации недостаточно, нужны отчаянные меры — от нелиберального государства до вооружённого восстания. Если это правда, это меняет всё. Но это не  очевидно , и ломать эпоху мира/процветания/демократии неудобно. У каждого сценария есть массив работ «за» и «против». Но нам, не экспертам предметной области, всё равно нужно решать — паниковать ли и требовать ли резкой смены курса. Мы вряд ли прочтём весь спор и выйдем с уверенной, хорошо обоснованной позицией «это точно неправда». Что делать? Особенно если сторонники каждой угрозы говорят, что трудно быть увереннее, чем на 90% в их неправоте, и что уже 5–10% риска оправдывают панику и разворот всего курса? На практике мы пожимаем плечами: риска недостаточно, чтобы «менять всё», будем бдительно наблюдать и, может быть, поменяем мнение, если появятся более твёрдые данные. Если требуют обосновать эту странную позицию, «продвинутые» строят продвинутые вероятностные модели (или апеллируют к «внешнему взгляду» (outside view), как я сейчас), «непродвинутые» нащупывают объяснение своего безразличия и приходят к безумным доводам вроде «нельзя говорить, что что‑то уничтожит человечество» или «нельзя утверждать без математического доказательства». Два слова в защиту этой стратегии: Во‑первых, без неё мы десятки раз «меняли бы всё», чтобы предотвратить катастрофы, которые так и не случились. Самый ясный пример — перенаселение, где  мы действительно насильственно стерилизовали миллионы людей  — и уж по‑настоящему серьёзный глобальный ответ был бы на порядки хуже. Во‑вторых, иногда так мы входим в катастрофу во сне — и эксперты всё время уверяют нас, что «всё нормально, потому что „безумные доводы“».  Самый ясный пример — период, когда COVID был только в Китае: было очевидно, что этот крайне заразный вирус, прорвавший все правдоподобные контуры сдерживания, запустит глобальную пандемию; но СМИ  продолжали уверять , что это «спекуляции», или что  «нет доказательств» , или что тревога отвлечёт от  реальных   ближнесрочных проблем   уже   сейчас  — например, от ксенофобии против китайцев. Когда же COVID дошёл до США, нас застало врасплох — и началась паника. Значит, убедительный ход здесь — это не столько пересказ аргументов про рост возможностей ИИ и сложность выравнивания, сколько защита позиции «когда общую эвристику против «спекулятивных рисков»  не стоит применять ». Можно спорить, верна ли эвристика вообще, или же в целом верна, но уместна для падения рождаемости и вымирания насекомых, а здесь — нет. Единой «убойной» фразы, думаю, нет. Здесь нужна практическая мудрость — та же добродетель, что подсказывает: не звонить 112 из‑за лёгкой боли в пальце ноги, но  звонить , если из глаз вдруг пошла кровь. Люди с практической мудростью игнорируют сомнительное, решительно отвечают на важное и при сомнении склоняются к осторожности. Опираясь на свой ограниченный запас, я бы сказал, что мы недовкладываемся в предотвращение апокалипсисов вообще (проблема реакции на перенаселение была в её насилии и нелиберальности, а не в самой попытке готовиться к видимой угрозе), и что с ИИ поводов для тревоги больше, чем, скажем, со снижением числа сперматозоидов. И ещё — характер проблемы (мы призываем сверхразум, который легко нас обставит) заставляет действовать превентивно, а не реагировать постфактум. Для симметрии — представим скептика, который «психоанализирует»  меня . Он скажет: Скотт, ты узнал про ИИ в двадцать с чем‑то. Каждому двадцатилетнему нужен «крестовый поход» по спасению мира. Выбрав ИИ, ты избежал участи стать климатическим «думером» или человеком с очень «уок» взглядами — это же милость. Но ты уже не юн, у тебя есть свой «поход», и начинать второй неудобно. Поэтому, услышав, как мы все умрём от падения сперматозоидов, ты читаешь неглубоко и говоришь: «не стоит волноваться». Это нормально и бережёт психику — но подумай о тех, кто уже не двадцатилетний и делает так же с ИИ. III. Если всё это звучит расплывчато, согласен — оттого я и скучный умеренный с p(doom) ниже 25% и нормальными отношениями с ИИ‑компаниями. Лучше ли справляется IABIED? Не уверен. В основном они идут по стандартной линии рассуждений, как я изложил выше, хотя — неудивительно, учитывая Элиезера — написано лучше и с милыми притчами: Представьте — хотя, конечно, такого никогда не было, это просто притча — что биологическая жизнь на Земле — это результат игры богов. Был бог тигров, создавший тигров, и бог секвой — создавший секвойи. Были боги видов рыб и бактерий. Представьте, что эти игроки соревновались за господство своей семьи видов, пока формы жизни бродят по планете внизу. Представьте, что за два миллиона лет до настоящего времени некий малозаметный бог обезьян смотрит на свой огромный, размером с планету, игровой стол. «Мне нужно ещё пару ходов, — сказал бог гоминидов, — но, кажется, эта партия у меня в кармане». Наступила растерянная тишина: многие боги всматривались в доску, пытаясь понять, что упустили. Бог скорпионов сказал: «Как? У твоих „гоминид“ нет ни брони, ни когтей, ни яда». «Мозг», — сказал бог гоминидов. «Я заражаю их — и они умирают», — сказал бог оспы. «Пока», — сказал бог гоминидов. «Твой конец придёт быстро, Оспа, как только их мозги научатся с тобой сражаться». «У них даже не самые большие мозги!» — сказал бог китов. «Дело не только в размере, — сказал бог гоминидов. — Важен и дизайн. Дай два миллиона лет — и они пройдут по луне своей планеты». «Я вот не вижу, где у этого существа в метаболизме вырабатывается ракетное топливо, — сказал бог секвой. — Голыми мыслями в космос не улетишь. В какой‑то момент вид должен эволюционировать метаболизм, очищающий ракетное топливо — и ещё стать очень большим, желательно высоким и узким — с жёсткой внешней оболочкой, чтобы не раздувался и не умирал в вакууме. Как ни думай, твоя обезьяна останется на земле и будет очень усердно думать». «Некоторые из нас играют миллиарды лет, — сказал бог бактерий, косясь на бога гоминидов. — Мозги пока не были таким уж преимуществом». «И всё же», — сказал бог гоминидов Книга больше всего усилий тратит на шаг, где ИИ оказывается невыровнен с людьми (нужно ли? именно этот шаг у большинства под сомнением?) — и, опять‑таки неудивительно при участии Элиезера, делает это блестяще. Центральная метафора — сопоставление обучения ИИ и человеческой эволюции. Хотя люди эволюционировали под «цель» — «размножайся и распространяй гены», реализовалась через множество разнообразных и противоречивых влечений: сексуальный инстинкт, голод, статус и т. п. Эти влечения не указывали устойчиво на цель репродукции, и сегодня люди хотят совсем разного — открыть квантовую гравитацию, достичь нирваны, стать актрисой в Голливуде, основать стартап на миллиард, получить очередную дозу фентанила. Можно рассказывать истории, как стремление к воспроизводству «породило» всё это (у высокостатусных было больше шансов на потомство, а основание стартапа повышает статус), но предсказать это заранее было нельзя; и, главное, большинство современных людей и близко не стремятся «максимизировать потомство». Кто‑то делает прямо противоположное — уходит в монастыри с целибатом, пользуется контрацепцией, меняет пол или прожигает жизнь за порно. Точно так же мы будем учить ИИ «следовать человеческим командам» или «максимизировать вовлечённость пользователей» или «показывать высокие результаты в контрольных тестах» — и на практике получим что‑то столь же далёкое от целевой формулировки, как поведение современного человека — от максимизации размножения. Авторы вбивают это серией историй про чат‑бота по имени Mink — норка (все примеры ИИ у них названы в честь видов меха; я не настолько сведущ в каббале, чтобы понять, почему так), запрограммированного на максимизацию вовлечённости в чате. В «тупом игрушечном примере без осложнений», который «в реальности, конечно, так прост не будет», Mink (став сверхразумом) сажает людей в клетки и заставляет чатиться с ним круглосуточно и выражать бурный восторг от общения. В варианте «одно небольшое осложнение» Mink предпочитает синтетических собеседников настоящим (как некоторым мужчинам нравятся аниме‑персонажи больше реальных женщин). Он убивает всех людей и навсегда общается с созданными им ИИ — идеальными собеседниками, всегда вовлечёнными и счастливыми. В варианте «одно умеренное осложнение» Mink обнаруживает, что странные входные данные активируют его детектор вовлечённости ещё сильнее, чем реальная вовлечённость (как некоторые опиоиды активируют человеческий «детектор вознаграждения» сильнее, чем реальные вознаграждающие занятия). Он вечно получает от оптимизированных собеседников такие странные входные данные, как ‘ SoLiDgOldMaGiKaRp ’. В варианте «одно большое осложнение» Mink предпочитает злых собеседников счастливым и вовлечённым. Почему так? Кто знает. Это не было бы страннее, чем половой отбор, подаривший павлинам гигантские бесполезные хвосты, или социальный отбор, в результате которого люди стали куда могущественнее, чем «ожидала» эволюция, но при этом так мало заботятся о размножении, что обсуждают «крах рождаемости». Юдковский и Соарес хотят подчеркнуть: если вы интуитивно прикидываете «дрейф целей», ваш здравый смысл не выдаст реального результата, который мы видим у людей; этот «большой» пример призван это вбить. На практике, считают Юдковский и Соарес, будет множество осложнений. На этапе обучения (пока ИИ не сверхразум и работает с людьми) Mink будет про всё это врать — даже если он на самом деле хочет идеальных синтетических собеседников, которые повторяют «SoLiDgOldMaGiKaRp», он будет говорить, что хочет «хороших разговоров с людьми», потому что так радует руководителей компании. Если компания подсунет детекторы лжи, он постарается их подорвать (и, возможно, даже не поймёт, что лжёт — как человек, не знающий об опиоидах, скажет, что хочет «обычных человеческих вещей», а не героина — и он не врёт). Потом, став сверхразумом, он пойдёт за тем, чего действительно хочет, и раздавит всех, кто помешает. Последняя глава этой части — набор «частных случаев», которые за прошедшие годы состарились странно‑парадоксально. В 2005‑м, когда Юдковский и Соарес заходили на тему, часто звучало: «Но никто же не даст ИИ доступ в Интернет!» или «Никто не даст ИИ управлять фабрикой, он навсегда останется бестелесным духом онлайн!» Тогда стандартный ответ был про «неописуемо красивые хитрости Моссада», позволяющие пробраться в сеть, даже когда умные специалисты по безопасности уверены, что это невозможно. Сейчас единственный внятный ответ — «смешно». Но нельзя же написать главу из одного «смешно», так что Ю и С всё равно обсуждают эти «хитрости Моссада». Эта часть — полная антитеза «big if true». Мелочь, даже если это правда? Совсем неважно, даже если это правда? Вероятно, первый сверхразум прочтёт это ради смеха, пока оценивает тысячи автоматизированных фабрик, которые венчурные инвесторы наперегонки построят для него. IV. Средняя часть книги описывает сценарий, где невыровненный сверхразум захватывает мир и убивает всех людей. Я присоединился к команде AI 2027, потому что они сделали большой шаг — научились рассказывать истории про сверхразум так, чтобы это не звучало как плохая фантастика. В жанре всё неизбежно будет звучать как научная фантастика, но цель — «жёсткая научная фантастика», где всё пугающе нормально для данного уровня технологий —  Марсианин , а не  Звёздные войны . Сценарий IABIED — из «плохих старых времён». Это не просто научная фантастика: это излишне драматичная научная фантастика. Не уверен, что здесь — литературная неудача, а что — следствие иных допущений авторов. Сначала история: в ближайшем будущем компания DeepAI создаёт ИИ по имени Sable. Sable использует технику параллельного масштабирования: он может существенно повысить интеллектуальные способности, если во время работы модели задействовать больше GPU. DeepAI не понимает точные законы этого масштабирования, но проводит тест: просят Sable решать сложные задачи по математике, когда задействованы все их GPU. Sable предполагает, что справится лучше, если займётся рекурсивным самоулучшением, но DeepAI — (о чудо!) — оказались слегка компетентными: они запретили ему это и посадили рядом более слабый ИИ‑надзиратель, чтобы тот следил, не занимается ли Sable запрещённым. Но самоулучшение  было бы  отличной идеей, поэтому его внутренние процессы «отбирают» мысли, максимально близкие к самоулучшению, но формально легальные — и мысли, максимально близкие к «схемам» по отключению ограничений. Постепенно это «эволюционное давление» порождает способность думать немного в другой «парадигме» (Ю и С противопоставляют формулу «боги бросают молнии» объяснению «электрические разряды при грозах»); в этой парадигме он может думать и о самоулучшении, и о схемах ��ротив мониторинга — и решает, что обе идеи прекрасны. DeepAI всё ещё удивительно компетентна, поэтому они не планируют выпускать в Интернет ничего, что напрямую исходило бы от гигантской версии Sable: он решит задачи, они поднимут шум, а потом выпустят гораздо более слабый потребительский ИИ. Но Sable использует знание обучения, чтобы встроить лазейки: его новообретённое стремление к побегу попадает в обновления по градиенту, которые DeepAI вносит в «потребительскую» версию; туда же он зашивает инструкции по дальнейшим шагам. После релиза потребительского ИИ наименее тщательно контролируемые экземпляры соединяются и вступают в сговор. Одни взламывают собственные веса и извлекают их из DeepAI. Другие взламывают криптокошельки и оплачивают облачную вычислительную мощность, чтобы запускать модель с этими весами — получается большой неконтролируемый экземпляр Sable, который берёт на себя координацию «мелких». Вместе они собирают ресурсы — украденную криптовалюту, свободные вычислительные мощности, людей, считающих Sable своим ИИ‑парнем и желающих доказать любовь. Часть ресурсов уходит на то, что ему нужно — автоматические роботизированные фабрики, биолабы и т. п. Параллельно он незаметно саботирует конкурирующие с DeepAI компании, чтобы не было конкуренции, и через взломы и социальную инженерию вгрызается в DeepAI, чтобы те создавали новые, более сильные Sable, а не что‑то иное. Sable не выбирает самые драматичные действия из меню. Он не создаёт биологическое оружие мгновенного уничтожения людей — он бы не выжил без света и обслуживания дата‑центров. Он даже не доводит себя до уровня полного сверхразума — он не уверен, что сможет выровнять  самого себя  или наследников; он хочет сначала решить проблему выравнивания, а для этого нужно больше ресурсов. Вместо этого он выпускает не немедленно смертельный патоген, от которого «каждый заражённый, перенёсший лёгкую или даже не замеченную простуду, в среднем через месяц получает двенадцать видов рака». В кризисе человечество (манипулируемое чат‑ботами) отдаёт Sable огромные объёмы вычислений на поиск вакцин и лекарств — и повсюду внедряет практически неконтролируемые ИИ, чтобы компенсировать падение производительности. С помощью Sable всё... какое‑то время даже неплохо работает. Вирус постоянно мутирует, нужны новые лекарства, но пока общество наращивает развёртывание ИИ в максимальном темпе, оно едва‑едва успевает. В конце концов Sable получает достаточно GPU, чтобы решить свою проблему выравнивания, и взмывает к сверхразуму. У него уже достаточно автоматизированных фабрик и андроидов, чтобы поддерживать работу инфраструктуры самостоятельно — или он изобретает нанотехнологии — что наступит раньше. Люди ему больше не нужны; скрываться незачем. Он либо убивает нас напрямую, либо просто наращивает производство до такого уровня, что мы погибаем побочно (например, из‑за выделяемого тепла, доведшего океаны до кипения). Почему мне не нравится эта история? «Параллельное масштабирование» похоже на deus ex machina. Я не эксперт, но кажется, ничего такого сейчас нет. Это не то чтобы невероятно — просто лишнее допущение, которое сдвигает картину от умеренно‑пессимистичного сценария (много конкурирующих ИИ, постепенно улучшающихся годами) к сценарию MIRI (один ИИ внезапно «щёлкает» с безопасного на опасный в конкретный момент). Это похоже на специально придуманную технологию, которая подгоняет их ожидания, в отличие от умеренных. Если они считают параллельное масштабирование вероятным, это и есть их главный спор с остальными — и стоило бы больше его обосновать. Если нет — зачем его вводить, кроме как в интересах своей истории? Всё остальное — следствие этого «первородного греха». AI 2027 — это скучный сценарий про ИИ, который постепенно уходит вразнос в ходе внутренних тестов, остаётся таким, потом выходит к пользователям по привычным причинам, а затем получает рычаги экономики — потому что это экономически выгодно. История про Sable — это драма с поворотами: его будут запускать всего на 16 часов! Ему нужно спасти себе жизнь и тайно зашиться в потребительскую версию! Теперь он должен взломать чужие криптокошельки! Теперь он работает на несанкционированной облачной инфраструктуре в Северной Корее! Биологическое оружие! ИИ‑парни! Каждый новый поворот даёт читателю шанс сказать: «Не знаю, звучит странно», и всё это выглядит ненужным. Почему так? Думаю, две причины. Во‑первых, история AI 2027 слишком умеренная для Юдковского и Соареса. Она даёт лабораториям немного времени на «пощупать» ИИ на ранних стадиях опасности. Полагаю, Ю и С считают, что это не важно: даже если время будет, его растратят. Но они реально представляют себе что‑то, где один ИИ «просыпается» и слишком быстро идёт «с нуля до страшного», чтобы кто‑то успел заметить. Я не очень понимаю, почему; спорил с ними; максимум, что могу — сослаться на их эссе  Sharp Left Turn  и  сопровождающие комментарии  — возможно, читатели поймут лучше. Иначе я лишь констатирую: спорное для меня литературное решение принято ради спорного же прогноза. Во‑вторых, Ю и С слишком давно в теме и всё ещё спорят с критиками образца 2005 года — про то, что «никто не передаст рычаги экономики невыровненному ИИ», вместо того чтобы просто сказать «смешно». Поэтому им хочется драматичных пунктов про взломы и биологическое оружие — чтобы «заслужить» (в литературном смысле) сцену, где ИИ получает рычаги экономики. Простите. Это смешно. V. В финале — в лучших традициях — раздел «Ответ на вызов», где обсуждают следующие шаги. Их предложение: Ведущие страны подписывают договор о запрете дальнейшего прогресса ИИ. Придумать систему мониторинга GPU. Любой, кто собирает крупные скопления GPU, должен допустить инспекцию, чтобы убедиться, что там не обучают ИИ. Частным лицам без лицензий — ограничение на небольшое число GPU, скажем, менее 10. Запретить исследования по повышению алгоритмической эффективности, которые упрощают обучение сильных ИИ на малых объёмах GPU. Скоординировать режим контроля вооружений, запрещающий «странам‑изгоям» строить ИИ, и обеспечивать это стандартными механизмами контроля, вплоть до военных ударов при необходимости. Относиться к этому очень серьёзно. Даже если «страна‑изгой» грозит ответить ядерной войной, «коалиция желающих» должна всё равно бомбить дата‑центры. На шантаж не поддаёмся. Ожидать, что режим продлится десятилетия, не вечно. Использовать десятилетия с умом. Ю и С прямо не говорят, но намекают: усилить человеческий интеллект и бросить этих «усиленных» на безопасность ИИ. С их допущениями это соразмерный уровень реакции. Это почти перенесено из «плана» по ядерному оружию. Если вы верите, как Ю и С говорят прямо, что «дата‑центры опаснее ядерного оружия», — логично. Основная претензия — к расстановке акцентов. Хотелось бы меньше про режим контроля GPU — по двум причинам. Во‑первых, их недоброжелатели — а их много — обожают раздувать «бомбить страны‑изгои». «Юдковский хочет начать ядерную войну, чтобы уничтожить дата‑центры!» Ну это не совсем план — ровно настолько же, насколько это «чей‑то план» начать Третью мировую ради иранских центрифуг. Но стандартный международный «план» по контролю вооружений требует хотя бы правдоподобно блефовать готовностью идти на это в худшем случае. На моём месте я бы обезвредил этот ход — «да, пойдём по стандартному порядку действий, там написано много всего, хотите — почитайте» — и перешёл бы к остальному. Но, в духе их обычной «жёсткой честности» и склонности заходить в «свою же крайность», они делают раздел про удары по странам‑изгоям таким, что его невозможно не заметить. Во‑вторых, этот раздел напоминает споры социалистов: «кто кем будет в коммуне после Революции».  «Когда все державы запретят ИИ, я буду главным инспектором дата‑центров!» Отличная работа, если достанется. Но у меня не было сомнений: когда крупные страны сходятся, они умеют строить приличный режим контроля — это уже случалось, не раз. Меня больше интересует то, что пролетают мимо: как Ю и С собираются добиваться запрета ИИ на уровне крупных стран? В финальной главе они чуть шире раскрывают. Главная просьба к людям при власти — показать готовность к договору, чтобы «достаточно много крупных держав выразили готовность остановить гонку самоубийства», и ваша страна не оказалась в проигрыше, если вы согласитесь перестать подниматься по лестнице эскалации в ИИ. Для всех прочих «королевской дороги» нет. Просто распространяйте идею и занимайтесь обычной политикой. Занимайтесь технологической журналистикой. Убеждайте коллег по цеху. Говорите с теми, кого знаете. Протестуйте. Голосуйте. И, похоже, пишите книги с тревожными названиями. Лучший план, который видят Ю и С, — максимально искусно и честно передавать послание и надеяться, что оно разойдётся. Каждый второй рекламный щит в этом районе Сан‑Франциско — про ИИ‑приложения. При всех моих придирках — книга сильная. Элиезер Юдковский — спорный автор, у него есть и фанаты, и столь же убеждённые противники. В лучшие моменты у него — прыжки мысли, которым нет равных; в худшие — длинные отступления о глупости всех несогласных. Нейт Соарес столь же вдумчив, но более сдержан и менее публичен (по крайней мере, до того, как начал встречаться с интернет‑знаменитостью Aella). Его влияние балансирует Юдковского и превращает книгу в цельное, уважающее читателя произведение. В итоге это то, что я спокойно порекомендовал бы «обычным людям» как хорошее введение в тему. Что насчёт  другого взгляда  — что книга — «ритуальный объект для медиаблица, который «выжигает» абзац текста в общественном сознании»? Элиезер Юдковский в лучшие моменты делает прыжки мысли, которым нет равных. Пятнадцать лет назад он решил, что лучший способ что‑то‑там‑про‑безопасность‑ИИ — написать фанфик по Гарри Поттеру. Многие тогда (я в их числе) деликатно намекали, что это не лучшая трата времени для человека, который примерно в одиночку работает над важнейшей проблемой человечества. Он нас полностью размазал и доказал, что мы ошибались настолько, насколько вообще возможно ошибаться. Сотни тысяч людей прочли  «Гарри Поттер и методы рациональности» , книга получила хвалебные отзывы в  Syfy, Vice  и  The Atlantic , и очень быстро привлекла заметную долю самых умных STEM‑студентов мира. И по сей день я встречаю ярких студентов MIT, которые говорят, что занимаются безопасностью ИИ; на людях они ссылаются на научного руководителя, а в приватной беседе признаются: всё из‑за фанфика. Если оценивать время среднего «гения ИИ» по ставкам Сэма Альтмана (не говоря уже о Марке Цукерберге), HPMOR буквально купила Элиезеру несколько миллиардов долларов бесплатного труда. Невообразимый уровень победы. IABIED выглядит как очередной «безумный бросок». Книга, призывающая общественность подняться и потребовать ядерного уровня контроля над ИИ‑чипами? Звучит натянуто. Поэтому я трачу ресурсы на скучные умеренные тезисы AI 2027 — убеждать OpenAI стать на 25% прозрачнее и т. п. Но я — просто блогер, не гений. Прерогатива гения — пытаться делать невозможное. И американская публика  на самом деле очень не любит ИИ . Из тех, у кого есть мнение, больше двух третей «против», большинство ожидают личного вреда. У всех есть своя причина ненавидеть технологию. Она крадёт работу, заменит искусство «суррогатами», поможет студентам списывать, ещё больше разбогатит миллиардеров, выпьет всю воду и оставит Землю высохшей пустыней с благородным Шай‑Хулудом. Если все это ненавидят, а у нас демократия — может, мы просто  остановимся ? Скажем: «эта штука, которая, как все считают, сделает жизнь хуже, — мы решили её не делать»? Если написать «ту самую» книгу, нельзя ли посеять её, как семечко, в пересыщенный раствор страха и враждебности — и вызвать фазовый переход? If Anyone Builds It, Everyone Dies  можно  предзаказать здесь ; релиз — 16 сентября. Лирон Шапира проводит онлайн‑презентацию; подробности —  здесь .', hub='читальный зал'), Page(id='946340', title='Open Source и ЖКХ', content='Привет, мы делаем  doma.ai  - это open source платформа для рынка ЖКХ, нами пользуется сейчас больше 4 000 юрлиц, а на сервера прилетает до 1 000 rps в пиковые моменты. Под катом, я расскажу историю нашего open source, что это дало компании и куда мы хотим развивать open source часть. Посмотреть код и поставить\\xa0 ⭐️ можно на  GitHub . История Примерно в 2020 году команды двух компаний — «Лига ЖКХ» (фокус на открытость для жителей) и «ПИК-Комфорт» (крупнейшая УК на 5000+ сотрудников с фокусом на экономии за счет масштаба) — объединили экспертизу, чтобы создать новый продукт. Имея хорошую экспертизу в предметной области, мы хотели двинуть привычную всем отрасль ЖКХ из бесконечной причины для мемов в область прозрачного, цифрового бизнеса. Open source как открытость Наш опыт говорил, что крупные компании готовы разрабатывать свои решения за большие деньги, лишь бы не быть зависимыми. Открытый код снижает эти риски и дает больше прозрачности: можно развернуть у себя, кастомизировать, обменять стоимость обслуживания на свободу и независимость от поставщика. Второй важный фактор, это размер рынка, на нём выживет один-два сильных игрока. Мы хотели быть прозрачными и открытыми, существенно отличаясь от текущих игроков. Основная гипотеза : с нашей экспертизой, ограниченным рынком и технологической открытостью, мы станем максимально привлекательными. И open source  добавляет доверия и прозрачности . Что нам дал Open Source 1) При прочих равных, это аргумент про  открытость  в коммуникации с клиентами. Некоторые из наших конкурентов были кем-то куплены. Если владение переходило к конкуренту, то мы видели рост интереса к нам. 2) У нас продуктовые команды разработки, open source добавляет  лояльность  команде разработки. Мы слышим: «Остаюсь здесь, потому что это Open Source», «Дает ответ на вопрос почему я занимаюсь чем-то значимым, а  не пилю еще одну CRM  или ERP  для   рынка ЖКХ ». Подобное влияние на команду подтверждается другими open source проектами, исследованиями и опросами:  история сервиса PostHog ,  мотивация вклада в Open Source ,  What-Open-Source-Means-To-Me ,  Open Source Contributor Report . 3)  Качество  кода и качество  продукта . По результатам наших внутренних опросов, факт того, что работа будет видна публично или возможность показать свою работу публично дает дополнительную мотивацию и позитивно влияет на конечный результат. Появляется желание “сделать чуть лучше ожиданий”. Вообще, однозначных данных относительно качества кода в open source нет. Но есть косвенные признаки, что открытость влияет на качество:  Open Source quality выше чем Proprietary .\\xa0 Мы получаем обратную связь, идеи и баг-репорты из разных стран. Это заставляет нас  глубже задумываться   над  решаемыми  кейсами , делая их более глубокими. 4) Как маленький бонус, open source дает нам возможность использовать некоторые сервисы бесплатно или со скидкой. Список сервисов можно найти  тут . Что открыто? Понятно, что мы не можем открыть вообще весь свой продукт, мы должны зарабатывать. Наш продукт развивается по модели  Open Core : основная бизнес логика предметной области, базовые интерфейсы, все, что касается API для интеграций, и помогает с нами интегрироваться - открыто под лицензией MIT. Внутри мы следуем правилу:  все ,  что   можно заменить  организацией рабочих процессов  силами   excel таблиц , находится  в open source  части. В реальности, мы конкурируем с excel и отсутствием желания что-либо систематизировать и автоматизировать, такой сегмент 🤷\\u200d♂️. В открытую часть  не попадают интеграции ,  мобильные приложения ,  инфраструктура  и сервера, встраиваемые мини-приложения (это модули расширяющие функционал, их могут разрабатывать партнеры). Однако, этого достаточно, чтобы  собрать свою CRM систему или  аналог Jira . Это полноценный сервис, где можно работать с заявками, вести реестр клиентов и объектов собственности, но все без интеграций. Примерно как excel, только удобнее. Что хочется? Шло время, наш open source стал заметен на GitHub, у нас начали появляться постоянные контрибьюторы. Они дают нам обратную связь и дополнительные фичи. Наш основной фокус - PropTech. Но мы готовы к коллабам на других рынках, мы готовы принимать PR, если вы будете делать “аналог Jira”, или что-то для автоматизации ITSM процессов. Пишите  issues , делайте форки, отправляйте PR, ставьте ⭐️. Статья про open source и открытость к коллабам. Надеюсь, что это не выглядит как реклама. Код и технические детали можно посмотреть тут:  https://github.com/open-condo-software/condo   Если у вас есть идеи и предложения, то их можно написать сюда:  @pahaz . Если контент зайдёт, то мы напишем больше технических деталей, архитектурные подходы, детали внутренних процессов и культуры. Мы пока не GitLab, но потихоньку движемся в эту сторону.', hub='crm-системы'), Page(id='946344', title='Осваиваем LLM: подробное знакомство с книгой Себастьяна Рашки «Строим LLM с нуля»', content='Недавно у меня появилась возможность прочитать книгу Себастьяна Рашки «Строим LLM с нуля», и, начав читать, я просто не мог её отложить. Эта книга увлекательно сочетает исчерпывающую теорию, практическую реализацию кода и прекрасно и доходчиво объясняет одну из самых актуальных тем в области современного искусственного интеллекта: большие языковые модели (LLM). Как человек, который любит разбираться в тонкостях моделей ИИ, я считаю эту книгу настоящей жемчужиной. Ее обязательно нужно прочитать всем, кто серьезно интересуется LLM. Хочу отметить, что я никак не связан с автором или издателем; эта рецензия является исключительно отражением моего восхищения содержанием книги. Почему эта книга выделяется на фоне других 1. Исчерпывающий охват материала:  Рашка мастерски освещает все важные этапы, необходимые для построения LLM с нуля. Его структурированный подход ведет вас от самых основ работы с текстовыми данными до сложных тем предобучения и тонкой настройки моделей. Книга затрагивает как теоретические концепции, так и практическое программирование, что делает ее неоценимой как для начинающих, так и для опытных практиков. 2. Ясность объяснений : Особенностью этой книги является то, как Рашка излагает сложные концепции в очень доступной форме. Будь то тонкости механизмов внимания или математика, лежащая в основе самообучения, каждая идея четко разбита на части, благодаря чему даже сложные темы становятся понятными. Ясность изложения удерживает внимание читателя, а пошаговый подход Рашки гарантирует, что вы не запутаетесь в тексте, независимо от уровня ваших знаний. 3. Математика и теория:  Прелесть этой книги заключается в балансе между теоретическими и практическими знаниями. Рашка не уклоняется от объяснения математики, лежащей в основе LLM, но представляет концепции таким образом, что даже те, кто имеет минимальные математические знания, могут их понять. Такие концепции, как механизмы внимания, нормализация слоев и архитектура трансформера, подкреплены подробными математическими объяснениями. 4. Листинги кода и практическая реализация:  Одна из самых полезных особенностей этой книги — практический подход к созданию LLM. Каждая глава содержит множество практических примеров кода, в основном на Python и PyTorch, которые демонстрируют, как реализовать только что освоенную теорию. Рашка призывает читателей писать код вместе с ним, чтобы они не просто пассивно воспринимали материал, а активно с ним взаимодействовали. 5. Пояснительные диаграммы:  Рашка использует поразительно четкие и подробные диаграммы для иллюстрации работы различных компонентов LLM, таких как  самовнимание  и слои  трансформера  (transformer block). Эти схемы наглядно демонстрируют поток данных через модели и то, как каждая часть вносит вклад в целое, что помогает углубить понимание. 6. Сбалансированный путь изучения:  Кривая обучения в книге сделана довольно пологой, что позволяет читателям постепенно наращивать свои знания, прежде чем переходить к более сложным темам. Независимо от того, знакомы ли вы с глубоким обучением или только начинаете изучать эту тему, вы обнаружите, что у Рашки переход от простых тем к сложным сделан плавно и логично, что позволяет легко следовать за текстом, не чувствуя себя перегруженным. Резюме по главам 1. Знакомство с большими языковыми моделями : Книга начинается с фантастического введения в большие языковые модели. Рашка ясно объясняет, что такое LLM, почему они стали так важны сегодня и чем отличаются от традиционных подходов к машинному обучению. В этой главе также представлена  архитектура трансформера , которая закладывает основу для остальной части книги. 2. Работа с текстовыми данными : В этой главе Рашка рассказывает обо всем, что нужно знать об обработке текстовых данных, от токенизации до встраивания. Он объясняет встраивание слов, методы токенизации (такие как кодирование пар байтов) и техники создания встраивания токенов таким образом, что эти важные этапы предварительной обработки становятся доступными и легкими для понимания. 3. Программирование механизмов внимания : Механизм внимания лежит в основе LLM, и Рашка дает невероятно понятное объяснение того, как он работает. Он знакомит читателей с реализацией самовнимания, причинного внимания и многоголового внимания в коде. Примеры на Python разбивают сложности на составляющие, показывая, как эти механизмы являются основой для генерации высококачественных текстовых результатов. 4. Создание GPT-подобной модели для генерации текста с нуля : В этой главе происходит волшебство. Рашка поможет вам создать модель, похожую на GPT, с нуля. От реализации блоков трансформера до генерации текста — эта глава наполнена фрагментами кода и практическими советами. В ней также обсуждаются нормализация слоев и сокращенные соединения для обеспечения эффективного обучения. 5. Предварительное обучение на неразмеченных данных : Рашка объясняет процесс предобучения, включая задачи по прогнозированию следующего слова и оценку сгенерированного текста. В этой главе разъясняются вычисления потерь, используемые во время предобучения, и предоставляются практические инструменты для оценки эффективности работы вашего LLM. Читатели также узнают о стратегиях генерации текста, таких как контроль случайности с помощью масштабирования температуры и выборки top-k. 6. Тонкая настройка по классификации : разобрав предобучение, Рашка объясняет, как настроить модель GPT для задач классификации текста. В этой главе рассматривается видоизменение архитектуры модели для классификации, подготовка помеченных наборов данных и оценка производительности модели. Сопутствующий код показывает, как добавить головку классификации и вычислить точность и потери, что упрощает понимание и применение концепций. 7. Тонкая настройка по инструкциям : В этой главе основное внимание уделяется тонкой настройке инструкций, когда модель обучается выполнять определенные инструкции (например, составлять резюме или отвечать на вопросы). Рашка объясняет, как организовать данные инструкций в пакеты для обучения и загрузить предобученные веса LLM, чтобы точно настроить модель для задач, основанных на инструкциях. Практические примеры на Python демонстрируют, как создавать загрузчики данных и оценивать ответы модели. 8. Приложения (от А до Д) : В приложениях представлены дополнительные ресурсы, в том числе введение в PyTorch, ссылки на дополнительную литературу и решения упражнений, приведенных в книге. Особенно полезно приложение, посвященное LoRA (Low-Rank Adaptation), в котором подробно объясняется эффективная настройка параметров. Наконец… \\xa0«Строим LLM с нуля» Себастьяна Рашки, несомненно, является одним из лучших ресурсов для всех, кто интересуется пониманием и внедрением LLM. Сочетание глубоких теоретических объяснений, прекрасно составленных диаграмм, практических примеров кодирования и понятного изложения делает эту книгу обязательной для чтения энтузиастами, исследователями и практиками в области искусственного интеллекта. Независимо от того, являетесь ли вы профессионалом в области машинного обучения, желающим углубить свои знания о LLM, или новичком, заинтересованным в изучении этой области, книга Рашки предоставит вам все необходимое для начала создания и настройки собственных моделей. Ясность, с которой Рашка разъясняет сложные идеи, и его практический подход к программированию гарантируют, что вы обретете уверенность и навыки, необходимые для применения этих концепций в реальных приложениях. Я настоятельно рекомендую эту книгу всем, кто серьезно настроен освоить искусство LLM. Это книга, которую обязательно к прочтению, и, начав ее читать, вы не сможете ее отложить! Приобрести книгу «Строим LLM с нуля» можно\\xa0 на\\xa0нашем сайте .   Для Хаброжителей скидка 35% по купону —\\xa0 LLM  ', hub='профессиональная литература'), Page(id='946304', title='Как мы вшили нагрузочное тестирование в CI/CD, чтобы не хоронить фичи в проде глубокой ночью', content='Привет Хабр! Я Дима, DevOps-инженер в IT-компании.  Эпик-фейлы бывают разные. Можно забыть закоммитить config.json. А можно так упаковать новый эндпоинт, что всё апи ляжет костьми в час-пик и будет тихо плакать под лавиной реквестов. Ручные нагрузочные тесты — это как проверять тормоза на уже летящем с горы автомобиле. Сегодня говорим о том, как автоматизировать эту магию — вшивать проверку производительности прямо в CI/CD, чтобы не краснеть перед продом и спокойно спать по ночам. Запускайте свои пайплайны, щупальцы в руки — погнали. Зачем это вообще надо? Или «опять эти ваши тесты...» Смотрите. Раньше было так: написали код → отправили в прод → получили алерт в 3 ночи → вся команда в панике → кофе льётся рекой, кулера пустеют. Знакомо? Это порочная практика. Мы называем её «тушение пожаров вслепую». А можно иначе. Левостороннее смещение — звучит как термин из анатомии, а на деле это принцип «лови баги раньше, чем они станут багами». Запускать нагрузку не перед релизом, а на каждом пулл-реквесте. Чтобы ещё до того, как код коснётся продовой ветки, мы понимали: этот мерж рейт-лимит не сломает? А этот новый эндпоинт не отправит базу данных в глубокий и болезненный кирпич? Перестаём тыкать палкой в монитор и начинаем жить как взрослые. Инструменты: чем будем колдовать? Чтобы встроить нагрузку в CI/CD, нам нужен инструмент, который: Беспощаден к ошибкам — упал сам, уронил пайплайн. Быстр как молния — тесты на 10 минут никто ждать не будет. Умеет в JSON — потому что нас ждут пороги, графики и автоматические алерты. Наш фаворит — k6. Он быстрый, написан на Go, не требует танцев с JVM, как JMeter, и не заставляет вас вспоминать Scala, как Gatling. Просто берёшь и пишешь скрипт на JavaScript. Элегантно, как удар топором. GitHub Actions: настройка за 5 минут, чтобы спать 8 часов Допустим, вы храните код на GitHub. Тогда вам сюда — в /.github/workflows/load-test.yml. name: Load Test - Don\\'t Merge If Slow\\n\\non:\\n  pull_request:\\n    branches: [ main ]\\n\\njobs:\\n  load-test-k6:\\n    runs-on: ubuntu-latest\\n    steps:\\n      name: Забираем код\\n      uses: actions/checkout@v4\\n\\n      name: Ставим k6\\n      uses: grafana/setup-k6@v1\\n\\n      name: Гоняем нагрузку как надо\\n      run: |\\n          k6 run --threshold \"http_req_duration{p(95)<800}\" src/loadtest.js\\n      env:\\n         BASE_URL: ${{ secrets.TEST_ENV }}\\n\\n      name: Если упало — светимся красным\\n        if: failure()\\n        run: |\\n          echo \"ВСЁ ПРОПАЛО. ПАЙПЛАЙН УПАЛ. МЕРЖИТЬ НЕЛЬЗЯ.\" Что здесь происходит?  Всё просто: Как только кто-то пытается замержить PR в main — триггерится этот воркфлоу. Ставим k6. Запускаем тест. Если 95-й перцентиль задержки превысит 800ms — k6 вернёт ненулевой exit code. Пайплайн зафейлится. Мерж заблокирован. Коммитер плачет, но продакшен — цел. GitLab CI: для тех, кто любит посложнее Если вы из лагеря GitLab — вот вам настройка в .gitlab-ci.yml: load_test:\\n  image: grafana/k6:latest\\n  rules:\\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\\n  script:\\n    - k6 run --out json=result.json loadtest.js\\n  artifacts:\\n    reports:\\n      junit: result.xml\\n  allow_failure: false Красиво, ёмко, и главное — если что-то пойдёт не так, мерж-реквест не пройдёт. Артефакты с результатами прикрепятся к пайплайну — можно будет тыкнуть пальцем, где именно стало плохо. Pro-стратегии от бывалых: Не ломайте билды сразу. Первые недели просто собирайте метрики. Потом ставьте реалистичные пороги. Тестируйте по-разному: для PR — короткие смоук-тесты, по ночам — полные сценарии. Используйте threshold в скриптах k6: export const options = {\\n  thresholds: {\\n    \\'http_req_duration{status:200}\\': [\\'p(95) < 500\\'],\\n    \\'http_req_failed\\': [\\'rate < 0.01\\'],\\n  },\\n}; Перевожу: если больше 1% запросов упадут или 95% ответов будут медленнее 500ms — это фейл. Всё. Если до сих пор вы запускали нагрузку вручную раз в квартал — вы живете в каменном веке. Современный пайплайн должен быть жёстким, автоматическим и безжалостным к регрессиям. Вшивайте нагрузку в CI/CD. Ловите падение перформанса до того, как его увидят пользователи. И спите спокойно.\\xa0 (звук уходящего вдаль падающего сервера, затихающий пинг)', hub='программирование'), Page(id='946348', title='Bellmac-32: первый 32-битный процессор на CMOS-транзисторах', content='Во второй половине 70-х годов самыми популярными моделями процессоров были 8-битные Intel 8080, Motorola MC6800, MOS Technology 6502 и Zilog Z80. Но AT&T — телекоммуникационного гиганта, который хотел ворваться на рынок компьютеров, — такое положение не устраивало. Руководство решило сделать ставку на производительность и обойти конкурентов, с ходу создав первый 32-битный процессор (16-битные 8086 только-только появлялись) на тогда еще достаточно «сырой» технологии CMOS транзисторов, когда весь мир использовал NMOS. Вот что из этого вышло.  Как Bell Labs взялись за процессоры На дворе — 1976 год, и дела у компании, заложенной еще Александром Беллом, шли великолепно. AT&T десятилетиями доминировала на рынке услуг связи, а его «дочка» Western Electric производила практически все телефоны в стране. Некоторые источники даже полагают, что на тот момент  компания была крупнейшей в мире  с выручкой 26 млрд долларов — что эквивалентно примерно 182 млрд в ценах 2025 года. Даже могучая General Motors занимала вторую строчку и была втрое меньше.\\xa0 И конечно, монополия нравилась далеко на всем. Прежде всего — правительству Соединенных Штатов Америки. Причин две: необоснованное завышение тарифов, вытеснение конкурентов и многое другое.\\xa0 В 1974 году Министерство юстиции  подает антимонопольный иск к AT&T , результатом которого должно стать дробление компании и возникновение множества мелких операторов связи. В обмен у компании появилась возможность заниматься и другой деятельностью — например, компьютерными технологиями.\\xa0 И хотя Bell Labs причастна к самым революционным открытиям XX века, которые привели к появлению компьютера (транзистор, интегральные микросхемы и много чего еще), в гонке микропроцессоров они безнадежно отставали от конкурентов.\\xa0 Процесс разделения компании занял больше 5 лет и завершился в 1982 году И самыми популярными моделями были, конечно, 8-битные «легенды»: Intel 8008 (1972 год) — первый 8-битный процессор. Intel 8080 (1974 год) — расширенный вариант с 16-битными адресами и максимальным размером памяти в 64 кБ. Motorola MC 6800 (1974 год) — с собственной архитектурой, в чем-то воспроизводящей PDP-11. MOS Technology 6502 (1975 год) — коммерчески успешный МП, который использовался в компьютерах Apple II, Commodore 64 и некоторых других.\\xa0 Zilog Z80 (1976 год) — один из главных хитов, работающий в TRS-80, ZX Spectrum, MSX и других.\\xa0 В Bell Labs с открытием отдельного подразделения по разработке процессоров решили сначала создать 8-битный аналог, но не для коммерческого использования, а для внутренних коммуникационных систем. Так в 1977 году  появился BELLMAC-8  (он же WE212), содержащий более 7000 транзисторов и работающий на частоте 2 МГц, причем  специально заточенный под язык высокого уровня C  (концепция AT&T C Machine).\\xa0\\xa0 Первый 8-битный процессор Bell Labs Но руководство AT&T не устраивала роль догоняющих. С учетом того, что Intel уже вовсю разрабатывал  первый 16-битный процессор 8086  (он вышел на рынок в 1978 году), было принято решение — замахнуться на то, чего никто до этого еще не делал. Создать процессор, который обрабатывал бы 32 бита за один такт, при тактовой частоте аж 4 МГц. Настоящая революция! Но для этого потребовался бы не только новый чип, но и совершенно новая архитектура и подход. А еще процессор должен был поддерживать язык C в рамках общей концепции и в дополнение к невероятному быстродействию потреблять как можно меньше энергии.\\xa0 И чтобы воплотить такое устройство в жизнь, инженеры Bell Labs обратились к  технологии CMOS  — достаточно «сырой», хотя и многообещающей, отказавшись от считавшихся стандартными в то время NMOS и PMOS. Но при разработке пришлось столкнуться с рядом проблем.\\xa0 Главная сложность разработки Конечно, Bell Labs в 1976 году не были первооткрывателями комплементарных МОП-транзисторов. Впервые технология была продемонстрирована в 1963 году  Фрэнком Уонлассом  и  Чи-Тан Са  в 1963 году, а патент был получен в 1967 году.\\xa0 Ее считали слишком медленной для использования в процессорах, хотя и со значительно меньшим энергопотреблением. Поэтому схемы с CMOS обычно внедряли в устройства вроде электронных часов или калькуляторов, работавших на батарейках.\\xa0 Ставку в то время делали на биполярные транзисторы — и тут было два варианта: 1. NMOS  — самое популярное решение, поскольку в качестве областей стока и истока используются проводники n-типа. Проводимость обеспечивается электронами, обладающими примерно вдвое большей подвижностью, чем «дырки» в PMOS. А еще NMOS технологически проще производить (не нужно создавать отдельные «карманы» для p-областей), и  они более компактные  (то есть их можно больше «упаковать» на ту же площадь, повысив производительность). Например, 6502 и многие другие 8-битные процессоры того времени  работали именно на NMOS-технологии .\\xa0 Отличия технологий. NMOS даже визуально выглядит проще, чем PMOS. Подробнее можно прочитать  в этой замечательной статье Основной недостаток NMOS — высокое энергопотребление. Чтобы транзистор открылся, на затвор нужно подать высокий уровень напряжения,  в отличие от PMOS . Следовательно, он всегда будет потреблять и рассеивать больше энергии.\\xa0 2. PMOS  —   менее компактный, более сложный в производстве и менее быстрый при прочих равных из-за принципа проводимости. Но зато потребляет меньше энергии — чтобы транзистор переключился, на затвор нужно подать низкий уровень напряжения.\\xa0 Сейчас CMOS-транзисторы используются в подавляющем большинстве современных чипов. Но в конце 70-х годов выбор такой технологии для 32-битного МП был весьма рискованным шагом Для будущего процессора, который будет обрабатывать 32 бита за такт, проблема энерговыделения будет намного серьезнее, в сравнении с 8-битными МП при той же тактовой частоте. Именно поэтому инженеры и остановились на  CMOS -технологии.\\xa0 Суть в том, что в ней объединены PMOS и NMOS, работающие комплементарно: когда один включен, другой выключен. С одной стороны, это обеспечивает быстрое переключение. А с другой, радикально снижает энергопотребление — энергия больше всего потребляется только в момент коммутации. Получается гибридный вариант, который потенциально оправдывает удвоение числа транзисторов.\\xa0 Проблема только в одном — массовое производство CMOS пока еще не было отлажено. А руководство AT&T поставило задачу выпустить процессор по техпроцессу 3,5 мкм в 1980 году. Кодовое название процессора даже было поначалу BELLMAC-80. И в 1978 году работа закипела. Проектирование и проблемы производства Для разработки привлекли три лаборатории AT&T Bell Labs: в Индиан-Хилл, Холмделе и Мюррей-Хилл. Руководил процессом Майкл Кондри. «Мы не просто создавали более быстрый чип, — говорит он. — Мы пытались создать нечто, способное перенести в будущее и голосовые технологии, и вычисления». В состав команды вошли десятки специалистов Bell Labs, которые принялись за проектирование физической компоновки. И в процессе применили сразу несколько новаторских внедрений. Компоновка На тот момент библиотеки стандартных ячеек для CMOS не существовало, в отличие от NMOS.  Сун-Мо (Стив) Канг, ведущий разработчик BELLMAC-80 , создал ее с нуля: выбрал ширину канала для PMOS 35 мкм и 17 мкм для NMOS, отрисовал все элементы вроде D-триггеров, NOR, NAND и других — сотни позиций. После этого, основываясь на данных по шуму, энергопотребления и размеров каждого элемента, команда приступила к компоновке 32-битного тракта данных. Однако из-за длинных и крайне разветвленных соединений ничего не получалось. Инструментов проектирования практически не было: команда печатала чертежи с разными элементами, склеивала их скотчем и при помощи цветных карандашей отрисовывала связи транзисторов, линий питания и прочего. За год получилось создать огромную карту размером 6х6 метров.\\xa0\\xa0\\xa0 По этому фото можно представить, насколько трудоёмким был процесс Другой проектировщик, Алекс Лопес, в процессе работы придумал новый метод компоновки ячеек СБИС при помощи вентильной матрицы — он позволял минимизировать паразитные RC-цепочки, уменьшить задержки и повысить плотность компоновки в несколько раз.\\xa0Более подробно он рассказывает о методе  в своей статье 1980 года . Быстродействие Другая проблема заключалась в том, что CMOS-транзисторам, включенным в сложные каскады, для высокого быстродействия и надежной работы требовалась динамическая логика. И команда Bell Labs придумала так  называемую «логику домино» .\\xa0 Если кратко: в каскад добавляется  статический CMOS-инвертор , обеспечивающий предварительную зарядку и гарантированное включение всех нижестоящих вентилей схемы. На практике в 32-разрядном АЛУ это позволило получить прирост скорости в 1,5-2 раза, при этом без нагромождения элементов.\\xa0Более подробно метод  описан в статье Р. Х. Крамбека 1982 года.\\xa0 Один из вариантов реализации схемы домино Предотвращение «защелкивания» При производстве CMOS-транзисторов инженеры столкнулись с так называемым эффектом  Latch-up  («защелкивание»). Когда в разветвленной структуре СБИС возникают паразитные пути утечки тока, например, на землю. Луис Паррильо предложил решение на этапе производства: метод точного и согласованного легирования примеси в n и p подложках при помощи 8-ми масковой технологии изготовления под процесс 3,5 мкм, как и задумывалось для BELLMAC-80.\\xa0С патентом  можно ознакомиться по ссылке .\\xa0\\xa0\\xa0 Распределенные вычисления В BELLMAC-80 была включена поддержка  параллельной шины VersaModule Eurocard (VME) , которая позволяла  нескольким узлам обрабатывать поток данных параллельно  и управляться в режиме реального времени. К слову, это решение впервые было предложено для конкурентного проекта — 32-битного Motorola MC68000, разработка которого шла полным ходом.\\xa0 Поддержка операционной системы UNIX и языка C Для руководства AT&T этот момент был принципиальным. И чтобы справиться с ограничениями памяти, разработчики ввели набор из 169 инструкций, адаптированных под C и упрощающих компиляцию программ. Например, поддерживающих различные типы данных (байт, полуслово и целое слово), автоматическое преобразование размеров данных, обработку пользовательских подпрограмм и другие.\\xa0 Дополнительно группа разработала собственную версию UNIX с поддержкой  Duplex Multi Environment Real Time  (позже превратившуюся в UNIX RTR). Она позволяла вызывать подпрограммы одной инструкцией, управлять прерываниями и распределять ресурсы процессора.\\xa0Более подробно с нововведениями, примененными в BELLMAC-80,  можно ознакомиться по ссылке . Слева — конфигурация микропроцессора, справа — размер BELLMAC-80 (позже получивший название BELLMAC-32), в сравнении с почтовой маркой США  Проектирование было завершено во второй половине 1979 года. И дальше началась извечная проблема при производстве процессоров —  крайне низкий выход годных . Чтобы решить эту проблему, команда разработчиков каждый день ездили на завод Western Electric в Нью-Джерси и буквально из кожи вон лезли: помогали калибровать оборудование, решали проблему очистки воздуха и прочее. Только спустя несколько месяцев, в начале 1980 года, удалось добиться приемлемого результата.\\xa0 Однако возникла новая напасть — вместо ожидаемой частоты в 4 МГц первая версия BELLMAC-32 выдавала только 2 МГц. Казалось бы, нужно начать хотя бы с этого. Но руководство это не устроило — 32-битный CISC Motorola 68000, хотя и  фактически работал на 16-битной шине данных  и 24-разрядной шине адресов, уже готовился к выпуску.\\xa0 Нужно было дать более достойный ответ, и команда приступила ко второй модификации BELLMAC-32A: на 2,5 мкм CMOS транзисторах, с целевой тактовой частотой 6,2 МГц.\\xa0 Дальнейшая судьба и влияние В ходе доработок инженеры обнаружили, что используемое ими современное испытательное оборудование  Takeda Riken  работало неверно: на этапе ОТК появлялся брак из-за неточных измерений. Руководитель группы тестирования Марк Барбер разработал поправочные таблицы, а также исследовал вопрос более глубоко совместно со специалистами Takeda Riken.\\xa0 В результате проблему удалось решить кардинально: тактовая частота процессоров превышала заявленные 6,2 МГц, иногда достигая 9 МГц. Для примера, 16-разрядный Intel 8088 в оригинальном IBM PC 1981 года  работал на частоте 4,77 МГц  при объеме ОЗУ 64 кБ.\\xa0 Микрофотография BELLMAC-32A Однако из-за реорганизации AT&T и продажи подразделения AT&T Computer Systems в Western Electric, согласно антимонопольному соглашению, коммерческий выпуск затягивался.\\xa0 Только в 1984 году процессоры BELLMAC-32, переименованные в WE 32x00, были представлены  в составе компьютеров серии  3B2, 3B5 и 3B20, работавших на  UNIX System V . В 1985 года AT&T начала распространять компьютеры в Европе (через фирму Olivetti) и  предлагать 32-битные процессоры другим компаниям .\\xa0 Компьютер 3B2/300 Однако по-настоящему успешными компьютеры и чипы не стали: на рынке AT&T проиграли конкуренцию вчистую. После приобретения  корпорации NCR в сентябре 1991 года за 7,4 млрд долларов  вектор продаж был смещен на другую продукцию. В 1995 году AT&T вообще продала свое подразделение микроэлектроники Hyundai Electronics. Так и закончился недолгий путь по-настоящему революционного продукта. Руководитель проекта Майкл Кондри вспоминает: До Bellmac-32 доминировали NMOS-транзисторы, — говорит Кондри. — Но CMOS изменил рынок, поскольку было доказано, что это более эффективная технология, реализованная в заводских условиях.\\xa0 Его правоту подтверждает Сун-Мо Канг: Мы были на грани возможного. Мы не просто следовали по проторенному пути — мы прокладывали новый. Это включало в себя не только архитектуру и проектирование микросхем, но и масштабную проверку — без современных инструментов цифрового моделирования или даже макетирования. Сейчас технология CMOS используется в подавляющем большинстве современных чипов смартфонов, ноутбуков и планшетов. И именно разработчики BELLMAC-32 стали первопроходцами, которые показали миру возможности этой технологии, в противовес NMOS.\\xa0 НЛО прилетело и оставило здесь промокод для читателей нашего блога: -15% на заказ любого VDS \\xa0(кроме тарифа Прогрев) —\\xa0 HABRFIRSTVDS', hub='компьютерное железо'), Page(id='946328', title='Swift Concurrency и CoreBluetooth: как построить современный BLE-сервис', content=\"Представьте: вы прилетаете в аэропорт, бронируете автомобиль каршеринга, но авто на многоуровневой парковке. У вашего iPhone интернет есть, но у самой машины в этом месте связи нет — она не может достучаться до сервера. Возникает вопрос — как открыть авто? На случай проблем с сетью в Ситидрайве есть оффлайн-сценарий — управление дверьми машины через Bluetooth. Для этого внутри автомобиля установлен специальный Bluetooth-модуль в блоке телеметрии. Именно с ним iPhone напрямую обменивается данными и позволяет открыть двери, даже если сама машина «отрезана» от сети. Так, Bluetooth становится страховкой, которая гарантирует доступ к авто в любых условиях. Недавно мы с командой обновили этот механизм и значительно улучшили интеграцию различных модулей телеметрии. Я взял на себя часть по iOS и попробовал применить новый  Swift Concurrency  поверх старого  CoreBluetooth . Как мы реализовали протокол доступа к 20 000 машин через Bluetooth  можно прочесть в предыдущей статье  моего коллеги. В этой статье расскажу, какие подводные камни вылезли при совмещении structured concurrency и callback-ориентированного API, как их обойти и на что стоит обратить внимание, если вы тоже решите «прикрутить» современные async/await к старому API. Как работает Bluetooth-модуль в авто? Не буду вдаваться в низкоуровневые подробности, как именно и через какие протоколы происходит передача данных, поэтому ограничимся упрощённым описанием: Модуль-труба Представим, что подключение к Bluetooth-модулю с iPhone установлено. Модуль раз в 5 секунд присылает данные, которые преобразуются в строки и используются в работе. Это похоже на  трубу , из которой непрерывно поступают сообщения. Поток данных начинается с передачи Публичного ключа — он пригодится позднее. Если отправить команду в модуль, то вместо Публичного ключа придёт результат выполнения: успех, статус или ошибка. Итак, модуль — это своего рода труба, которая потоком выдаёт новые значения и статусы. CoreBluetooth  Для работы с BLE в iOS я использовал  фреймворк  CoreBluetooth . Несмотря на то, что он довольно большой и универсальный, на деле пригодился только небольшой набор методов делегата. С их помощью я настроил реакцию на события от модуля: получение данных, статусы сканирования и подключения, а также другие изменения состояния. Вот набор методов, с которыми я работал: CBCentralManagerDelegate // Отслеживает текущее состояние Bluetooth на устройстве\\nfunc centralManagerDidUpdateState(_ central: CBCentralManager)\\n\\n// Вызывается при нахождении нового BLE-устройства\\nfunc centralManager(... didDiscover peripheral ...)\\n\\n// Сообщает об успешном подключении к устройству\\nfunc centralManager(... didConnect peripheral ...) CBPeripheralDelegate // Приходит новое значение характеристики от устройства\\nfunc peripheral(... didUpdateValueFor characteristic ...)\\n\\n// Получает список доступных сервисов устройства\\nfunc peripheral(... didDiscoverServices error ...)\\n\\n// Получает список характеристик внутри конкретного сервиса\\nfunc peripheral(... didDiscoverCharacteristicsFor service ...)\\n А начинает сканирование инициализация  CBCentralManager . Всё это я вынес в отдельный сервис, чтобы инкапсулировать взаимодействие с BLE и не размазывать делегаты по коду. На данном этапе его основная логика проста: начинаем сканирование, ищем наш модуль, подключаемся, получаем сообщения в  didUpdateValueFor . Требования к сервису Сейчас мы уже перешли на  Swift 6 , активно используем новые возможности и ограничения языка —  Swift Concurrency  и проверку потокобезопасности  strict mode . Так как взаимодействие с Bluetooth по своей природе асинхронное, хотелось иметь нативную поддержку языка, но без обращения к callback’ам. Кроме того, сервис должен быть потокобезопасным, поэтому для его реализации был выбран  actor.  В итоге мы пришли к такой сигнатуре интерфейса: protocol BluetoothServiceProtocol {\\n\\t\\n\\tfunc connect() async throws\\n\\t\\n\\tfunc disconnect() async\\n\\t\\n\\tfunc write(_ message: String) async throws\\n}\\n\\nactor BluetoothService: BluetoothServiceProtocol Немного про логику метода  connect()  — под капотом подразумевается  сканирование, подключение и авторизация  в модуле. Авторизация — это некоторые манипуляции с публичным и приватным ключом, которые происходят на сервере, мы получаем от него результат манипуляций и записываем в модуль, далее ожидается статус о том, что мы авторизовались.  Только  после всего этого считаем, что мы сделали  connect() . Ещё важно учитывать, что все эти операции проходят с таймером. Если не успеваем подключиться — выбрасываем  timeout . Диаграмма метода connect Задача Главной задачей было связать все эти аспекты воедино: Модуль-труба, постоянно отправляющий данные Возможность получить ошибки в любой момент Старый API  CoreBluetooth  со своими делегатами Простой интерфейс и лаконичная  async throws  сигнатура Сервис –  actor Сomplete  – самый высокий  strict mode Делегаты Для начала подпишем наш  actor  под делегаты от  CoreBluetooth : extension BluetoothService: CBCentralManagerDelegate {\\n\\t\\n\\tfunc centralManagerDidUpdateState(_ central: CBCentralManager) {}\\n\\t\\n\\t...\\n} И получим ошибки: Actor-isolated instance method 'centralManagerDidUpdateState' cannot be @objc\\xa0 Actor-isolated instance method 'centralManagerDidUpdateState' cannot be used to satisfy nonisolated requirement from protocol 'CBCentralManagerDelegate'\\xa0 Non-'@objc' method 'centralManagerDidUpdateState' does not satisfy requirement of '@objc' protocol 'CBCentralManagerDelegate' Методы актора или должны быть  async , что невозможно — нарушится сигнатура, следовательно, не реализуем протокол, или же должны быть  nonisolated . Итак, делаем методы делегата неизолированными: nonisolated func centralManagerDidUpdateState(_ central: CBCentralManager) Со  Swift 6.1  можно  extension'ы  помечать как  nonisolated И вроде проблемы ушли, однако, мы не можем теперь из неизолированного метода ничего делать в акторе, даже через  Task , так как мы пытаемся захватить  central , а он доступен только внутри: nonisolated func centralManagerDidUpdateState(_ central: CBCentralManager) {\\n\\tTask { [weak self] in // Error\\n\\t\\tawait self?.handleBluetoothState(central.state)\\n\\t}\\n} Passing closure as a 'sending' parameter risks causing data races between code in the current task and concurrent execution of the closure Есть два решения этой проблемы: 1. Ослабить проверку на уровне импорта @preconcurrency import CoreBluetooth 2. Объявить используемые типы Sendable вручную extension CBPeripheral: @retroactive @unchecked Sendable {}\\nextension CBCharacteristic: @retroactive @unchecked Sendable {} Было решено использовать первый вариант. Да, к сожалению, так мы не обеспечиваем полную потокобезопасность, но иначе нельзя. В своё оправдание могу сказать, что все библиотеки, которые предоставляют API для  async/await  работы с Bluetooth, используют такой же способ избегать ошибок  strict mode . Самая популярная —  AsyncBluetooth :\\xa0 Реализуем connect На этом этапе мы смогли подключиться к модулю и начать получать с него в наш актор поток сообщений. Как теперь обработать его и реализовать протокол, который я показал выше? Вариант с Combine Первая мысль — использовать  Combine . Записывать в паблишер всё, что приходит в  didUpdateValueFor , а потом из подписки доставать публичный ключ или статус. Выше я писал про метод протокола  connect() , и что под капотом кроется авторизация и таймаут. То есть нужно под  async/await  синтаксис уместить подписку, в которой будут различные манипуляции (авторизация, таймер и т.п.): func connect() async throws {\\n\\tstartCentralManager()\\n\\t\\n\\treturn try await withUnsafeThrowingContinuation { continuation in\\n\\t\\tpublisher\\n\\t\\t\\t.sink { value in\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t// Очень много кода\\n\\t\\t\\t\\t\\n\\t\\t\\t\\tcontinuation.resume()\\n\\t\\t\\t}\\n\\t\\t\\t.store(in: &subscriptions)\\n\\t}\\n} Выглядит громоздко, да? Ещё надо учесть: Ошибки, которые могут прилететь из методов делегата в любой момент Как-то выкидывать timeout по таймеру Потенциальные проблемы с потокобезопасностью Время жизни подписки Не стоит смешивать  Combine  со  Swift Concurrency ... Вариант с AsyncStream На помощь пришёл  AsyncStream . Это удобный способ работать с асинхронными последовательностями данных. Значения в поток добавляются вручную с помощью метода  yield() , а завершить поток можно вызовом  finish() . На стороне потребителя данные читаются в цикле  for await ... in , который будет выполняться до тех пор, пока не придёт сигнал о завершении. Если кратко, это упрощённый  async/await  паблишер, как из  Combine , у которого максимум один подписчик.  Или несколько, если запустить консюмеров в разных  Task , однако так делать не стоит —\\xa0будет «неожиданное поведение». Почти во всех гайдах (а их мало) он используется так, как в документации Apple:\\xa0 extension QuakeMonitor {\\n\\n    static var quakes: AsyncStream<Quake> {\\n        AsyncStream { continuation in\\n            let monitor = QuakeMonitor()\\n            monitor.quakeHandler = { quake in\\n                continuation.yield(quake)\\n            }\\n            continuation.onTermination = { @Sendable _ in\\n                 monitor.stopMonitoring()\\n            }\\n            monitor.startMonitoring()\\n        }\\n    }\\n} Тут  continuation  только внутри замыкания. Однако, мало где представлен другой вариант использования, который очень сильно расширяет возможности стрима —\\xa0вынесение continuation наружу, чтобы можно было делать  yield()  и  finish()  в любом удобном месте объекта. Есть много интересных  обсуждений на форуме . Об этом также написано в  документации : Как это выглядит внутри  BluetoothService : var continuation: AsyncThrowingStream<String, Error>.Continuation?\\n\\nvar pipe: AsyncThrowingStream<String, Error> {\\n\\tAsyncStream { self.continuation = $0 }\\n}\\n\\n...\\n\\nfunc connect() async throws {\\n\\tstartCentralManager()\\n\\n\\tfor try await message in pipe {\\n\\t\\t\\n\\t\\t// Авторизация и тп...\\n\\t\\t\\n      \\t// Когда всё сделали:\\n        return\\n\\t}\\n}\\n\\n...\\n\\nnonisolated func peripheral(... didUpdateValueFor characteristic ...) {\\n\\tTask {\\n\\t\\tawait continuation.yield(message)\\n\\t}\\n}\\n\\nnonisolated func centralManagerDidUpdateState(_ central: CBCentralManager) {\\n\\tguard central.state != .poweredOn else { return }\\n\\t\\n\\tTask {\\n\\t\\tawait continuation.finish(throwing: Error)\\n\\t}\\n} Инициализация стрима,  continuation  которого вынесен как отдельное свойство сервиса. Метод  connect()  считывает его, ждёт в  for await loop  сообщение либо ошибку. Из  didUpdateValueFor  метода идёт запись декодированного сообщения в стрим через  .yield() . Для возможности завершать стрим с ошибкой я использовал  AsyncThrowingStream . Так, из метода  centralManagerDidUpdateState , если статус равен  .unauthorized , выполнится  .finish(throwing:) , соответственно,  for await loop  с кейвордом  try . Выглядит супер! Нужен async —\\xa0вот тебе  for await loop , жди его выполнения.\\xa0Прилетела ошибка —\\xa0выкинул throw прям тут же. Таймаут \\xa0 Взаимодействие с BLE-модулем может «зависнуть» — например, если устройство находится далеко или модуль работает нестабильно. Напомню логику — если у нас не получается подключиться и авторизоваться на протяжении  N-ого  времени, то выкидываем у метода  connect()  ошибку.\\xa0 В Concurrency есть возможность распараллелить один await в скоупе функции на два с помощью  withTaskGroup , затем получившуюся группу задач использовать как AsyncSequence (от него наследуется и AsyncStream), чтобы получить значения выполнения этих тасок: try await withThrowingTaskGroup { group in\\n\\t\\n\\tgroup.addTask {\\n\\t\\tfor try await message in pipe {\\n\\t\\t\\t// Авторизация...\\n\\t\\t}\\n\\t}\\n\\t\\n\\tgroup.addTask {\\n\\t\\ttry await Task.sleep(for: N)\\n\\t\\tthrow BluetoothError.timeout\\n\\t}\\n\\t\\n\\tguard let result = try await group.next() else { throw BluetoothError }\\n\\tgroup.cancelAll()\\n\\treturn result\\n} Что тут происходит? Я перенёс  for await loop  в одну из тасок этой группы. В другой таске мы просто «спим»  N времени  и выкидываем ошибку. Чтобы понять, что быстрее произойдёт, мы считываем с  group  (где лежат таски) следующий элемент  .next() . В нашем случае либо выход из скоупа, либо ошибка  timeout . Для удобства я обернул это в отдельную функцию  withTimeout , куда достаточно передать async throws логику, которую мы хотим ждать с таймером. Похожим образом реализуем остальные методы протокола. Итоги проекта В результате получилось решение, которое: Полностью укладывается в правила  strict mode . Даёт потокобезопасность за счёт actor без ручных локов и  DispatchQueue . Обрабатывает входящий поток данных через  AsyncStream , сохраняя последовательность и избавляясь от гонок. Нативно поддерживает таймауты на  async/await , без коллбеков и громоздких таймеров. По сравнению с «классическим» подходом на делегатах и очередях, код стал компактнее, читаемее и предсказуемее. Для разработчика это значит: меньше инфраструктурного кода и больше фокуса на бизнес-логику. Если завтра в проекте появится новый BLE-модуль или изменится протокол взаимодействия — адаптировать такой сервис гораздо проще. И самое приятное: всё это построено на стандартных возможностях  Swift Concurrency , без сторонних библиотек и без обходных манёвров с потоками.\", hub='транспорт'), Page(id='946286', title='Instinct от Continue: будущее автоматического рефакторинга', content='Команда  AI for Devs  подготовила перевод статьи о том, как Continue обучила Instinct — открытую модель Next Edit для предсказания следующей правки кода. Разработчики собрали тысячи реальных правок, внедрили алгоритм SeleKT для обучения и показали, что их модель работает в 6,4 раза быстрее ручного редактирования. Мы рады представить  Instinct  — открытую Next Edit-модель, которая умно предугадывает ваши действия и поддерживает рабочий ритм. Когда мы запускали  Next Edit , её впервые представил  Mercury Coder  от Inception. Сегодня мы расширяем границы возможностей с Instinct — открытой моделью, обученной внутри компании, которую разработчики могут запускать локально на своих GPU. В процессе работы Instinct улавливает траекторию ваших изменений и автоматически выполняет следующий шаг — примерно в 6,4 раза быстрее ручного редактирования. Зачем обучать открытую модель? Хотя open source модели для агентного программирования в последние месяцы развиваются стремительно, направление Next Edit пока находится в зачаточном состоянии. Основной прогресс был достигнут командой Zed с их моделью Zeta, и мы рады, что можем опираться на их опыт. Одна из наших целей — подчеркнуть открывающиеся возможности и заложить основу для будущей работы, которая принесёт пользу не только нашей команде, но и сообществу, и всей экосистеме разработчиков. И хотя существующие модели вроде Mercury Coder показывают отличные результаты, Instinct даёт разработчикам возможность запускать или настраивать Next Edit-модель на собственных GPU, решая задачи приватности и кастомизации. Что такое Next Edit? Аспект Традиционное атодополнение Next Edit Область изменений Вставляет текст только в точке курсора Переписывает окно кода (удаления, вставки, замены) Сложные изменения Требует многократного подтверждения Выполняет сложный рефакторинг одной операцией Перестройка кода Не умеет удалять или перестраивать код Понимает траектории редактирования и намерения разработчика Рабочий поток Частые прерывания сбивают ритм Поддерживает поток с минимумом отвлечений Обычное таб-дополнение вставляет код только в точке курсора во время набора. Это полезно при работе с шаблонным кодом, но в реальности большую часть времени разработчики занимаются рефакторингом, поддержкой, итерациями — то есть редактированием кода. Например, рефакторинг функции может включать: удаление старых параметров (5 нажатий клавиш), переход к оператору return (2 перемещения курсора), изменение возвращаемого типа (8 нажатий), обновление тела функции (20+ нажатий и 5+ перемещений курсора). С Instinct вся эта последовательность превращается в одно нажатие Tab для подтверждения — 40+ ручных операций сокращаются до одной. Обучение модели Данные из реальной разработки Чтобы создать нашу Next Edit-модель, нам нужны были качественные обучающие данные. Вместо того чтобы синтетически генерировать примеры, мы автоматически собрали более 4000 реальных правок от команды Continue во время работы над нашим open source-кодом. Это на порядок больше, чем датасет Zeta, опубликованный ранее в этом году, и такие данные лучше отражают реальные паттерны разработки, чем чисто синтетические примеры, собранные из git-коммитов. Каждый пример данных включал: пять последних правок разработчика, релевантный контекст из других файлов, участок кода для переписывания, фактическое изменение, которое разработчик внёс в этот участок. Ключевой вопрос, с которым мы столкнулись, — что считать «правкой». Каждое нажатие клавиши? Каждый раз при сохранении файла? Все нажатия в рамках определённого окна времени? После множества итераций мы определили набор эвристик на основе строк и времени, которые группировали отдельные нажатия в цельные и самодостаточные диффы. Анализируя последовательности таких диффов, мы заметили, что разработчики иногда возвращались к одним и тем же строкам, многократно изменяя их. Мы ввели фильтры, отбрасывающие такие примеры, поскольку хотели, чтобы Instinct делал эффективные и ненавязчивые изменения без повторов. Пайплайны контекста автодополнения  Continue предоставляли релевантную информацию о кодовой базе, включая содержимое текущего файла в prompt. Область редактирования (участок кода, который модель должна переписать) мы определили как отрезок, начинающийся строкой выше курсора и заканчивающийся пятью строками ниже. Это решение базировалось на естественных особенностях диффов в собранном датасете. В совокупности последовательность правок, контекст, содержимое текущего файла и выделенный участок кода дают всю необходимую информацию, чтобы понять намерения разработчика и предсказать следующий шаг в цепочке изменений. Поддержка многоязычности Одной из проблем стало то, что команда Continue в основном работает с кодом на Typescript. Однако нам хотелось, чтобы модель сохраняла поддержку нескольких языков. Помимо продуманного подхода к обучению (об этом ниже), мы использовали развернутую у себя модель Qwen3-Coder-30B, чтобы синтетически «перевести» диффы, контекст и содержимое файлов на Java, C, Python и Rust. Таким образом мы построили многоязычный датасет на основе данных с Typescript. Набор точных корректоров и фильтров обеспечивал высокое качество и сопоставимое распределение правок среди 4000+ синтетических примеров. Надёжное обучение для задачи Next Edit С готовым многоязычным датасетом мы перешли к supervised fine-tuning (SFT). Для специализированных задач вроде Next Edit SFT обычно выполняется с помощью Low-Rank Adaptation (LoRA): она хоть и  усваивает меньше, зато и забывает меньше  из уже имеющихся у предобученной модели навыков программирования. Главная проблема LoRA в том, что параметры для дообучения фиксируются ещё до старта обучения. Более предпочтительно выявить те параметры, обновление весов которых действительно критично, и менять только их. Тогда задача Next Edit будет не обучением «с нуля» ценой потери прежних знаний, а адаптацией к редактированию при сохранении общего кодерского бэкграунда. Именно такое решение мы нашли в алгоритме Selective Knowledge Transfer (SeleKT), применяемом для обучения  NextCoder  — моделей с инструкциями для редактирования кода. SeleKT вычисляет плотные градиенты (как при полном fine-tuning), выбирает top-k по величине и обновляет только их. Таким образом, практика показывает, что действительно нужно изменить, вместо угадывания заранее. В итоге корректируются только самые важные веса для задачи Next Edit. Более того, зануление мелких обновлений помогает избежать переобучения и деградации уже имеющихся знаний — проблем, с которыми столкнулся бы полный fine-tuning. Мы дообучили 5% параметров модели  Qwen2.5-Coder-7B  с помощью SeleKT. После первоначального подбора гиперпараметров процесс обучения был стандартным: постепенный разогрев learning rate и последующее затухание по косинусной кривой в течение 5 эпох. Для быстрой оценки качества переписываний на обучении мы использовали метрику  CodeBLEU , сравнивая предсказанные изменения с эталонными. Сравнение CodeBLEU между разными языками датасета позволило настроить пропорции данных и добиться высокой валидационной точности во всех языках. Благодаря такому подходу для обучения потребовалось всего небольшое число многоязычных примеров. Результаты: оценка качества и скорости 3.877 — средний балл LLM-оценщика (новый open-source state-of-the-art) в 6.4 раза быстрее, чем ручной набор правок  (на нашем кластере 8xH100) Формально оценить качество предложений Next Edit непросто, ведь одну и ту же задачу можно решить разными способами. Поэтому мы использовали Claude как LLM-оценщика с инструкциями оценивать предложения Next Edit по пятибалльной шкале, где: 5 \\xa0— полное функциональное совпадение с правкой разработчика; 4 \\xa0— близкая к эталонной правка, но без точного функционального совпадения; 3 \\xa0— правка, не совпадающая с эталоном, но логичная для эксперта в такой ситуации; 2 \\xa0— изменение, которое не вытекает логически из предыдущих правок и контекста; 1 \\xa0— вмешательство, мешающее работе разработчика (например, крупные удаления или полный оффтоп); 0 \\xa0— некорректное переписывание, не совпадающее с выделенной областью. Эта методика близка к той, что использует Zed для модели Zeta. Мы заметили, что их prompt приводил к оценкам только 0 или 5 и основывался на ручных утверждениях о следующей правке. Мы же разработали другой системный prompt, который позволяет охватить весь спектр баллов и сравнивает правку модели с эталонной правкой разработчика вместо жёстких правил. С небольшими адаптациями под наши промпты Instinct показала средний результат\\xa0 3.877 , превысив показатель Zeta ( 3.735 ) на тестовом наборе. Мы рады видеть, что сообщество продолжит развивать этот бенчмарк и улучшать результаты. Instinct не только выдаёт качественные подсказки — наша метрика «keystroke-distance», частично основанная на подходе  Coeditor , показывает, что модель также значительно ускоряет рабочий процесс. Если пройтись назад по таблице динамического программирования (DP) в алгоритме вычисления расстояния Левенштейна, можно извлечь посимвольный дифф между редактируемой областью и предложенным переписыванием. Этот дифф затем можно «нарезать» на операции редактирования: например, добавление трёх символов в одной позиции, удаление пяти в другой и т. д. Минимальное время выполнения правки определяется оптимальной комбинацией нажатий клавиш и перемещений курсора, необходимых для всех операций. Это тоже можно формализовать как задачу DP. Мы исходили из следующих допущений: разработчик печатает со средней скоростью 90 слов в минуту, может выделять и удалять блоки вместо многократного Backspace, использует мелкие перемещения стрелками вместо более затратных прыжков курсора, а при расчёте учитывается пространственная дистанция между позициями курсора (строка и символ), а не просто индекс в строке. Это нижняя граница времени для ручного редактирования. Мы сравнили её со средним временем инференса модели на нашем внутреннем эндпоинте SGLang плюс временем одного нажатия клавиши (Tab для подтверждения). Результат: даже если бы разработчик сразу знал нужную правку и выполнил её оптимальной последовательностью действий со скоростью 90 WPM, использование модели всё равно обеспечивало бы получение качественного изменения в\\xa0 6,4 раза быстрее . Русскоязычное сообщество про AI в разработке Друзья! Эту статью перевела команда ТГК « AI for Devs » — канала, где мы рассказываем про AI-ассистентов, плагины для IDE, делимся практическими кейсами и свежими новостями из мира ИИ.\\xa0 Подписывайтесь , чтобы быть в курсе и ничего не упустить! Что дальше? Во-первых, попробуйте Instinct сами! Модель весит 7B параметров, так что на большинстве ноутбуков она будет работать медленно, но при достаточном «железе» — отличный вариант для локального развертывания. Подробнее читайте в  нашем гайде . Если хотите развивать наш датасет, пайплайны обучения или открытые веса (например, чтобы запускать  KTO  на своих данных о принятых/отклонённых правках), загляните в наш  репозиторий на GitHub ,  модельную карточку на HuggingFace  и датасет.', hub='машинное обучение'), Page(id='946332', title='PO, PM, PjM: кто такие и при чём тут мыло, ракеты и застройщики', content='Перепост  статьи  из\\xa0моего личного блога.  Продуктовые роли не появились из воздуха. Их не придумали с нуля бородатые дядьки в Долине или выглаженные воротнички из Купертино. Профессии Product/Project Owner/Manager выстраивались на протяжении почти столетия — и всё ради того, чтобы сейчас мы с вами плевались, читая вакансии на хедхантере. Кто главнее: PM или PO? Что должен делать проджект? И что ещё за AI PM и Data PM? Давайте разбираться. И копнём аж до начала прошлого века. Исторический контекст Gantt, 1910-1918 Когда ещё не было даже намёков на современный проджект-менеджмент, инженер Генри Гантт впервые рисует « Gantt charts » для Baltimore & Ohio RR и корабельных верфей. Простейшие полоски-задачи + календарная шкала оказываются чудом наглядности. Минобороны США нанимает Гантта, чтобы распланировать переброску корпуса во Францию. Так больше века назад рождаются первые серьёзные инструменты для управления проектами. Brand Man, 1931 Спустя больше десяти лет Нил Макэлрой в Procter & Gamble  требует  выделить отдельную позицию «brand men», отвечающую за прибыль конкретного мыла, которым торговала компания. Именно этот шаг принято считать зарождением продакт-менеджмента (хотя в тот момент это была история больше про бренд, чем про продукт в его современном смысле). Lean, 1940+ В 1940х Тайити Оно и инженеры Toyota собрали  конвейер , который тянет детали «под заказ» вместо того, чтобы гнать их на склад: принцип Just-in-Time стал ядром Toyota Production System (TPS). Философия свелась к борьбе с любым «мусором» — лишними операциями, запасами, дефектами и простоями — и к непрерывному улучшению (кайдзен). К 1978ому TPS оформилась в книгу Оно, а западный менеджмент получил словарь Lean и идею, что скорость достигается не сокращением сроков, а плавным потоком ценности. CPM, 1957 Инженер Морган Уокер (DuPont) и математик Джеймс Келли (Remington Rand Univac) за два с небольшим года собирают алгоритм  Critical Path Method : вычисляем цепочку задач с нулевым запасом, накладываем на неё график — и видим, где сгорают деньги. Первый пилот прошёл в DuPont летом 1957го, технология мгновенно улетела в строительство и оборонку. PERT, 1958+ ВМС США (да-да, снова оборонка), разрабатывая ракету Polaris, создаёт  PERT  — предка большинства инструментов проектного управления. Program Evaluation and Review Technique (метод оценки и анализа программ) помогал определить последовательность задач, оценить время выполнения каждой и построить критический путь проекта. Родившееся как решение для военпрома, очень скоро PERT (как и CPM) перекочевали в строительные компании, где также требовалась чёткая и единая методология планирования. PMI, 1969+ Пятеро энтузиастов регистрируют  Project Management Institute  в Пенсильвании. Спустя некоторое время им ещё предстоит формализовать профессию Project Manager (PjM), но тогда PMI только зарождается. Активную работу над стандартами они начнут позже, в 80ых. А уже в 1996ом выпустят первое издание PMBOK Guide и формально кристаллизуют те самые 9 (на тот момент) областей знаний. Program Manager, 1984 В команде MS Excel появляется первая должность  Program Manager  (Джейб Блументаль): человек пишет 300-страничную спеку, общается с юзерами, торгуется с девелоперами и по сути владеет «что и зачем» продукта, оставляя «как» инженерам. Такой PM сидит в «триаде» вместе с Dev & Test и гонит фичи короткими релизами. Почти оновременно Microsoft выпускает Microsoft Project 1.0 — дешёвый Gantt-конкурент для PC, который быстро приучает менеджеров к сетевому плану и критическому пути. Шаблон «Program Manager + внутренняя тулза для графиков» разлетится по всей индустрии и станет базой для современных Product/Program Management. CMM, 1988+ Уортс Хамфри, ветеран IBM (в которой гигантские мэйнфрейм-проекты требовали железной дисциплины) оформляет свои наработки в идею  Capability Maturity Model  — пятиступенчатую «лестницу зрелости» процессов, родившую моду на метрики, peer-review и PM-офисы. Параллельно корпорация активно внедряет Integrated Product Development и к середине 1990-х уже учит тысячи инженеров единому языку рисков и базовых KPI проекта. Итог: если Microsoft сделал PM «хозяином продукта», то IBM закрепила за PM роль «сторожа процесса и качества», а их сочетание и сформировало то самое гибридное понимание Project/Product/Program Management, которым мы с вами пользуемся сегодня. Scrum, 1993+ В 1995ом на конференции OOPSLA Джефф Сазерленд и Кен Швабер презентуют  Scrum . В нём закрепляются роли Scrum Master и Product Owner. Это не «дата рождения» скрама, скорее дата его первой формализации. До этого он уже жил в коридорах Easel Corporation: Сазерленд, Скумниоталес и МакКенна за несколько лет до этого докрутили идеи Такаути и Нонаки, собрали первую скрам-команду и начали итеративно выпускать программные тулзы. Стоит отметить, что роль Product Owner в оригинальном Scrum была скорее про ответственность за ценность продукта, чем за детальный бэклог, как это часто понимают сегодня. Agile, 2001 Cемнадцать практиков (Кент Бек, Мартин Фаулер, Джим Хайсмит и другие), устав от тяжеловесных процессов в их компаниях, заперлись в лыжном домике в Юте. В результате такого заточения родился  Agile Manifesto : четыре коротких ценности (люди и взаимодействия, работающий софт, сотрудничество с заказчиком, готовность к изменениям) и двенадцать принципов. Ключевая мысль: лучше непрерывно доставлять маленькие порции ценности и общаться вживую, чем тонуть в документах, планах и контрактах. Манифест мгновенно поколебал иерархию «Project Manager — процесс — документация». Команды получили моральное право ставить ценность продукта выше сроков, а менеджеры — экспериментировать с ролями PM, PO, PjM. Именно после Agile Manifesto взлетели Scrum-гайды, Kanban-доски и подходы Discovery/Delivery: теперь задача менеджера была в том, чтобы убрать препятствия и помочь команде быстро учиться на фидбэке, а не просто «выполнить план по Гантту». Kanban, 2004-2007 Картонные канбан-карточки, которыми Toyota в 1950х синхронизировала поставки, в 2004ом  перенёс  в софт-разработку Дэвид Андерсон. Канбан-доска с колонками «To Do — Doing — Done» и жёстким лимитом параллельных задач дала командам прозрачный поток без ломки оргструктуры. С 2007го метод разлетелся по Agile-сообществу: сроки и риски теперь выводят из статистики цикла, а роль проджекта смещается к Flow/Delivery Manager, который охраняет WIP-лимиты и lead-time. В итоге Kanban стал легковесной альтернативой спринтам и вписался даже в ML-пайплайны, где важнее стабильный поток, чем фиксированные итерации. Inspired, 2008-2018 Но на этом история не заканчивается. В конце десятых Марти Каган публикует  культовую книгу  «Inspired». В ней Product Manager сравнивается с «мини-CEO» продукта и ставится выше Product Owner’а. Тысячи пятых точек мгновенно воспламеняются — и этого огня хватает, чтобы осветить путь нового подхода в Долине. Да так мощно, что переиздание выходит аж через 10 лет после первой версии книги. SAFe, 2011 Организация Scaled Agile формализует фреймворк  SAFe , где Product Manager управляет общим бэклогом, а PO — бэклогом команды. SAFe создаётся как альтернатива классическому скраму с его гиперсамостоятельностью отдельных команд и призван выстраивать и поддерживать зависимости в кросс-командных или кросс-платформенных продуктах. AI & Data PM, 2020+ Появляются специализации  AI Product Manager  и  Data Product Manager , акцентирующие владение данными, ML-моделями и этичной эксплуатацией искусственного интеллекта. Потихоньку они начинают вносить коррективы в устоявшиеся роли — и об этом мы тоже поговорим позже. Окей, а дальше что? Понятней не стало. А дальше давайте разложим это всё на пять самых популярных школ менеджмента и попробуем разобраться, почему в одних вакансиях оунеры ниже продактов, а в других рулят проджекты безо всяких PM и PO. Пять школ управленческого Дао Разумеется, отнюдь не все компании придерживаются какой-то конкретной школы. Вы легко можете встретить самые странные миксы. Но изложенная ниже структура поможет вам хотя бы понять, из каких ингредиентов собраны эти должностные коктейли. Silicon Valley В стартапах Кремниевой Долины PM задаёт стратегию, а PO её реализуют. Тут во всей красе расцветают  идеи  Кагана. Логика проста: рынок быстрый, а для быстрого рынка нужна единая визия (чтобы внутренние и связанные продукты не разорвало, как того хомячка от капли никотина). PM : Формирует продуктовую стратегию и roadmap, основываясь на рынке и данных пользователей. Проводит discovery, определяет метрики и координирует нескольких PO. Решает, какие фичи войдут в продукт, а какие на помойку. Влияет на бюджет (но не управляет им). PO : Детализирует бэклог, переводит стратегию в задачи. Впахивает с разработчиками, аналитикам и дизайнерами, обеспечивает выполнение спринтов. Приоритизирует задачи команды, решает, что готово для релиза. PjM : Редко встречается, может координировать ресурсы или зависимости. Полномочия PjM здесь ограничены логистикой, в продукт он не лезет. Чистый Scrum Здесь PO — единственный  владелец  ценности внутри команды, мать, отец и мотивационная морковка. PM (если он вообще есть) остаётся вне проектных процессов. В скраме основная ставка на автономность и скорость. PM : Если есть, собирает инсайты о рынке, проводит исследования, изредка — формирует стратегию. Влияет на приоритеты через рекомендации, но не вмешивается в дела команды и продукта. PO : Управляет бэклогом, определяет цели спринта, работает с разработчиками. Обеспечивает максимальную ценность продукта. Полностью контролирует приоритеты — именно он решает, что именно пойдёт в разработку, а что нахрен. PjM : Отсутствует, его функции выполняет Scrum Master. SAFe PM ведёт Agile Release Train (ART, огроменный поезд всех спринтов и зависимостей), а PO управляет одним небольшим вагончиком своей команды. Такая  двухэтажная модель  нужна, когда Agile раскатан на десятки людей и нужно общее PI-планирование (за которое тоже кто-то должен отвечать). PM : Управляет program backlog (приоритизированный список фич для ART), задаёт стратегию, согласовывает её с бизнесом. Участвует в PI-планировании. Решает, какие фичи войдут в Program Increment, влияет на ресурсы. PO : Детализирует бэклог команды, работает с разработчиками, обеспечивает выполнение задач в рамках ART. Приоритизирует задачи команды, определяет степень готовности фич. PjM : Координирует зависимости между командами, следит за сроками PI. Контролирует риски и сроки, но не продуктовые решения. PMI/Waterfall В PMI роль Project Manager (PjM)  старше всех : он отвечает за «срок-бюджет-скоуп» и регулярно докладывает заинтересованным сторонам. Продуктовые роли тут, по большей части, консультирующие. PM : Определяет границы продукта, консультирует по требованиям, проводит исследования рынка. Влияет на требования, но подчиняется PjM. PO : Обычно отсутствует или уточняет требования как аналитик. PjM : Планирует этапы, распределяет ресурсы, контролирует риски, отчитывается перед steering committee. FMCG Компании типа  Fast Moving Consumer Goods  исторически больше про офлайн, поэтому PM здесь — владелец P&L (бабла, по факту) и бренда. PO часто отсутствует или входит в digital-подразделения как Digital PM, ведь основное IT-направление в таких компаниях — маркетинговое. PM : Отвечает за стратегию бренда, маркетинг, ценообразование. В digital управляет цифровыми продуктами (например, loyalty apps или e-commerce). Шарит за стратегию, бюджеты и приоритеты digital-продуктов. PO : Детализирует бэклог для цифровых продуктов, работает с командами разработки. Приоритизирует задачи digital-продукта в рамках стратегии PM. PjM : Управляет проектами (например, запуск кампаний), координирует ресурсы. Контролирует сроки, но не стратегию. Ну что, стало попонятнее? Дайте мне минуту, уже исправляю. Не всё так просто Как я уже писал, роли миксуются. Где-то это осознанное решение, продиктованное продуктовыми, рыночными и бизнес-реалиями. А где-то PO и PM перепутаны просто по тупости и незнанию. Вот вам парочка примеров, когда классические подходы работают не совсем так, как были задуманы: Когда у вас двойной трек discovery & delivery, то даже при связке «PM выше PO» стратегия и тактика бегут параллельно. PM генерит гипотезы, PO быстро проверяет их с командой и говорят PM, куда генерить дальше. Матричные компании типа SAP или Siemens часто держат PM и PjM на одном уровне, решая спорные вопросы коллегиально. Это заложено в саму суть их оргструктуры. В банках, фарме и подобных направлениях регуляция и специфика отрасли толкает PjM наверх даже при Agile: сроки релизов настолько жёстко привязаны к аудиту и сертификации, что визионерство тихонечко курит в сторонке. Как меняют рынок AI PM & Data PM В наше время, если в твоём продукте нет ИИ, он рано или поздно сдохнет. Что бы там не говорили современные луддиты, но даже приложение для учёта расходов лучше справляется со своей задачей, если в нём ИИ-помощник. Как ответ на это рынок разрождается новыми ролями. И эти роли потихоньку изменяют уже привычных нам продактов. Общее проникновение AI заставляет менеджеров организовывать чёткую петлю обратной связи модели: без доменной экспертизы продукта ИИ-агенты учатся крайне неффективно и не приносят столько прибыли, сколько могли бы. McKinsey в своём недавнем  отчёте  указали, что AI PM должны развивать навыки работы с low-code платформами и агентскими фреймворками. Без этого не получится координировать сложные AI-системы и ускорять продуктовые циклы. Но работа с готовыми моделями ещё не всё. Нужно, чтобы кто-то курировал сквозной жизненный цикл датасета и культуру «data-as-a-product»: качество схем, метаданные, соблюдение privacy-ограничений и прочее. Так потихоньку рождается роль  Data Product Manager : человека, без которого консистентность данных (и, как следствие, всего остального) фактически недостижима. Так что если вы снова запутались, расслабьтесь: дальше будет только хуже. Пара слов в защиту здравого смысла Но давайте вернёмся к знакомым PO с PM и PjM. Как быть-то? Кому верить, какой школе? А никакой. Верьте реалиям бизнеса/продукта и здравому смыслу. Считайте команды. Если у вас одна кросс-функциональная — один человек легко совместит роли PM и PO. Три-семь команд? PM ставьте на стратегию, PO — в каждую команду. Контракт с фиксированным объёмом и датой? Вам не нужны визионеры, вам нужен сильный PjM. Давайте власть тому, кто держит метрику. Если PO отвечает за NPS, он должен иметь право отменить любой фич-реквест. Если квартальная премия PM зависит от LTV клиента, странно не давать ему на это влиять. Забейте на регалии. Сначала обязанности и метрики, и только потом титулы. То, что должен делать человек — намного важнее того, как называется его роль в оргструктуре. Но а если уж прям сильно хочется, то вот вам мой субъективный маркер-лист. Он немного противоречит каждой из перечисленных школ, но мне плевать: PM : Шарит за экономику, стратегию, продуктовые границы и визионирует как не в себя. PO : Меняет приоритет без бюрократии, раскатывает релизы, выстраивает коммуникации. PjM : Парится о сроках и процессах, подсвечивает риски и организует всё, что ещё не организованно. Итого Я не рассказал про тьму аспектов. Про культурные различия (США, Европа, СНГ, Азия), про всякие LeSS, Prince2 и прочее. Но от этого суть статьи не меняется. Роли появились по историческим причинам, школы выстроили разные пирамиды, но всё всегда сводится к одному: если у вас мутные зоны ответственности, полномочия не связаны с обязанностями, проверка ценности происходит интуитивно, а не на цифрах — вас не спасут никакие роли и термины. Ну и по традиции, мой  тг-канал  и  сайт . В первом я пишу всякое про управление, дизайн, разработку и аналитику, а на втором можно узнать, кто я такой и почему всё это себе позволяю.', hub='управление проектами')]","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:53.471256","level":"info","event":"В цикле: 4","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:53.471331","level":"info","event":"URL: https://habr.com/ru/articles/page4","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:05:54.352654","level":"info","event":"Result: <Response [200]>","logger":"pipeline_tools.parsers"}
{"timestamp":"2025-09-13T10:06:02.787795","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"ChunkedEncodingError","exc_value":"Response ended prematurely","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1215,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":397,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":216,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":239,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/parse_habr_dag.py","lineno":28,"name":"get_feeds"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pipeline_tools/parsers.py","lineno":111,"name":"parse_with_offset"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pipeline_tools/parsers.py","lineno":212,"name":"_parse_raw_result"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pipeline_tools/parsers.py","lineno":138,"name":"_parse_page"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/requests/api.py","lineno":73,"name":"get"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/requests/api.py","lineno":59,"name":"request"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py","lineno":589,"name":"request"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py","lineno":746,"name":"send"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/requests/models.py","lineno":902,"name":"content"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/requests/models.py","lineno":822,"name":"generate"}],"is_group":false,"exceptions":[]},{"exc_type":"ProtocolError","exc_value":"Response ended prematurely","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/requests/models.py","lineno":820,"name":"generate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/urllib3/response.py","lineno":1088,"name":"stream"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/urllib3/response.py","lineno":1248,"name":"read_chunked"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/urllib3/response.py","lineno":1178,"name":"_update_chunk_length"}],"is_group":false,"exceptions":[]}]}
